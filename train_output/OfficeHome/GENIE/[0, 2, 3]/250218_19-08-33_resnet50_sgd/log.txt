[37m[36mINFO[0m[0m 02/18 19:08:33 | Command :: /jsm0707/GENIE/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm GENIE --test_envs 0 2 3 --dataset OfficeHome --trial_seed 1 --hparams_seed 20
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 20
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/OfficeHome/GENIE/[0, 2, 3]/250218_19-08-33_resnet50_sgd
	out_root: train_output/OfficeHome/GENIE/[0, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 2, 3]
	trial_seed: 1
	unique_name: 250218_19-08-33_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 1.6701621650738547e-05
	batch_size: 28
	weight_decay: 2.09977539257237e-05
	momentum: 0.9323995445065565
	convergence_rate: 0.06623723551352119
	moving_avg: 0.9862532627566535
	p: 0.6377062156336589
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 02/18 19:08:33 | n_steps = 5001
[37m[36mINFO[0m[0m 02/18 19:08:33 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/18 19:08:33 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/18 19:08:33 | 
[37m[36mINFO[0m[0m 02/18 19:08:33 | Testenv name escaping te_A_P_R -> te_A_P_R
[37m[36mINFO[0m[0m 02/18 19:08:33 | Test envs = [0, 2, 3], name = te_A_P_R
[37m[36mINFO[0m[0m 02/18 19:08:33 | Train environments: [1], Test environments: [0, 2, 3]
[37m[36mINFO[0m[0m 02/18 19:08:33 | Batch sizes for each domain: [0, 28, 0, 0] (total=28)
[37m[36mINFO[0m[0m 02/18 19:08:33 | steps-per-epoch for each domain: 124.71 -> min = 124.71
[37m[36mINFO[0m[0m 02/18 19:08:34 | # of params = 23641217
[37m[36mINFO[0m[0m 02/18 19:10:30 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/18 19:10:30 | 0.015718    0.015820    0.021478    0.024055    4.308078    0.011329    0.014433    0.021478    0.024055    0.016892    0.014656    0.018933    0.018370    0           0.000000    4.260278    1.480032    114.040461 
[37m[36mINFO[0m[0m 02/18 19:13:17 | 0.066584    0.066538    0.095074    0.093929    3.808060    0.054068    0.053608    0.095074    0.093929    0.062782    0.051860    0.082903    0.094145    200         1.603666    3.967460    0.246765    117.773757 
[37m[36mINFO[0m[0m 02/18 19:15:50 | 0.221833    0.215664    0.421535    0.365407    2.635561    0.153965    0.158763    0.421535    0.365407    0.254505    0.234498    0.257028    0.253731    400         3.207331    2.996123    0.204445    112.242541 
[37m[36mINFO[0m[0m 02/18 19:18:27 | 0.285020    0.289003    0.611111    0.532646    1.864242    0.202884    0.228866    0.611111    0.532646    0.310811    0.323563    0.341365    0.314581    600         4.810997    1.910912    0.144465    127.812339 
[37m[36mINFO[0m[0m 02/18 19:20:53 | 0.306987    0.313260    0.691008    0.573883    1.789864    0.237899    0.255670    0.691008    0.573883    0.331081    0.321308    0.351979    0.362801    800         6.414662    1.339159    0.198999    106.768395 
[37m[36mINFO[0m[0m 02/18 19:23:40 | 0.339179    0.340390    0.729095    0.595647    1.593500    0.256437    0.249485    0.729095    0.595647    0.375845    0.366404    0.385255    0.405281    1000        8.018328    1.081631    0.148388    136.811709 
[37m[36mINFO[0m[0m 02/18 19:26:21 | 0.326635    0.332481    0.774055    0.640321    1.421565    0.243048    0.255670    0.774055    0.640321    0.353322    0.369786    0.383534    0.371986    1200        9.621993    0.946599    0.187955    123.244592 
[37m[36mINFO[0m[0m 02/18 19:28:44 | 0.335584    0.339214    0.821879    0.655212    1.474893    0.233780    0.249485    0.821879    0.655212    0.385135    0.370913    0.387837    0.397245    1400        11.225659   0.713724    0.182094    106.793343 
[37m[36mINFO[0m[0m 02/18 19:31:22 | 0.336811    0.336181    0.847938    0.643757    1.504383    0.240989    0.239175    0.847938    0.643757    0.374437    0.367531    0.395009    0.401837    1600        12.829324   0.613012    0.163152    125.554576 
[37m[36mINFO[0m[0m 02/18 19:33:41 | 0.336893    0.339768    0.849656    0.678121    1.303142    0.239444    0.253608    0.849656    0.678121    0.385980    0.379932    0.385255    0.385763    1800        14.432990   0.546721    0.159372    106.789461 
[37m[36mINFO[0m[0m 02/18 19:36:11 | 0.340748    0.347466    0.869416    0.668958    1.516458    0.264161    0.276289    0.869416    0.668958    0.368525    0.357384    0.389558    0.408726    2000        16.036655   0.471584    0.153090    119.720639 
[37m[36mINFO[0m[0m 02/18 19:38:55 | 0.349279    0.352457    0.900630    0.681558    1.543626    0.254892    0.249485    0.900630    0.681558    0.390766    0.395716    0.402180    0.412170    2200        17.640321   0.452416    0.262400    111.307778 
[37m[36mINFO[0m[0m 02/18 19:41:30 | 0.335479    0.331583    0.892898    0.699885    1.840697    0.247168    0.228866    0.892898    0.699885    0.366554    0.369786    0.392714    0.396096    2400        19.243986   0.357612    0.186890    117.752084 
[37m[36mINFO[0m[0m 02/18 19:44:05 | 0.352757    0.357043    0.897766    0.681558    1.785434    0.271370    0.292784    0.897766    0.681558    0.391892    0.378805    0.395009    0.399541    2600        20.847652   0.373738    0.191454    116.909223 
[37m[36mINFO[0m[0m 02/18 19:46:58 | 0.333589    0.326848    0.894616    0.663230    1.545087    0.253347    0.232990    0.894616    0.663230    0.371059    0.367531    0.376363    0.380023    2800        22.451317   0.348151    0.272657    118.394321 
[37m[36mINFO[0m[0m 02/18 19:49:31 | 0.342250    0.348287    0.924971    0.686140    1.662070    0.245623    0.270103    0.924971    0.686140    0.393863    0.386697    0.387263    0.388060    3000        24.054983   0.322510    0.148315    123.357880 
