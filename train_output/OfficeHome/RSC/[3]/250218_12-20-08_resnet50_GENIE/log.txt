[37m[36mINFO[0m[0m 02/18 12:20:08 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 3 --dataset OfficeHome --trial_seed 2 --hparams_seed 15
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 15
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/OfficeHome/RSC/[3]/250218_12-20-08_resnet50_GENIE
	out_root: train_output/OfficeHome/RSC/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 2
	unique_name: 250218_12-20-08_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.00023237243746776078
	batch_size: 13
	weight_decay: 1.8132449266758068e-05
	rsc_f_drop_factor: 0.24183226524778567
	rsc_b_drop_factor: 0.3096137712605297
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 02/18 12:20:08 | n_steps = 5001
[37m[36mINFO[0m[0m 02/18 12:20:08 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/18 12:20:08 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/18 12:20:08 | 
[37m[36mINFO[0m[0m 02/18 12:20:08 | Testenv name escaping te_R -> te_R
[37m[36mINFO[0m[0m 02/18 12:20:08 | Test envs = [3], name = te_R
[37m[36mINFO[0m[0m 02/18 12:20:08 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 02/18 12:20:08 | Batch sizes for each domain: [13, 13, 13, 0] (total=39)
[37m[36mINFO[0m[0m 02/18 12:20:08 | steps-per-epoch for each domain: 149.38, 268.62, 273.23 -> min = 149.38
[37m[36mINFO[0m[0m 02/18 12:20:09 | # of params = 23641217
[37m[36mINFO[0m[0m 02/18 12:22:24 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/18 12:22:24 | 0.020080    0.025258    0.017127    0.022496    4.180126    0.021627    0.028866    0.011455    0.018328    0.018300    0.020293    0.020080    0.025258    0           0.000000    5.444713    1.262586    133.449162 
[37m[36mINFO[0m[0m 02/18 12:25:09 | 0.015204    0.020666    0.020318    0.022166    4.167977    0.017508    0.016495    0.022050    0.025200    0.021396    0.024803    0.015204    0.020666    200         1.338826    4.186955    0.166323    131.608220 
[37m[36mINFO[0m[0m 02/18 12:27:44 | 0.019220    0.012629    0.025780    0.025944    4.161804    0.041195    0.039175    0.023196    0.020619    0.012950    0.018038    0.019220    0.012629    400         2.677652    4.171304    0.189093    116.945700 
[37m[36mINFO[0m[0m 02/18 12:30:35 | 0.019220    0.012629    0.025780    0.025944    4.156922    0.041195    0.039175    0.023196    0.020619    0.012950    0.018038    0.019220    0.012629    600         4.016478    4.166349    0.236261    124.398854 
[37m[36mINFO[0m[0m 02/18 12:33:12 | 0.019220    0.012629    0.025780    0.025944    4.150579    0.041195    0.039175    0.023196    0.020619    0.012950    0.018038    0.019220    0.012629    800         5.355304    4.165625    0.150889    126.520427 
[37m[36mINFO[0m[0m 02/18 12:35:57 | 0.019220    0.012629    0.025780    0.025944    4.137115    0.041195    0.039175    0.023196    0.020619    0.012950    0.018038    0.019220    0.012629    1000        6.694130    4.161568    0.194312    126.349810 
[37m[36mINFO[0m[0m 02/18 12:38:32 | 0.034137    0.028703    0.040255    0.038476    4.017855    0.037590    0.037113    0.040664    0.033219    0.042511    0.045096    0.034137    0.028703    1200        8.032956    4.145131    0.198796    115.525392 
[37m[36mINFO[0m[0m 02/18 12:41:09 | 0.039300    0.033295    0.043460    0.057720    3.908640    0.049434    0.074227    0.042096    0.053837    0.038851    0.045096    0.039300    0.033295    1400        9.371782    4.090085    0.152213    125.786744 
[37m[36mINFO[0m[0m 02/18 12:43:51 | 0.044464    0.026406    0.036607    0.038934    3.827934    0.026777    0.035052    0.032932    0.036655    0.050113    0.045096    0.044464    0.026406    1600        10.710608   4.048435    0.157536    130.377223 
[37m[36mINFO[0m[0m 02/18 12:46:26 | 0.038439    0.037887    0.041912    0.042481    3.821815    0.036560    0.030928    0.045819    0.043528    0.043356    0.052988    0.038439    0.037887    1800        12.049434   4.017214    0.191261    117.064721 
[37m[36mINFO[0m[0m 02/18 12:49:21 | 0.039300    0.047072    0.039238    0.034469    3.762462    0.028321    0.020619    0.042096    0.030928    0.047297    0.051860    0.039300    0.047072    2000        13.388260   3.995146    0.242806    126.733403 
[37m[36mINFO[0m[0m 02/18 12:52:12 | 0.067126    0.058553    0.065934    0.069553    3.691318    0.063337    0.059794    0.060424    0.074456    0.074043    0.074408    0.067126    0.058553    2200        14.727085   3.945924    0.185957    133.652513 
[37m[36mINFO[0m[0m 02/18 12:54:48 | 0.077166    0.060850    0.066373    0.065318    3.612768    0.071061    0.072165    0.064433    0.057274    0.063626    0.066516    0.077166    0.060850    2400        16.065911   3.915862    0.167955    122.702043 
[37m[36mINFO[0m[0m 02/18 12:57:39 | 0.090361    0.080367    0.105535    0.101112    3.355573    0.098352    0.101031    0.103952    0.103093    0.114302    0.099211    0.090361    0.080367    2600        17.404737   3.820779    0.242449    121.821209 
[37m[36mINFO[0m[0m 02/18 13:00:30 | 0.150029    0.159587    0.143484    0.132890    3.095999    0.143151    0.121649    0.141466    0.122566    0.145833    0.154453    0.150029    0.159587    2800        18.743563   3.691376    0.197390    131.629110 
