[37m[36mINFO[0m[0m 02/26 13:39:21 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 2 3 --dataset OfficeHome --trial_seed 2 --hparams_seed 1
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 1
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/OfficeHome/RSC/[0, 2, 3]/250226_13-39-21_resnet50_GENIE
	out_root: train_output/OfficeHome/RSC/[0, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 2, 3]
	trial_seed: 2
	unique_name: 250226_13-39-21_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 3.1159521264763664e-05
	batch_size: 11
	weight_decay: 1.4650833494643601e-05
	rsc_f_drop_factor: 0.1230288403437661
	rsc_b_drop_factor: 0.041552181381268205
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 02/26 13:39:21 | n_steps = 5001
[37m[36mINFO[0m[0m 02/26 13:39:21 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/26 13:39:21 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/26 13:39:21 | 
[37m[36mINFO[0m[0m 02/26 13:39:21 | Testenv name escaping te_A_P_R -> te_A_P_R
[37m[36mINFO[0m[0m 02/26 13:39:21 | Test envs = [0, 2, 3], name = te_A_P_R
[37m[36mINFO[0m[0m 02/26 13:39:21 | Train environments: [1], Test environments: [0, 2, 3]
[37m[36mINFO[0m[0m 02/26 13:39:21 | Batch sizes for each domain: [0, 11, 0, 0] (total=11)
[37m[36mINFO[0m[0m 02/26 13:39:21 | steps-per-epoch for each domain: 317.45 -> min = 317.45
[37m[36mINFO[0m[0m 02/26 13:39:22 | # of params = 23641217
[37m[36mINFO[0m[0m 02/26 13:41:09 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/26 13:41:09 | 0.039031    0.037446    0.041523    0.046964    4.204698    0.035015    0.024742    0.041523    0.046964    0.043637    0.043968    0.038439    0.043628    0           0.000000    4.605099    0.892786    106.248319 
[37m[36mINFO[0m[0m 02/26 13:43:25 | 0.046886    0.055597    0.078179    0.071019    4.015180    0.049434    0.057732    0.078179    0.071019    0.045327    0.063134    0.045898    0.045924    200         0.630011    4.153367    0.144131    107.378336 
[37m[36mINFO[0m[0m 02/26 13:45:40 | 0.200832    0.201342    0.298683    0.300115    2.830415    0.179712    0.187629    0.298683    0.300115    0.199606    0.207441    0.223178    0.208955    400         1.260023    3.692036    0.119213    110.527642 
[37m[36mINFO[0m[0m 02/26 13:48:05 | 0.258759    0.251926    0.406644    0.369989    2.409320    0.219361    0.185567    0.406644    0.369989    0.266610    0.271702    0.290304    0.298507    600         1.890034    2.865762    0.180025    109.563040 
[37m[36mINFO[0m[0m 02/26 13:50:24 | 0.286649    0.277816    0.512600    0.459336    2.074444    0.269310    0.259794    0.512600    0.459336    0.258164    0.271702    0.332473    0.301952    800         2.520046    2.301812    0.137539    111.232489 
[37m[36mINFO[0m[0m 02/26 13:52:38 | 0.366837    0.351772    0.623425    0.549828    1.772164    0.288877    0.270103    0.623425    0.549828    0.388514    0.379932    0.423121    0.405281    1000        3.150057    1.946840    0.123626    109.794380 
[37m[36mINFO[0m[0m 02/26 13:55:00 | 0.358352    0.376880    0.630298    0.540664    1.614633    0.313594    0.356701    0.630298    0.540664    0.356700    0.368658    0.404762    0.405281    1200        3.780069    1.642746    0.119301    118.113085 
[37m[36mINFO[0m[0m 02/26 13:57:14 | 0.395744    0.383836    0.699313    0.611684    1.459706    0.328012    0.317526    0.699313    0.611684    0.420608    0.412627    0.438612    0.421355    1400        4.410080    1.552190    0.122157    108.922091 
[37m[36mINFO[0m[0m 02/26 13:59:25 | 0.386547    0.378343    0.754868    0.649485    1.503051    0.297116    0.311340    0.754868    0.649485    0.427928    0.410372    0.434596    0.413318    1600        5.040092    1.436627    0.119651    107.443195 
[37m[36mINFO[0m[0m 02/26 14:01:46 | 0.338659    0.346012    0.726231    0.620848    1.645678    0.263646    0.259794    0.726231    0.620848    0.354167    0.384442    0.398164    0.393800    1800        5.670103    1.294956    0.108639    118.788334 
[37m[36mINFO[0m[0m 02/26 14:03:56 | 0.373358    0.364482    0.735682    0.625430    1.537360    0.297116    0.284536    0.735682    0.625430    0.411599    0.402480    0.411360    0.406429    2000        6.300115    1.189175    0.116941    107.438340 
