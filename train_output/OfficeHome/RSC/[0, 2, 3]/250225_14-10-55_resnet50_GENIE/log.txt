[37m[36mINFO[0m[0m 02/25 14:10:55 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 2 3 --dataset OfficeHome --trial_seed 1 --hparams_seed 1
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 1
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/OfficeHome/RSC/[0, 2, 3]/250225_14-10-55_resnet50_GENIE
	out_root: train_output/OfficeHome/RSC/[0, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 2, 3]
	trial_seed: 1
	unique_name: 250225_14-10-55_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.2332416678311953e-05
	batch_size: 13
	weight_decay: 0.0018634819595667504
	rsc_f_drop_factor: 0.157459736969642
	rsc_b_drop_factor: 0.14769128459366804
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 02/25 14:10:55 | n_steps = 5001
[37m[36mINFO[0m[0m 02/25 14:10:55 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/25 14:10:55 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/25 14:10:55 | 
[37m[36mINFO[0m[0m 02/25 14:10:55 | Testenv name escaping te_A_P_R -> te_A_P_R
[37m[36mINFO[0m[0m 02/25 14:10:55 | Test envs = [0, 2, 3], name = te_A_P_R
[37m[36mINFO[0m[0m 02/25 14:10:55 | Train environments: [1], Test environments: [0, 2, 3]
[37m[36mINFO[0m[0m 02/25 14:10:55 | Batch sizes for each domain: [0, 13, 0, 0] (total=13)
[37m[36mINFO[0m[0m 02/25 14:10:55 | steps-per-epoch for each domain: 268.62 -> min = 268.62
[37m[36mINFO[0m[0m 02/25 14:10:56 | # of params = 23641217
[37m[36mINFO[0m[0m 02/25 14:12:49 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/25 14:12:49 | 0.021389    0.024629    0.021764    0.020619    4.230578    0.027291    0.032990    0.021764    0.020619    0.021959    0.023675    0.014917    0.017222    0           0.000000    4.757322    1.121706    112.116138 
[37m[36mINFO[0m[0m 02/25 14:15:06 | 0.030891    0.029476    0.038373    0.041237    4.108399    0.033986    0.026804    0.038373    0.041237    0.021396    0.020293    0.037292    0.041332    200         0.744559    4.171124    0.133901    109.711310 
[37m[36mINFO[0m[0m 02/25 14:17:27 | 0.028319    0.039135    0.042669    0.029782    3.876626    0.033471    0.053608    0.042669    0.029782    0.023086    0.027057    0.028399    0.036739    400         1.489118    4.085801    0.130694    115.563099 
[37m[36mINFO[0m[0m 02/25 14:19:44 | 0.076492    0.074091    0.116552    0.122566    3.514952    0.057158    0.041237    0.116552    0.122566    0.080236    0.082300    0.092083    0.098737    600         2.233677    3.843054    0.137941    109.176871 
[37m[36mINFO[0m[0m 02/25 14:22:03 | 0.179048    0.180465    0.270619    0.254296    2.908552    0.142122    0.144330    0.270619    0.254296    0.196227    0.197294    0.198795    0.199770    800         2.978236    3.437137    0.136256    111.372203 
[37m[36mINFO[0m[0m 02/25 14:24:24 | 0.274230    0.278210    0.487973    0.402062    2.281369    0.205458    0.237113    0.487973    0.402062    0.307995    0.285231    0.309237    0.312285    1000        3.722795    2.820018    0.143651    112.099678 
[37m[36mINFO[0m[0m 02/25 14:26:39 | 0.364188    0.366676    0.627434    0.561283    1.764186    0.300721    0.334021    0.627434    0.561283    0.373311    0.363021    0.418531    0.402985    1200        4.467354    2.217146    0.139874    107.036246 
[37m[36mINFO[0m[0m 02/25 14:29:02 | 0.395165    0.393910    0.634880    0.549828    1.681516    0.335221    0.346392    0.634880    0.549828    0.408221    0.401353    0.442054    0.433984    1400        5.211913    1.840529    0.170672    109.455830 
