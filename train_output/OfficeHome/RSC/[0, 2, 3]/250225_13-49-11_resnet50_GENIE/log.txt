[37m[36mINFO[0m[0m 02/25 13:49:11 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 2 3 --dataset OfficeHome --trial_seed 0 --hparams_seed 1
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 1
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/OfficeHome/RSC/[0, 2, 3]/250225_13-49-11_resnet50_GENIE
	out_root: train_output/OfficeHome/RSC/[0, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 2, 3]
	trial_seed: 0
	unique_name: 250225_13-49-11_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5.0781288859686544e-05
	batch_size: 44
	weight_decay: 0.00046410133598234803
	rsc_f_drop_factor: 0.2665134852412123
	rsc_b_drop_factor: 0.28056710878365126
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 02/25 13:49:11 | n_steps = 5001
[37m[36mINFO[0m[0m 02/25 13:49:11 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/25 13:49:11 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/25 13:49:11 | 
[37m[36mINFO[0m[0m 02/25 13:49:11 | Testenv name escaping te_A_P_R -> te_A_P_R
[37m[36mINFO[0m[0m 02/25 13:49:11 | Test envs = [0, 2, 3], name = te_A_P_R
[37m[36mINFO[0m[0m 02/25 13:49:11 | Train environments: [1], Test environments: [0, 2, 3]
[37m[36mINFO[0m[0m 02/25 13:49:11 | Batch sizes for each domain: [0, 44, 0, 0] (total=44)
[37m[36mINFO[0m[0m 02/25 13:49:11 | steps-per-epoch for each domain: 79.36 -> min = 79.36
[37m[36mINFO[0m[0m 02/25 13:49:12 | # of params = 23641217
[37m[36mINFO[0m[0m 02/25 13:50:59 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/25 13:50:59 | 0.015990    0.017208    0.014891    0.019473    4.182472    0.020082    0.018557    0.014891    0.019473    0.011824    0.012401    0.016064    0.020666    0           0.000000    5.356624    1.460098    105.354923 
[37m[36mINFO[0m[0m 02/25 13:53:39 | 0.024863    0.023339    0.024628    0.018328    4.165633    0.028836    0.026804    0.024628    0.018328    0.022804    0.022548    0.022949    0.020666    200         2.520046    4.184645    0.234046    112.759077 
[37m[36mINFO[0m[0m 02/25 13:56:15 | 0.015470    0.018599    0.025200    0.013746    4.151500    0.012358    0.020619    0.025200    0.013746    0.019707    0.022548    0.014343    0.012629    400         5.040092    4.166656    0.236161    108.632812 
[37m[36mINFO[0m[0m 02/25 13:58:45 | 0.015442    0.019591    0.041237    0.027491    4.114840    0.007724    0.014433    0.041237    0.027491    0.021678    0.023675    0.016925    0.020666    600         7.560137    4.157276    0.216315    106.995881 
[37m[36mINFO[0m[0m 02/25 14:01:27 | 0.029097    0.034809    0.059565    0.041237    3.794581    0.021627    0.024742    0.059565    0.041237    0.031813    0.037204    0.033850    0.042480    800         10.080183   4.088455    0.291077    103.465531 
[37m[36mINFO[0m[0m 02/25 14:03:55 | 0.046327    0.047577    0.070733    0.081329    3.561724    0.047889    0.051546    0.070733    0.081329    0.037162    0.036077    0.053930    0.055109    1000        12.600229   3.930014    0.205299    107.229483 
[37m[36mINFO[0m[0m 02/25 14:06:39 | 0.073305    0.075372    0.103379    0.088202    3.487270    0.055098    0.068041    0.103379    0.088202    0.082489    0.082300    0.082329    0.075775    1200        15.120275   3.836119    0.207895    122.161035 
[37m[36mINFO[0m[0m 02/25 14:09:07 | 0.067519    0.077119    0.123998    0.096220    3.318080    0.063337    0.084536    0.123998    0.096220    0.070946    0.069899    0.068273    0.076923    1400        17.640321   3.710910    0.199353    108.274924 
[37m[36mINFO[0m[0m 02/25 14:11:46 | 0.083687    0.081396    0.174112    0.154639    3.073131    0.055613    0.070103    0.174112    0.154639    0.101070    0.085682    0.094378    0.088404    1600        20.160367   3.582089    0.225484    114.271565 
[37m[36mINFO[0m[0m 02/25 14:14:18 | 0.105218    0.120799    0.252864    0.197022    2.867357    0.071061    0.103093    0.252864    0.197022    0.126408    0.134160    0.118187    0.125144    1800        22.680412   3.461701    0.199232    112.006789 
[37m[36mINFO[0m[0m 02/25 14:16:43 | 0.141792    0.137615    0.370848    0.278351    2.662023    0.089598    0.088660    0.370848    0.278351    0.173986    0.164600    0.161790    0.159587    2000        25.200458   3.261752    0.194534    105.640547 
[37m[36mINFO[0m[0m 02/25 14:19:14 | 0.208216    0.213243    0.477377    0.358534    2.323535    0.153965    0.150515    0.477377    0.358534    0.230293    0.243517    0.240390    0.245695    2200        27.720504   2.978107    0.235504    104.445571 
[37m[36mINFO[0m[0m 02/25 14:21:45 | 0.245557    0.258156    0.633162    0.483391    2.052894    0.177137    0.191753    0.633162    0.483391    0.279842    0.278467    0.279690    0.304248    2400        30.240550   2.670289    0.196624    112.018522 
[37m[36mINFO[0m[0m 02/25 14:24:21 | 0.293219    0.307702    0.683276    0.513173    1.931933    0.220391    0.261856    0.683276    0.513173    0.330236    0.315671    0.329030    0.345580    2600        32.760596   2.389201    0.200430    115.907681 
[37m[36mINFO[0m[0m 02/25 14:26:47 | 0.312932    0.323617    0.765178    0.563574    2.063439    0.247683    0.261856    0.765178    0.563574    0.345158    0.341601    0.345955    0.367394    2800        35.280641   2.097870    0.200343    105.289709 
