[37m[36mINFO[0m[0m 02/26 13:23:40 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 1 3 --dataset OfficeHome --trial_seed 0 --hparams_seed 2
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 2
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/OfficeHome/RSC/[0, 1, 3]/250226_13-23-40_resnet50_GENIE
	out_root: train_output/OfficeHome/RSC/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 0
	unique_name: 250226_13-23-40_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.9041073434446342e-05
	batch_size: 9
	weight_decay: 0.0006566989842279891
	rsc_f_drop_factor: 0.2998869208594543
	rsc_b_drop_factor: 0.46590533469384005
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 02/26 13:23:40 | n_steps = 5001
[37m[36mINFO[0m[0m 02/26 13:23:40 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/26 13:23:40 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/26 13:23:40 | 
[37m[36mINFO[0m[0m 02/26 13:23:40 | Testenv name escaping te_A_C_R -> te_A_C_R
[37m[36mINFO[0m[0m 02/26 13:23:40 | Test envs = [0, 1, 3], name = te_A_C_R
[37m[36mINFO[0m[0m 02/26 13:23:40 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/26 13:23:40 | Batch sizes for each domain: [0, 0, 9, 0] (total=9)
[37m[36mINFO[0m[0m 02/26 13:23:40 | steps-per-epoch for each domain: 394.67 -> min = 394.67
[37m[36mINFO[0m[0m 02/26 13:23:41 | # of params = 23641217
[37m[36mINFO[0m[0m 02/26 13:25:30 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/26 13:25:30 | 0.028663    0.027512    0.033784    0.031567    4.198160    0.030381    0.020619    0.025773    0.035510    0.033784    0.031567    0.029834    0.026406    0           0.000000    6.238510    1.033862    107.459169 
[37m[36mINFO[0m[0m 02/26 13:27:52 | 0.024841    0.021551    0.022241    0.022548    4.172107    0.027806    0.026804    0.024055    0.017182    0.022241    0.022548    0.022662    0.020666    200         0.506757    4.189017    0.126849    116.732855 
[37m[36mINFO[0m[0m 02/26 13:30:15 | 0.025089    0.021551    0.022241    0.022548    4.169355    0.028836    0.026804    0.024055    0.017182    0.022241    0.022548    0.022375    0.020666    400         1.013514    4.168922    0.120785    119.032403 
[37m[36mINFO[0m[0m 02/26 13:32:38 | 0.025089    0.021551    0.022241    0.022548    4.167120    0.028836    0.026804    0.024055    0.017182    0.022241    0.022548    0.022375    0.020666    600         1.520270    4.166591    0.163814    110.429651 
[37m[36mINFO[0m[0m 02/26 13:35:08 | 0.025089    0.021551    0.022241    0.022548    4.165218    0.028836    0.026804    0.024055    0.017182    0.022241    0.022548    0.022375    0.020666    800         2.027027    4.164888    0.130166    124.036813 
[37m[36mINFO[0m[0m 02/26 13:37:24 | 0.025089    0.021551    0.022241    0.022548    4.163452    0.028836    0.026804    0.024055    0.017182    0.022241    0.022548    0.022375    0.020666    1000        2.533784    4.162326    0.120262    111.991589 
[37m[36mINFO[0m[0m 02/26 13:39:46 | 0.024115    0.021627    0.022523    0.022548    4.162229    0.029351    0.024742    0.020046    0.019473    0.022523    0.022548    0.022949    0.020666    1200        3.040541    4.159609    0.140599    113.746869 
[37m[36mINFO[0m[0m 02/26 13:41:57 | 0.025089    0.021551    0.022241    0.022548    4.160472    0.028836    0.026804    0.024055    0.017182    0.022241    0.022548    0.022375    0.020666    1400        3.547297    4.158519    0.124581    105.436271 
[37m[36mINFO[0m[0m 02/26 13:44:08 | 0.025089    0.021551    0.022241    0.022548    4.158812    0.028836    0.026804    0.024055    0.017182    0.022241    0.022548    0.022375    0.020666    1600        4.054054    4.159293    0.131140    105.665884 
[37m[36mINFO[0m[0m 02/26 13:46:39 | 0.025089    0.021551    0.022241    0.022548    4.157392    0.028836    0.026804    0.024055    0.017182    0.022241    0.022548    0.022375    0.020666    1800        4.560811    4.150504    0.136740    123.532441 
[37m[36mINFO[0m[0m 02/26 13:48:55 | 0.016424    0.015973    0.022804    0.020293    4.156170    0.018023    0.022680    0.010023    0.009164    0.022804    0.020293    0.021228    0.016073    2000        5.067568    4.151355    0.124848    110.615934 
[37m[36mINFO[0m[0m 02/26 13:51:12 | 0.016273    0.015898    0.023649    0.016911    4.155190    0.009269    0.004124    0.022050    0.025200    0.023649    0.016911    0.017499    0.018370    2200        5.574324    4.149850    0.120733    112.861111 
[37m[36mINFO[0m[0m 02/26 13:53:24 | 0.016273    0.015898    0.023649    0.016911    4.153869    0.009269    0.004124    0.022050    0.025200    0.023649    0.016911    0.017499    0.018370    2400        6.081081    4.145531    0.121647    107.825862 
[37m[36mINFO[0m[0m 02/26 13:55:46 | 0.016273    0.015898    0.023649    0.016911    4.152732    0.009269    0.004124    0.022050    0.025200    0.023649    0.016911    0.017499    0.018370    2600        6.587838    4.152854    0.143870    113.643919 
[37m[36mINFO[0m[0m 02/26 13:58:02 | 0.016273    0.015898    0.023649    0.016911    4.151595    0.009269    0.004124    0.022050    0.025200    0.023649    0.016911    0.017499    0.018370    2800        7.094595    4.141046    0.151922    105.616485 
[37m[36mINFO[0m[0m 02/26 14:00:16 | 0.016273    0.015898    0.023649    0.016911    4.150686    0.009269    0.004124    0.022050    0.025200    0.023649    0.016911    0.017499    0.018370    3000        7.601351    4.140742    0.132923    106.585587 
[37m[36mINFO[0m[0m 02/26 14:02:46 | 0.016273    0.015898    0.023649    0.016911    4.149782    0.009269    0.004124    0.022050    0.025200    0.023649    0.016911    0.017499    0.018370    3200        8.108108    4.143635    0.146210    121.402180 
