[37m[36mINFO[0m[0m 03/11 20:15:12 | Command :: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 1 --dataset OfficeHome --trial_seed 0 --hparams_seed 2
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_GENIE.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 2
	in_domain: False
	model_save: None
	mpa: False
	name: clip_vitb16_GENIE
	out_dir: train_output/OfficeHome/ERM/[1]/250311_20-15-12_clip_vitb16_GENIE
	out_root: train_output/OfficeHome/ERM/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250311_20-15-12_clip_vitb16_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.9041073434446342e-05
	batch_size: 9
	weight_decay: 0.0006566989842279891
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: openclip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 03/11 20:15:12 | n_steps = 5001
[37m[36mINFO[0m[0m 03/11 20:15:12 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/11 20:15:12 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/11 20:15:12 | 
[37m[36mINFO[0m[0m 03/11 20:15:12 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/11 20:15:12 | Test envs = [1], name = te_C
[37m[36mINFO[0m[0m 03/11 20:15:12 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 03/11 20:15:12 | Batch sizes for each domain: [9, 0, 9, 9] (total=27)
[37m[36mINFO[0m[0m 03/11 20:15:12 | steps-per-epoch for each domain: 215.78, 394.67, 387.33 -> min = 215.78
[37m[36mINFO[0m[0m 03/11 20:15:15 | # of params = 86225985
[37m[36mINFO[0m[0m 03/11 20:17:18 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/11 20:17:18 | 0.020619    0.018328    0.027242    0.028543    4.199873    0.032956    0.028866    0.020619    0.018328    0.030124    0.034949    0.018646    0.021814    0           0.000000    4.163729    1.965765    120.428935 
[37m[36mINFO[0m[0m 03/11 20:19:50 | 0.035510    0.044674    0.064310    0.072930    3.911796    0.066426    0.084536    0.035510    0.044674    0.060811    0.066516    0.065691    0.067738    200         0.926880    4.049552    0.174839    116.952377 
[37m[36mINFO[0m[0m 03/11 20:22:26 | 0.127434    0.140893    0.216547    0.215767    3.081437    0.186921    0.222680    0.127434    0.140893    0.226633    0.197294    0.236087    0.227325    400         1.853759    3.497491    0.178643    120.268893 
[37m[36mINFO[0m[0m 03/11 20:24:55 | 0.237686    0.227950    0.463509    0.457493    2.149868    0.383625    0.383505    0.237686    0.227950    0.544482    0.537768    0.462421    0.451206    600         2.780639    2.664833    0.174656    114.921268 
[37m[36mINFO[0m[0m 03/11 20:27:28 | 0.222795    0.239404    0.532820    0.474297    1.998861    0.446447    0.377320    0.222795    0.239404    0.637669    0.581736    0.514343    0.463835    800         3.707518    2.022650    0.184527    116.042059 
[37m[36mINFO[0m[0m 03/11 20:30:08 | 0.254296    0.265750    0.603200    0.554060    1.703311    0.507209    0.418557    0.254296    0.265750    0.710023    0.675310    0.592369    0.568312    1000        4.634398    1.709133    0.195425    120.580546 
[37m[36mINFO[0m[0m 03/11 20:32:45 | 0.256586    0.257732    0.633606    0.552218    1.686796    0.574665    0.459794    0.256586    0.257732    0.712556    0.658399    0.613597    0.538462    1200        5.561277    1.466677    0.207431    115.488266 
[37m[36mINFO[0m[0m 03/11 20:35:32 | 0.280069    0.272623    0.639290    0.533237    1.748334    0.584449    0.420619    0.280069    0.272623    0.707489    0.625705    0.625932    0.553387    1400        6.488157    1.320275    0.234016    119.856267 
[37m[36mINFO[0m[0m 03/11 20:38:23 | 0.249141    0.265750    0.642860    0.557709    1.706823    0.592688    0.457732    0.249141    0.265750    0.713401    0.649380    0.622490    0.566016    1600        7.415036    1.220782    0.262498    118.795234 
[37m[36mINFO[0m[0m 03/11 20:41:19 | 0.273196    0.274914    0.708491    0.592599    1.520887    0.653450    0.468041    0.273196    0.274914    0.780687    0.700113    0.691337    0.609644    1800        8.341916    1.071872    0.271423    121.585695 
[37m[36mINFO[0m[0m 03/11 20:44:09 | 0.280355    0.282932    0.748622    0.607909    1.472595    0.721421    0.494845    0.280355    0.282932    0.799831    0.721533    0.724613    0.607348    2000        9.268795    1.002943    0.245142    120.990376 
[37m[36mINFO[0m[0m 03/11 20:46:58 | 0.276632    0.286369    0.796104    0.640677    1.358920    0.780124    0.531959    0.276632    0.286369    0.846284    0.765502    0.761905    0.624569    2200        10.195675   0.816686    0.201283    128.868713 
[37m[36mINFO[0m[0m 03/11 20:49:35 | 0.315578    0.315006    0.797350    0.614865    1.478325    0.784243    0.498969    0.315578    0.315006    0.851351    0.747463    0.756454    0.598163    2400        11.122554   0.730860    0.188949    118.909126 
[37m[36mINFO[0m[0m 03/11 20:52:14 | 0.312715    0.304696    0.816007    0.652174    1.389945    0.804325    0.548454    0.312715    0.304696    0.864865    0.785795    0.778830    0.622273    2600        12.049434   0.647509    0.180776    122.895259 
