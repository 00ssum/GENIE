[37m[36mINFO[0m[0m 02/20 17:22:32 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset OfficeHome --trial_seed 0 --hparams_seed 7
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 7
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/OfficeHome/CORAL/[0, 1, 3]/250220_17-22-32_resnet50_GENIE
	out_root: train_output/OfficeHome/CORAL/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 0
	unique_name: 250220_17-22-32_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.6632780446310692e-05
	batch_size: 24
	weight_decay: 5.717289389191427e-06
	mmd_gamma: 3.812683559377669
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 02/20 17:22:32 | n_steps = 5001
[37m[36mINFO[0m[0m 02/20 17:22:32 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/20 17:22:32 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/20 17:22:32 | 
[37m[36mINFO[0m[0m 02/20 17:22:32 | Testenv name escaping te_A_C_R -> te_A_C_R
[37m[36mINFO[0m[0m 02/20 17:22:32 | Test envs = [0, 1, 3], name = te_A_C_R
[37m[36mINFO[0m[0m 02/20 17:22:32 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/20 17:22:32 | Batch sizes for each domain: [0, 0, 24, 0] (total=24)
[37m[36mINFO[0m[0m 02/20 17:22:32 | steps-per-epoch for each domain: 148.00 -> min = 148.00
[37m[36mINFO[0m[0m 02/20 17:22:33 | # of params = 23641217
[37m[36mINFO[0m[0m 02/20 17:24:49 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/20 17:24:49 | 0.024041    0.023080    0.034065    0.033822    4.192310    0.023687    0.016495    0.025773    0.029782    0.034065    0.033822    0.022662    0.022962    0           0.000000    4.292307    0.000000    1.757185    134.100873 
[37m[36mINFO[0m[0m 02/20 17:27:39 | 0.411583    0.436336    0.802646    0.758737    0.808953    0.368692    0.385567    0.321306    0.341352    0.802646    0.758737    0.544750    0.582090    200         1.351351    1.651961    0.000000    0.143592    140.893970 
[37m[36mINFO[0m[0m 02/20 17:29:57 | 0.458532    0.464772    0.893018    0.804961    0.657529    0.403193    0.422680    0.367411    0.352806    0.893018    0.804961    0.604991    0.618829    400         2.702703    0.553406    0.000000    0.160416    106.344329 
[37m[36mINFO[0m[0m 02/20 17:32:45 | 0.453194    0.446428    0.909910    0.829763    0.639319    0.377446    0.371134    0.379439    0.371134    0.909910    0.829763    0.602697    0.597015    600         4.054054    0.308569    0.000000    0.200777    127.196801 
[37m[36mINFO[0m[0m 02/20 17:35:18 | 0.453941    0.462797    0.934403    0.852311    0.607677    0.393409    0.414433    0.351661    0.341352    0.934403    0.852311    0.616753    0.632606    800         5.405405    0.268910    0.000000    0.133139    127.004662 
[37m[36mINFO[0m[0m 02/20 17:37:39 | 0.444374    0.456669    0.934122    0.844419    0.552849    0.388774    0.435052    0.334192    0.326460    0.934122    0.844419    0.610155    0.608496    1000        6.756757    0.206509    0.000000    0.134670    113.722332 
[37m[36mINFO[0m[0m 02/20 17:40:10 | 0.451692    0.462347    0.963964    0.851184    0.522722    0.382080    0.406186    0.355956    0.337915    0.963964    0.851184    0.617040    0.642939    1200        8.108108    0.156292    0.000000    0.131693    124.146097 
[37m[36mINFO[0m[0m 02/20 17:42:34 | 0.471542    0.481887    0.960023    0.878241    0.456998    0.405252    0.435052    0.386884    0.379152    0.960023    0.878241    0.622490    0.631458    1400        9.459459    0.127924    0.000000    0.125607    119.800061 
