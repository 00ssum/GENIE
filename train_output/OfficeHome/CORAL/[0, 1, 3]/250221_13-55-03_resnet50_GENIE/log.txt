[37m[36mINFO[0m[0m 02/21 13:55:03 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset OfficeHome --trial_seed 2 --hparams_seed 0
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/OfficeHome/CORAL/[0, 1, 3]/250221_13-55-03_resnet50_GENIE
	out_root: train_output/OfficeHome/CORAL/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 2
	unique_name: 250221_13-55-03_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	mmd_gamma: 1.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 02/21 13:55:03 | n_steps = 5001
[37m[36mINFO[0m[0m 02/21 13:55:03 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/21 13:55:03 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/21 13:55:03 | 
[37m[36mINFO[0m[0m 02/21 13:55:03 | Testenv name escaping te_A_C_R -> te_A_C_R
[37m[36mINFO[0m[0m 02/21 13:55:03 | Test envs = [0, 1, 3], name = te_A_C_R
[37m[36mINFO[0m[0m 02/21 13:55:03 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/21 13:55:03 | Batch sizes for each domain: [0, 0, 32, 0] (total=32)
[37m[36mINFO[0m[0m 02/21 13:55:03 | steps-per-epoch for each domain: 111.00 -> min = 111.00
[37m[36mINFO[0m[0m 02/21 13:55:05 | # of params = 23641217
[37m[36mINFO[0m[0m 02/21 13:57:11 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/21 13:57:11 | 0.023087    0.023922    0.032658    0.023675    4.230879    0.023687    0.024742    0.022050    0.021764    0.032658    0.023675    0.023523    0.025258    0           0.000000    4.242151    0.000000    1.447846    124.938399 
[37m[36mINFO[0m[0m 02/21 13:59:55 | 0.461318    0.454448    0.853041    0.808343    0.713020    0.425850    0.432990    0.341065    0.331042    0.853041    0.808343    0.617040    0.599311    200         1.801802    1.375306    0.000000    0.195446    124.872461 
[37m[36mINFO[0m[0m 02/21 14:02:26 | 0.473915    0.477298    0.909628    0.817362    0.695216    0.408342    0.435052    0.379439    0.374570    0.909628    0.817362    0.633964    0.622273    400         3.603604    0.350996    0.000000    0.153957    119.930822 
[37m[36mINFO[0m[0m 02/21 14:05:10 | 0.478959    0.481794    0.946509    0.863585    0.534690    0.408857    0.426804    0.386884    0.408935    0.946509    0.863585    0.641136    0.609644    600         5.405405    0.237231    0.000000    0.191773    125.923649 
[37m[36mINFO[0m[0m 02/21 14:07:44 | 0.471150    0.472579    0.957770    0.850056    0.585275    0.408342    0.418557    0.365693    0.357388    0.957770    0.850056    0.639415    0.641791    800         7.207207    0.178127    0.000000    0.179441    118.224534 
[37m[36mINFO[0m[0m 02/21 14:10:15 | 0.462108    0.469058    0.957489    0.846674    0.613552    0.409887    0.420619    0.356816    0.355097    0.957489    0.846674    0.619621    0.631458    1000        9.009009    0.126629    0.000000    0.152348    120.588221 
[37m[36mINFO[0m[0m 02/21 14:12:44 | 0.487018    0.480356    0.979167    0.869222    0.540980    0.420700    0.424742    0.387171    0.390607    0.979167    0.869222    0.653184    0.625718    1200        10.810811   0.100414    0.000000    0.194305    110.082343 
[37m[36mINFO[0m[0m 02/21 14:15:18 | 0.477095    0.484870    0.977477    0.855693    0.596202    0.394954    0.416495    0.392039    0.400916    0.977477    0.855693    0.644291    0.637199    1400        12.612613   0.075702    0.000000    0.151789    123.865118 
[37m[36mINFO[0m[0m 02/21 14:17:47 | 0.480890    0.475777    0.987613    0.865840    0.625488    0.418641    0.424742    0.372852    0.372279    0.987613    0.865840    0.651176    0.630310    1600        14.414414   0.063015    0.000000    0.162804    115.725241 
[37m[36mINFO[0m[0m 02/21 14:20:24 | 0.480950    0.483872    0.983390    0.883878    0.504638    0.409372    0.443299    0.383162    0.378007    0.983390    0.883878    0.650316    0.630310    1800        16.216216   0.071549    0.000000    0.196043    117.694645 
[37m[36mINFO[0m[0m 02/21 14:23:01 | 0.485779    0.485550    0.988176    0.889515    0.473493    0.410402    0.428866    0.397766    0.399771    0.988176    0.889515    0.649168    0.628014    2000        18.018018   0.058435    0.000000    0.173922    122.723396 
[37m[36mINFO[0m[0m 02/21 14:25:34 | 0.475351    0.464380    0.986768    0.881623    0.548279    0.395469    0.391753    0.397766    0.395189    0.986768    0.881623    0.632817    0.606200    2200        19.819820   0.037281    0.000000    0.172142    118.787332 
[37m[36mINFO[0m[0m 02/21 14:28:09 | 0.470017    0.470284    0.991836    0.878241    0.609543    0.385170    0.387629    0.378007    0.386025    0.991836    0.878241    0.646873    0.637199    2400        21.621622   0.039266    0.000000    0.191218    116.586454 
[37m[36mINFO[0m[0m 02/21 14:30:41 | 0.484730    0.481987    0.989302    0.877114    0.645445    0.425850    0.412371    0.366838    0.371134    0.989302    0.877114    0.661503    0.662457    2600        23.423423   0.032054    0.000000    0.150618    121.934906 
[37m[36mINFO[0m[0m 02/21 14:33:17 | 0.489493    0.484431    0.993806    0.900789    0.576724    0.421215    0.439175    0.373711    0.351661    0.993806    0.900789    0.673551    0.662457    2800        25.225225   0.030556    0.000000    0.154289    124.787207 
[37m[36mINFO[0m[0m 02/21 14:35:47 | 0.479025    0.478996    0.994088    0.899662    0.556951    0.399588    0.389691    0.386312    0.402062    0.994088    0.899662    0.651176    0.645235    3000        27.027027   0.025836    0.000000    0.157467    118.882147 
