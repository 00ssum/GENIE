[37m[36mINFO[0m[0m 02/19 14:00:26 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset OfficeHome --trial_seed 2 --hparams_seed 20
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: OfficeHome
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 20
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/OfficeHome/CORAL/[0, 1, 3]/250219_14-00-26_resnet50_GENIE
	out_root: train_output/OfficeHome/CORAL/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 2
	unique_name: 250219_14-00-26_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 3.0575564760860844e-05
	batch_size: 27
	weight_decay: 7.45235722180653e-06
	mmd_gamma: 1.450436842057683
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[OfficeHome] #envs=4, #classes=65
	env0: A (#2427)
	env1: C (#4365)
	env2: P (#4439)
	env3: R (#4357)

[37m[36mINFO[0m[0m 02/19 14:00:26 | n_steps = 5001
[37m[36mINFO[0m[0m 02/19 14:00:26 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/19 14:00:26 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/19 14:00:26 | 
[37m[36mINFO[0m[0m 02/19 14:00:26 | Testenv name escaping te_A_C_R -> te_A_C_R
[37m[36mINFO[0m[0m 02/19 14:00:26 | Test envs = [0, 1, 3], name = te_A_C_R
[37m[36mINFO[0m[0m 02/19 14:00:26 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/19 14:00:26 | Batch sizes for each domain: [0, 0, 27, 0] (total=27)
[37m[36mINFO[0m[0m 02/19 14:00:26 | steps-per-epoch for each domain: 131.56 -> min = 131.56
[37m[36mINFO[0m[0m 02/19 14:00:27 | # of params = 23641217
[37m[36mINFO[0m[0m 02/19 14:02:18 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/19 14:02:18 | 0.025225    0.025070    0.038007    0.033822    4.213679    0.025232    0.024742    0.025200    0.021764    0.038007    0.033822    0.025244    0.028703    0           0.000000    4.303727    0.000000    1.085403    109.648787 
[37m[36mINFO[0m[0m 02/19 14:04:33 | 0.458466    0.458270    0.851070    0.804961    0.708309    0.387230    0.391753    0.373711    0.378007    0.851070    0.804961    0.614458    0.605052    200         1.520270    1.399741    0.000000    0.124556    109.973585 
[37m[36mINFO[0m[0m 02/19 14:06:41 | 0.462106    0.470801    0.894707    0.821871    0.610817    0.394439    0.414433    0.384021    0.383734    0.894707    0.821871    0.607860    0.614237    400         3.040541    0.484953    0.000000    0.124053    103.431228 
[37m[36mINFO[0m[0m 02/19 14:08:56 | 0.468921    0.479370    0.935248    0.846674    0.572205    0.391349    0.420619    0.378580    0.382589    0.935248    0.846674    0.636833    0.634902    600         4.560811    0.308822    0.000000    0.152651    104.873630 
[37m[36mINFO[0m[0m 02/19 14:11:09 | 0.460865    0.455530    0.937782    0.843292    0.594534    0.404737    0.414433    0.364834    0.335624    0.937782    0.843292    0.613024    0.616533    800         6.081081    0.224707    0.000000    0.121392    108.136387 
[37m[36mINFO[0m[0m 02/19 14:13:19 | 0.478820    0.486166    0.957489    0.850056    0.633281    0.412461    0.422680    0.389748    0.400916    0.957489    0.850056    0.634251    0.634902    1000        7.601351    0.153119    0.000000    0.124188    105.612399 
[37m[36mINFO[0m[0m 02/19 14:15:31 | 0.476836    0.491514    0.965372    0.850056    0.659574    0.409372    0.443299    0.385739    0.394044    0.965372    0.850056    0.635399    0.637199    1200        9.121622    0.121104    0.000000    0.146810    102.211339 
[37m[36mINFO[0m[0m 02/19 14:17:45 | 0.443855    0.445736    0.972691    0.852311    0.563052    0.346550    0.369072    0.373711    0.376861    0.972691    0.852311    0.611302    0.591274    1400        10.641892   0.098880    0.000000    0.121815    109.861302 
[37m[36mINFO[0m[0m 02/19 14:20:05 | 0.443275    0.446051    0.957770    0.841037    0.746103    0.372297    0.402062    0.342497    0.332188    0.957770    0.841037    0.615032    0.603904    1600        12.162162   0.083483    0.000000    0.140997    111.763149 
[37m[36mINFO[0m[0m 02/19 14:22:21 | 0.483981    0.470121    0.980856    0.877114    0.606696    0.407312    0.402062    0.403494    0.384880    0.980856    0.877114    0.641136    0.623421    1800        13.682432   0.072471    0.000000    0.146717    106.845357 
[37m[36mINFO[0m[0m 02/19 14:24:56 | 0.488205    0.490988    0.983390    0.881623    0.599333    0.411946    0.416495    0.400344    0.407789    0.983390    0.881623    0.652324    0.648680    2000        15.202703   0.063704    0.000000    0.129098    129.206580 
[37m[36mINFO[0m[0m 02/19 14:27:26 | 0.459188    0.458430    0.982827    0.868095    0.631258    0.381050    0.408247    0.358820    0.352806    0.982827    0.868095    0.637694    0.614237    2200        16.722973   0.052314    0.000000    0.157959    118.515609 
[37m[36mINFO[0m[0m 02/19 14:30:10 | 0.488349    0.501677    0.989302    0.883878    0.628377    0.403193    0.437113    0.395762    0.421535    0.989302    0.883878    0.666093    0.646383    2400        18.243243   0.044093    0.000000    0.170131    129.034562 
[37m[36mINFO[0m[0m 02/19 14:33:03 | 0.479960    0.494341    0.987613    0.885006    0.631116    0.400103    0.439175    0.389462    0.404353    0.987613    0.885006    0.650316    0.639495    2600        19.763514   0.040062    0.000000    0.137857    145.889983 
[37m[36mINFO[0m[0m 02/19 14:35:48 | 0.489786    0.484119    0.986768    0.888388    0.590788    0.423275    0.416495    0.392325    0.381443    0.986768    0.888388    0.653758    0.654420    2800        21.283784   0.035586    0.000000    0.147279    135.496594 
[37m[36mINFO[0m[0m 02/19 14:38:23 | 0.491055    0.503969    0.994088    0.874859    0.593451    0.404737    0.437113    0.410653    0.427262    0.994088    0.874859    0.657774    0.647532    3000        22.804054   0.045372    0.000000    0.149250    125.006231 
[37m[36mINFO[0m[0m 02/19 14:41:19 | 0.458476    0.459547    0.974662    0.864713    0.738036    0.361483    0.397938    0.398053    0.404353    0.974662    0.864713    0.615892    0.576349    3200        24.324324   0.027482    0.000000    0.132564    149.581615 
