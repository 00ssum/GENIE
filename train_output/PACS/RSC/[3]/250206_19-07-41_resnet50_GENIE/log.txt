[37m[36mINFO[0m[0m 02/06 19:07:41 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 3 --dataset PACS --trial_seed 2 --hparams_seed 5
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 5
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/PACS/RSC/[3]/250206_19-07-41_resnet50_GENIE
	out_root: train_output/PACS/RSC/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 2
	unique_name: 250206_19-07-41_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.0002982828768978312
	batch_size: 9
	weight_decay: 0.0003180957897867641
	rsc_f_drop_factor: 0.22729325817372148
	rsc_b_drop_factor: 0.134901679846891
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 02/06 19:07:41 | n_steps = 5001
[37m[36mINFO[0m[0m 02/06 19:07:41 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/06 19:07:41 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/06 19:07:41 | 
[37m[36mINFO[0m[0m 02/06 19:07:41 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 02/06 19:07:41 | Test envs = [3], name = te_S
[37m[36mINFO[0m[0m 02/06 19:07:41 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 02/06 19:07:41 | Batch sizes for each domain: [9, 9, 9, 0] (total=27)
[37m[36mINFO[0m[0m 02/06 19:07:41 | steps-per-epoch for each domain: 182.11, 208.44, 148.44 -> min = 148.44
[37m[36mINFO[0m[0m 02/06 19:07:42 | # of params = 23522375
[37m[36mINFO[0m[0m 02/06 19:08:24 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/06 19:08:24 | 0.196247    0.197452    0.159549    0.153466    1.936743    0.186699    0.183374    0.168443    0.160256    0.123503    0.116766    0.196247    0.197452    0           0.000000    2.451568    1.174317    40.266055  
[37m[36mINFO[0m[0m 02/06 19:09:34 | 0.218830    0.236943    0.331189    0.316694    1.674072    0.289201    0.286064    0.277719    0.262821    0.426647    0.401198    0.218830    0.236943    200         1.347305    1.907618    0.165162    37.643522  
[37m[36mINFO[0m[0m 02/06 19:10:48 | 0.338740    0.366879    0.700167    0.704003    0.793815    0.666260    0.652812    0.642324    0.698718    0.791916    0.760479    0.338740    0.366879    400         2.694611    1.479581    0.165825    39.856077  
[37m[36mINFO[0m[0m 02/06 19:12:06 | 0.610051    0.626752    0.877119    0.880673    0.347833    0.851129    0.855746    0.832623    0.846154    0.947605    0.940120    0.610051    0.626752    600         4.041916    0.912243    0.179553    42.111651  
[37m[36mINFO[0m[0m 02/06 19:13:22 | 0.589695    0.625478    0.874068    0.878315    0.325863    0.841977    0.860636    0.832623    0.831197    0.947605    0.943114    0.589695    0.625478    800         5.389222    0.586841    0.177271    40.835516  
[37m[36mINFO[0m[0m 02/06 19:14:35 | 0.552799    0.563057    0.919446    0.925769    0.244609    0.874314    0.894866    0.914712    0.927350    0.969311    0.955090    0.552799    0.563057    1000        6.736527    0.451444    0.180111    37.223328  
[37m[36mINFO[0m[0m 02/06 19:15:53 | 0.709288    0.714650    0.949928    0.943834    0.221015    0.937157    0.914425    0.932836    0.938034    0.979790    0.979042    0.709288    0.714650    1200        8.083832    0.376739    0.208824    35.844568  
[37m[36mINFO[0m[0m 02/06 19:17:09 | 0.677481    0.685350    0.948613    0.936278    0.246018    0.930445    0.897311    0.937100    0.929487    0.978293    0.982036    0.677481    0.685350    1400        9.431138    0.331958    0.197098    36.766818  
[37m[36mINFO[0m[0m 02/06 19:18:22 | 0.651399    0.672611    0.935742    0.935651    0.283054    0.917023    0.926650    0.921642    0.925214    0.968563    0.955090    0.651399    0.672611    1600        10.778443   0.317390    0.186950    35.293612  
[37m[36mINFO[0m[0m 02/06 19:19:38 | 0.729962    0.737580    0.949249    0.927725    0.226206    0.929835    0.914425    0.953092    0.931624    0.964820    0.937126    0.729962    0.737580    1800        12.125749   0.336938    0.212016    33.795712  
[37m[36mINFO[0m[0m 02/06 19:20:58 | 0.717239    0.726115    0.973249    0.952368    0.163288    0.967663    0.941320    0.968550    0.948718    0.983533    0.967066    0.717239    0.726115    2000        13.473054   0.285675    0.201548    39.590942  
[37m[36mINFO[0m[0m 02/06 19:22:11 | 0.734097    0.724841    0.975354    0.949398    0.169377    0.973764    0.931540    0.968017    0.931624    0.984281    0.985030    0.734097    0.724841    2200        14.820359   0.233000    0.186015    35.862536  
[37m[36mINFO[0m[0m 02/06 19:23:33 | 0.602735    0.600000    0.950703    0.920853    0.278037    0.915802    0.855746    0.952026    0.933761    0.984281    0.973054    0.602735    0.600000    2400        16.167665   0.267064    0.189552    44.270688  
[37m[36mINFO[0m[0m 02/06 19:24:45 | 0.735369    0.724841    0.976154    0.949414    0.210630    0.960342    0.921760    0.969616    0.944444    0.998503    0.982036    0.735369    0.724841    2600        17.514970   0.221938    0.181138    36.276330  
[37m[36mINFO[0m[0m 02/06 19:25:57 | 0.744911    0.737580    0.970008    0.942688    0.207572    0.950580    0.897311    0.974414    0.948718    0.985030    0.982036    0.744911    0.737580    2800        18.862275   0.206795    0.165630    38.367317  
[37m[36mINFO[0m[0m 02/06 19:27:11 | 0.733461    0.722293    0.971769    0.951242    0.203276    0.947529    0.914425    0.976013    0.957265    0.991766    0.982036    0.733461    0.722293    3000        20.209581   0.190236    0.171978    39.744030  
[37m[36mINFO[0m[0m 02/06 19:28:22 | 0.748092    0.751592    0.952442    0.930826    0.221019    0.946309    0.916870    0.934968    0.914530    0.976048    0.961078    0.748092    0.751592    3200        21.556886   0.261331    0.164863    37.884964  
[37m[36mINFO[0m[0m 02/06 19:29:34 | 0.651081    0.659873    0.966005    0.934910    0.252168    0.956071    0.902200    0.962154    0.944444    0.979790    0.958084    0.651081    0.659873    3400        22.904192   0.234840    0.185530    34.725112  
[37m[36mINFO[0m[0m 02/06 19:30:54 | 0.605598    0.615287    0.956751    0.935923    0.204770    0.943258    0.892421    0.954691    0.957265    0.972305    0.958084    0.605598    0.615287    3600        24.251497   0.198127    0.201449    39.895003  
[37m[36mINFO[0m[0m 02/06 19:32:10 | 0.712150    0.700637    0.965259    0.937708    0.218497    0.956681    0.931540    0.948827    0.929487    0.990269    0.952096    0.712150    0.700637    3800        25.598802   0.199861    0.202909    35.471575  
[37m[36mINFO[0m[0m 02/06 19:33:23 | 0.726781    0.728662    0.950818    0.916868    0.303234    0.923734    0.872861    0.950426    0.916667    0.978293    0.961078    0.726781    0.728662    4000        26.946108   0.188931    0.191489    34.744106  
