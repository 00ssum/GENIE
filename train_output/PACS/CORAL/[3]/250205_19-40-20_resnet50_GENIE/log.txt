[37m[36mINFO[0m[0m 02/05 19:40:20 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 3 --dataset PACS --trial_seed 0 --hparams_seed 0
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/PACS/CORAL/[3]/250205_19-40-20_resnet50_GENIE
	out_root: train_output/PACS/CORAL/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250205_19-40-20_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	mmd_gamma: 1.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 02/05 19:40:20 | n_steps = 5001
[37m[36mINFO[0m[0m 02/05 19:40:20 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/05 19:40:20 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/05 19:40:20 | 
[37m[36mINFO[0m[0m 02/05 19:40:20 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 02/05 19:40:20 | Test envs = [3], name = te_S
[37m[36mINFO[0m[0m 02/05 19:40:20 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 02/05 19:40:20 | Batch sizes for each domain: [32, 32, 32, 0] (total=96)
[37m[36mINFO[0m[0m 02/05 19:40:20 | steps-per-epoch for each domain: 51.22, 58.62, 41.75 -> min = 41.75
[37m[36mINFO[0m[0m 02/05 19:40:22 | # of params = 23522375
[37m[36mINFO[0m[0m 02/05 19:40:56 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/05 19:40:56 | 0.069020    0.070064    0.312409    0.288022    1.850943    0.269677    0.244499    0.252132    0.224359    0.415419    0.395210    0.069020    0.070064    0           0.000000    1.999139    0.052959    2.944271    31.340029  
[37m[36mINFO[0m[0m 02/05 19:42:24 | 0.750318    0.751592    0.969798    0.959883    0.119435    0.964002    0.943765    0.953625    0.950855    0.991766    0.985030    0.750318    0.751592    200         4.790419    0.286070    0.045888    0.287294    30.861879  
[37m[36mINFO[0m[0m 02/05 19:43:55 | 0.755725    0.745223    0.982538    0.962915    0.109212    0.976205    0.941320    0.978145    0.959402    0.993263    0.988024    0.755725    0.745223    400         9.580838    0.076349    0.027080    0.285566    33.413480  
[37m[36mINFO[0m[0m 02/05 19:45:36 | 0.735051    0.733758    0.994619    0.968312    0.094090    0.992068    0.951100    0.992537    0.965812    0.999251    0.988024    0.735051    0.733758    600         14.371257   0.045045    0.021230    0.304250    40.186418  
[37m[36mINFO[0m[0m 02/05 19:47:18 | 0.740140    0.751592    0.990364    0.970125    0.118826    0.984747    0.953545    0.989339    0.965812    0.997006    0.991018    0.740140    0.751592    800         19.161677   0.035218    0.018926    0.308959    40.805270  
[37m[36mINFO[0m[0m 02/05 19:49:05 | 0.749364    0.773248    0.996728    0.967680    0.096650    0.995729    0.946210    0.995203    0.965812    0.999251    0.991018    0.749364    0.773248    1000        23.952096   0.019967    0.016491    0.332562    40.523062  
