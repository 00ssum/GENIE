[37m[36mINFO[0m[0m 01/26 23:39:49 | Command :: /jsm0707/Large-scale/train_all.py resnet50_adam config/resnet50_adam.yaml --algorithm CORAL --test_envs 0 2 3 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_adam
	out_dir: train_output/PACS/CORAL/[0, 2, 3]/250126_23-39-49_resnet50_adam
	out_root: train_output/PACS/CORAL/[0, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 2, 3]
	trial_seed: 0
	unique_name: 250126_23-39-49_resnet50_adam
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	mmd_gamma: 1.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 01/26 23:39:49 | n_steps = 5001
[37m[36mINFO[0m[0m 01/26 23:39:49 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 01/26 23:39:49 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 01/26 23:39:49 | 
[37m[36mINFO[0m[0m 01/26 23:39:49 | Testenv name escaping te_A_P_S -> te_A_P_S
[37m[36mINFO[0m[0m 01/26 23:39:49 | Test envs = [0, 2, 3], name = te_A_P_S
[37m[36mINFO[0m[0m 01/26 23:39:49 | Train environments: [1], Test environments: [0, 2, 3]
[37m[36mINFO[0m[0m 01/26 23:39:49 | Batch sizes for each domain: [0, 32, 0, 0] (total=32)
[37m[36mINFO[0m[0m 01/26 23:39:49 | steps-per-epoch for each domain: 58.62 -> min = 58.62
[37m[36mINFO[0m[0m 01/26 23:39:50 | # of params = 23522375
[37m[36mINFO[0m[0m 01/26 23:40:16 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 01/26 23:40:16 | 0.146883    0.133116    0.229211    0.198718    1.889133    0.203173    0.180929    0.229211    0.198718    0.086078    0.080838    0.151399    0.137580    0           0.000000    2.070246    0.000000    0.968587    24.546094  
[37m[36mINFO[0m[0m 01/26 23:41:03 | 0.749452    0.734944    0.960554    0.946581    0.169420    0.707138    0.716381    0.960554    0.946581    0.872006    0.841317    0.669211    0.647134    200         3.411514    0.357395    0.000000    0.118024    23.534346  
[37m[36mINFO[0m[0m 01/26 23:41:55 | 0.733850    0.735045    0.980810    0.950855    0.160094    0.728493    0.733496    0.980810    0.950855    0.873503    0.865269    0.599555    0.606369    400         6.823028    0.104470    0.000000    0.130374    25.886638  
[37m[36mINFO[0m[0m 01/26 23:42:46 | 0.774583    0.777553    0.991471    0.963675    0.158958    0.739475    0.753056    0.991471    0.963675    0.898204    0.880240    0.686069    0.699363    600         10.234542   0.054857    0.000000    0.115385    27.427712  
[37m[36mINFO[0m[0m 01/26 23:43:36 | 0.781849    0.782491    0.995203    0.963675    0.146283    0.776693    0.794621    0.995203    0.963675    0.892964    0.880240    0.675891    0.672611    800         13.646055   0.052548    0.000000    0.126051    25.109773  
[37m[36mINFO[0m[0m 01/26 23:44:26 | 0.761773    0.766211    0.995736    0.970085    0.127635    0.749847    0.777506    0.995736    0.970085    0.857036    0.838323    0.678435    0.682803    1000        17.057569   0.035892    0.000000    0.125937    24.363667  
[37m[36mINFO[0m[0m 01/26 23:45:13 | 0.732776    0.733142    0.995203    0.965812    0.229042    0.712630    0.743276    0.995203    0.965812    0.826347    0.778443    0.659351    0.677707    1200        20.469083   0.030695    0.000000    0.117366    23.715534  
[37m[36mINFO[0m[0m 01/26 23:46:03 | 0.763830    0.754527    0.996802    0.967949    0.115788    0.721782    0.718826    0.996802    0.967949    0.843563    0.817365    0.726145    0.727389    1400        23.880597   0.070387    0.000000    0.121126    25.899183  
[37m[36mINFO[0m[0m 01/26 23:46:52 | 0.758603    0.762422    0.997335    0.967949    0.094531    0.718731    0.743276    0.997335    0.967949    0.842066    0.829341    0.715013    0.714650    1600        27.292111   0.014452    0.000000    0.114524    25.619342  
[37m[36mINFO[0m[0m 01/26 23:47:42 | 0.757246    0.751743    0.995736    0.967949    0.081670    0.696156    0.731051    0.995736    0.967949    0.836078    0.790419    0.739504    0.733758    1800        30.703625   0.037242    0.000000    0.120654    26.163419  
[37m[36mINFO[0m[0m 01/26 23:48:32 | 0.720851    0.709596    0.994670    0.978632    0.069467    0.687614    0.679707    0.994670    0.978632    0.820359    0.799401    0.654580    0.649682    2000        34.115139   0.016992    0.000000    0.122277    25.712481  
[37m[36mINFO[0m[0m 01/26 23:49:22 | 0.768144    0.764523    0.996802    0.974359    0.087563    0.730323    0.755501    0.996802    0.974359    0.862275    0.832335    0.711832    0.705732    2200        37.526652   0.032119    0.000000    0.122619    24.730046  
[37m[36mINFO[0m[0m 01/26 23:50:10 | 0.730706    0.718853    0.993603    0.965812    0.131925    0.702868    0.711491    0.993603    0.965812    0.807635    0.772455    0.681616    0.672611    2400        40.938166   0.026854    0.000000    0.119224    24.228412  
[37m[36mINFO[0m[0m 01/26 23:51:01 | 0.722113    0.716950    0.995203    0.946581    0.203488    0.690055    0.713936    0.995203    0.946581    0.803892    0.760479    0.672392    0.676433    2600        44.349680   0.029767    0.000000    0.131621    25.086765  
[37m[36mINFO[0m[0m 01/26 23:51:51 | 0.737161    0.741952    0.995736    0.963675    0.156220    0.712020    0.723716    0.995736    0.963675    0.799401    0.796407    0.700064    0.705732    2800        47.761194   0.024601    0.000000    0.122268    25.137540  
[37m[36mINFO[0m[0m 01/26 23:52:40 | 0.761648    0.768483    0.997335    0.970085    0.099394    0.741916    0.777506    0.997335    0.970085    0.848054    0.841317    0.694975    0.686624    3000        51.172708   0.025065    0.000000    0.121970    25.399754  
[37m[36mINFO[0m[0m 01/26 23:53:28 | 0.778617    0.777350    1.000000    0.978632    0.099355    0.759610    0.775061    1.000000    0.978632    0.876497    0.865269    0.699746    0.691720    3200        54.584222   0.018478    0.000000    0.114288    24.825908  
[37m[36mINFO[0m[0m 01/26 23:54:17 | 0.733510    0.744344    0.998401    0.963675    0.220827    0.692495    0.740831    0.998401    0.963675    0.797156    0.772455    0.710878    0.719745    3400        57.995736   0.012836    0.000000    0.115074    25.642282  
[37m[36mINFO[0m[0m 01/26 23:55:07 | 0.723654    0.718598    0.994670    0.967949    0.117671    0.658328    0.667482    0.994670    0.967949    0.789671    0.763473    0.722964    0.724841    3600        61.407249   0.011564    0.000000    0.120753    25.637753  
[37m[36mINFO[0m[0m 01/26 23:55:55 | 0.776121    0.778912    0.999467    0.972222    0.138307    0.723612    0.740831    0.999467    0.972222    0.850299    0.844311    0.754453    0.751592    3800        64.818763   0.032908    0.000000    0.120651    23.887756  
[37m[36mINFO[0m[0m 01/26 23:56:44 | 0.694243    0.695437    0.979744    0.942308    0.300469    0.660159    0.699267    0.979744    0.942308    0.821108    0.793413    0.601463    0.593631    4000        68.230277   0.013130    0.000000    0.120112    25.110243  
[37m[36mINFO[0m[0m 01/26 23:57:33 | 0.755996    0.766465    0.998934    0.976496    0.105532    0.737645    0.777506    0.998934    0.976496    0.839820    0.826347    0.690522    0.695541    4200        71.641791   0.018694    0.000000    0.122216    24.938282  
[37m[36mINFO[0m[0m 01/26 23:58:21 | 0.706967    0.713287    0.994136    0.965812    0.159729    0.649786    0.691932    0.994136    0.965812    0.769461    0.757485    0.701654    0.690446    4400        75.053305   0.017904    0.000000    0.121515    22.907225  
[37m[36mINFO[0m[0m 01/26 23:59:11 | 0.744561    0.734619    0.998401    0.978632    0.105896    0.694326    0.721271    0.998401    0.978632    0.827844    0.793413    0.711514    0.689172    4600        78.464819   0.010576    0.000000    0.118066    26.977005  
[37m[36mINFO[0m[0m 01/26 23:59:59 | 0.785536    0.793221    0.996269    0.974359    0.096157    0.723002    0.753056    0.996269    0.974359    0.877246    0.862275    0.756361    0.764331    4800        81.876333   0.025804    0.000000    0.117781    24.385340  
[37m[36mINFO[0m[0m 01/27 00:00:51 | 0.764734    0.755760    0.988806    0.933761    0.312621    0.684564    0.674817    0.988806    0.933761    0.863772    0.838323    0.745865    0.754140    5000        85.287846   0.011696    0.000000    0.125691    26.818855  
[37m[36mINFO[0m[0m 01/27 00:00:52 | Cumulative gradient change saved at train_output/PACS/CORAL/[0, 2, 3]/250126_23-39-49_resnet50_adam/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 01/27 00:00:53 | ---
[37m[36mINFO[0m[0m 01/27 00:00:53 | test-domain validation(oracle) = 78.554%
[37m[36mINFO[0m[0m 01/27 00:00:53 | training-domain validation(iid) = 72.085%
[37m[36mINFO[0m[0m 01/27 00:00:53 | last = 76.473%
[37m[36mINFO[0m[0m 01/27 00:00:53 | last (inD) = 93.376%
[37m[36mINFO[0m[0m 01/27 00:00:53 | training-domain validation (iid, inD) = 97.863%
[37m[36mINFO[0m[0m 01/27 00:00:53 | === Summary ===
[37m[36mINFO[0m[0m 01/27 00:00:53 | Command: /jsm0707/Large-scale/train_all.py resnet50_adam config/resnet50_adam.yaml --algorithm CORAL --test_envs 0 2 3 --dataset PACS
[37m[36mINFO[0m[0m 01/27 00:00:53 | Unique name: 250126_23-39-49_resnet50_adam
[37m[36mINFO[0m[0m 01/27 00:00:53 | Out path: train_output/PACS/CORAL/[0, 2, 3]/250126_23-39-49_resnet50_adam
[37m[36mINFO[0m[0m 01/27 00:00:53 | Algorithm: CORAL
[37m[36mINFO[0m[0m 01/27 00:00:53 | Dataset: PACS
