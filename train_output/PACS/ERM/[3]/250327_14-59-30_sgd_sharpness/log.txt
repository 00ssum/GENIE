[37m[36mINFO[0m[0m 03/27 14:59:30 | Command :: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 3 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: sgd_sharpness
	out_dir: train_output/PACS/ERM/[3]/250327_14-59-30_sgd_sharpness
	out_root: train_output/PACS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250327_14-59-30_sgd_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/27 14:59:30 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 14:59:30 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 14:59:30 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 14:59:30 | 
[37m[36mINFO[0m[0m 03/27 14:59:30 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 03/27 14:59:30 | Test envs = [3], name = te_S
[37m[36mINFO[0m[0m 03/27 14:59:30 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/27 14:59:30 | Batch sizes for each domain: [32, 32, 32, 0] (total=96)
[37m[36mINFO[0m[0m 03/27 14:59:30 | steps-per-epoch for each domain: 51.22, 58.62, 41.75 -> min = 41.75
[37m[36mINFO[0m[0m 03/27 14:59:31 | # of params = 23522375
[37m[36mINFO[0m[0m 03/27 15:00:03 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 15:00:03 | 0.072837    0.072611    0.138214    0.133365    1.963606    0.170226    0.183374    0.186034    0.153846    0.058383    0.062874    0.072837    0.072611    0           0.000000    1.999146    0.930964    31.299996  
[37m[36mINFO[0m[0m 03/27 15:01:33 | 0.048028    0.047134    0.262494    0.245211    1.858899    0.233069    0.246944    0.254264    0.222222    0.300150    0.266467    0.048028    0.047134    200         4.790419    1.895735    0.176887    32.960154  
[37m[36mINFO[0m[0m 03/27 15:03:02 | 0.051209    0.053503    0.346175    0.339010    1.752360    0.330689    0.349633    0.296908    0.314103    0.410928    0.353293    0.051209    0.053503    400         9.580838    1.791255    0.176057    32.948996  
[37m[36mINFO[0m[0m 03/27 15:04:29 | 0.063295    0.078981    0.460306    0.453817    1.619156    0.431971    0.449878    0.372601    0.369658    0.576347    0.541916    0.063295    0.078981    600         14.371257   1.667861    0.175624    31.181105  
[37m[36mINFO[0m[0m 03/27 15:05:56 | 0.109415    0.123567    0.576724    0.602551    1.450373    0.531422    0.540342    0.474947    0.512821    0.723802    0.754491    0.109415    0.123567    800         19.161677   1.523001    0.175551    30.465175  
[37m[36mINFO[0m[0m 03/27 15:07:24 | 0.137405    0.149045    0.673685    0.698741    1.253352    0.614399    0.662592    0.569829    0.583333    0.836826    0.850299    0.137405    0.149045    1000        23.952096   1.340461    0.176056    31.391603  
[37m[36mINFO[0m[0m 03/27 15:08:51 | 0.181934    0.183439    0.739873    0.768552    1.043850    0.701037    0.750611    0.615139    0.653846    0.903443    0.901198    0.181934    0.183439    1200        28.742515   1.137545    0.176115    30.958890  
[37m[36mINFO[0m[0m 03/27 15:10:20 | 0.217557    0.229299    0.781395    0.816306    0.849547    0.748017    0.801956    0.665778    0.700855    0.930389    0.946108    0.217557    0.229299    1400        33.532934   0.935453    0.176599    32.174861  
[37m[36mINFO[0m[0m 03/27 15:11:48 | 0.261768    0.278981    0.805728    0.840454    0.692550    0.767541    0.823961    0.698294    0.739316    0.951347    0.958084    0.261768    0.278981    1600        38.323353   0.777658    0.176270    31.447502  
[37m[36mINFO[0m[0m 03/27 15:13:15 | 0.295165    0.312102    0.831217    0.860613    0.572519    0.802318    0.845966    0.737740    0.762821    0.953593    0.973054    0.295165    0.312102    1800        43.113772   0.645349    0.176246    30.902019  
[37m[36mINFO[0m[0m 03/27 15:14:44 | 0.316158    0.332484    0.851672    0.871590    0.485695    0.838926    0.863081    0.754264    0.775641    0.961826    0.976048    0.316158    0.332484    2000        47.904192   0.554881    0.175707    32.183468  
[37m[36mINFO[0m[0m 03/27 15:16:13 | 0.342239    0.352866    0.858798    0.883683    0.426312    0.842587    0.872861    0.765991    0.799145    0.967814    0.979042    0.342239    0.352866    2200        52.694611   0.475450    0.176097    32.889765  
[37m[36mINFO[0m[0m 03/27 15:17:41 | 0.352099    0.361783    0.871724    0.896084    0.380692    0.862721    0.889976    0.788380    0.816239    0.964072    0.982036    0.352099    0.361783    2400        57.485030   0.434415    0.176210    31.240107  
[37m[36mINFO[0m[0m 03/27 15:19:09 | 0.361641    0.377070    0.875147    0.908911    0.345453    0.864552    0.907090    0.791578    0.837607    0.969311    0.982036    0.361641    0.377070    2600        62.275449   0.398139    0.175491    31.759692  
[37m[36mINFO[0m[0m 03/27 15:20:36 | 0.384542    0.384713    0.882700    0.903389    0.322231    0.867602    0.887531    0.813433    0.837607    0.967066    0.985030    0.384542    0.384713    2800        67.065868   0.364046    0.175743    31.104912  
[37m[36mINFO[0m[0m 03/27 15:22:03 | 0.397583    0.400000    0.891153    0.912736    0.301910    0.885906    0.899756    0.814499    0.850427    0.973054    0.988024    0.397583    0.400000    3000        71.856287   0.335225    0.176834    30.609164  
[37m[36mINFO[0m[0m 03/27 15:23:31 | 0.409033    0.407643    0.895712    0.908786    0.285968    0.892007    0.897311    0.819829    0.844017    0.975299    0.985030    0.409033    0.407643    3200        76.646707   0.319511    0.175609    31.355927  
[37m[36mINFO[0m[0m 03/27 15:24:59 | 0.395992    0.414013    0.899822    0.913162    0.273094    0.888347    0.899756    0.835821    0.854701    0.975299    0.985030    0.395992    0.414013    3400        81.437126   0.301492    0.175144    32.189333  
[37m[36mINFO[0m[0m 03/27 15:26:27 | 0.419211    0.416561    0.903574    0.918963    0.258543    0.887126    0.902200    0.847548    0.869658    0.976048    0.985030    0.419211    0.416561    3600        86.227545   0.285745    0.175536    31.224867  
[37m[36mINFO[0m[0m 03/27 15:27:55 | 0.437341    0.439490    0.908047    0.924154    0.248693    0.898719    0.907090    0.844883    0.880342    0.980539    0.985030    0.437341    0.439490    3800        91.017964   0.276379    0.175399    31.962217  
[37m[36mINFO[0m[0m 03/27 15:29:23 | 0.438295    0.439490    0.907438    0.928024    0.241546    0.898719    0.914425    0.847548    0.884615    0.976048    0.985030    0.438295    0.439490    4000        95.808383   0.269922    0.175352    31.129570  
[37m[36mINFO[0m[0m 03/27 15:30:49 | 0.433206    0.436943    0.916348    0.928530    0.233724    0.908481    0.909535    0.859275    0.891026    0.981287    0.985030    0.433206    0.436943    4200        100.598802  0.263001    0.176184    30.334186  
[37m[36mINFO[0m[0m 03/27 15:32:17 | 0.456743    0.467516    0.914192    0.929345    0.228856    0.902990    0.911980    0.863539    0.891026    0.976048    0.985030    0.456743    0.467516    4400        105.389222  0.250534    0.175072    31.803834  
[37m[36mINFO[0m[0m 03/27 15:33:44 | 0.464377    0.466242    0.912945    0.927715    0.220729    0.896888    0.907090    0.861407    0.891026    0.980539    0.985030    0.464377    0.466242    4600        110.179641  0.244655    0.175809    30.701170  
[37m[36mINFO[0m[0m 03/27 15:35:13 | 0.470420    0.482803    0.919686    0.927715    0.216167    0.908481    0.907090    0.871535    0.891026    0.979042    0.985030    0.470420    0.482803    4800        114.970060  0.230742    0.175657    32.141534  
[37m[36mINFO[0m[0m 03/27 15:36:41 | 0.464377    0.478981    0.918743    0.930263    0.213143    0.909701    0.916870    0.866738    0.888889    0.979790    0.985030    0.464377    0.478981    5000        119.760479  0.221943    0.176020    31.997765  
[37m[36mINFO[0m[0m 03/27 15:37:03 | Cumulative gradient change saved at train_output/PACS/ERM/[3]/250327_14-59-30_sgd_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 15:37:04 | ---
[37m[36mINFO[0m[0m 03/27 15:37:04 | test-domain validation(oracle) = 47.042%
[37m[36mINFO[0m[0m 03/27 15:37:04 | training-domain validation(iid) = 46.438%
[37m[36mINFO[0m[0m 03/27 15:37:04 | last = 46.438%
[37m[36mINFO[0m[0m 03/27 15:37:04 | last (inD) = 93.026%
[37m[36mINFO[0m[0m 03/27 15:37:04 | training-domain validation (iid, inD) = 93.026%
[37m[36mINFO[0m[0m 03/27 15:37:04 | === Summary ===
[37m[36mINFO[0m[0m 03/27 15:37:04 | Command: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 3 --dataset PACS
[37m[36mINFO[0m[0m 03/27 15:37:04 | Unique name: 250327_14-59-30_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 15:37:04 | Out path: train_output/PACS/ERM/[3]/250327_14-59-30_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 15:37:04 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 15:37:04 | Dataset: PACS
