[37m[36mINFO[0m[0m 03/27 18:34:47 | Command :: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 3 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: GENIE_sharpness
	out_dir: train_output/PACS/ERM/[3]/250327_18-34-47_GENIE_sharpness
	out_root: train_output/PACS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250327_18-34-47_GENIE_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/27 18:34:47 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 18:34:47 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 18:34:47 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 18:34:47 | 
[37m[36mINFO[0m[0m 03/27 18:34:47 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 03/27 18:34:47 | Test envs = [3], name = te_S
[37m[36mINFO[0m[0m 03/27 18:34:47 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/27 18:34:47 | Batch sizes for each domain: [32, 32, 32, 0] (total=96)
[37m[36mINFO[0m[0m 03/27 18:34:47 | steps-per-epoch for each domain: 51.22, 58.62, 41.75 -> min = 41.75
[37m[36mINFO[0m[0m 03/27 18:34:48 | # of params = 23522375
[37m[36mINFO[0m[0m 03/27 18:35:20 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 18:35:20 | 0.068066    0.066242    0.324555    0.299834    1.844819    0.273337    0.254279    0.269190    0.235043    0.431138    0.410180    0.068066    0.066242    0           0.000000    1.999146    1.037501    30.475325  
[37m[36mINFO[0m[0m 03/27 18:36:54 | 0.613550    0.620382    0.947250    0.940786    0.181756    0.937767    0.926650    0.926439    0.916667    0.977545    0.979042    0.613550    0.620382    200         4.790419    0.279455    0.211143    31.141630  
[37m[36mINFO[0m[0m 03/27 18:38:31 | 0.748410    0.740127    0.981884    0.955199    0.124482    0.981696    0.933985    0.966951    0.946581    0.997006    0.985030    0.748410    0.740127    400         9.580838    0.077926    0.209572    33.361750  
[37m[36mINFO[0m[0m 03/27 18:40:05 | 0.758588    0.763057    0.992372    0.966682    0.097280    0.990238    0.946210    0.989872    0.965812    0.997006    0.988024    0.758588    0.763057    600         14.371257   0.046799    0.210845    30.691092  
[37m[36mINFO[0m[0m 03/27 18:41:39 | 0.688613    0.680255    0.985185    0.952734    0.170279    0.981696    0.936430    0.981343    0.948718    0.992515    0.973054    0.688613    0.680255    800         19.161677   0.033221    0.211476    30.655620  
[37m[36mINFO[0m[0m 03/27 18:43:14 | 0.745547    0.771975    0.983303    0.965341    0.117146    0.981086    0.958435    0.984542    0.961538    0.984281    0.976048    0.745547    0.771975    1000        23.952096   0.021169    0.210914    31.182586  
[37m[36mINFO[0m[0m 03/27 18:44:48 | 0.726463    0.728662    0.986031    0.959068    0.159206    0.982306    0.941320    0.980277    0.950855    0.995509    0.985030    0.726463    0.728662    1200        28.742515   0.021907    0.211584    31.183725  
[37m[36mINFO[0m[0m 03/27 18:46:24 | 0.741094    0.740127    0.991331    0.955972    0.176305    0.988408    0.938875    0.993070    0.952991    0.992515    0.976048    0.741094    0.740127    1400        33.532934   0.014601    0.210553    32.750469  
[37m[36mINFO[0m[0m 03/27 18:48:00 | 0.760496    0.782166    0.996501    0.966160    0.151647    0.998780    0.960880    0.991471    0.946581    0.999251    0.991018    0.760496    0.782166    1600        38.323353   0.011441    0.208654    32.703201  
[37m[36mINFO[0m[0m 03/27 18:49:35 | 0.758270    0.752866    0.997258    0.969451    0.112497    0.998170    0.951100    0.993603    0.972222    1.000000    0.985030    0.758270    0.752866    1800        43.113772   0.016840    0.208471    31.602391  
[37m[36mINFO[0m[0m 03/27 18:51:11 | 0.752545    0.763057    0.999009    0.968026    0.126707    0.997559    0.951100    0.999467    0.967949    1.000000    0.985030    0.752545    0.763057    2000        47.904192   0.006757    0.217129    31.184157  
[37m[36mINFO[0m[0m 03/27 18:52:45 | 0.762723    0.764331    0.995444    0.963813    0.146875    0.991458    0.955990    0.997868    0.959402    0.997006    0.976048    0.762723    0.764331    2200        52.694611   0.007274    0.211191    31.129495  
[37m[36mINFO[0m[0m 03/27 18:54:21 | 0.779580    0.785987    0.998247    0.968209    0.145151    0.996339    0.948655    0.998401    0.967949    1.000000    0.988024    0.779580    0.785987    2400        57.485030   0.006419    0.210757    32.246542  
[37m[36mINFO[0m[0m 03/27 18:55:54 | 0.765903    0.774522    0.997977    0.970552    0.145419    0.997559    0.953545    0.997868    0.970085    0.998503    0.988024    0.765903    0.774522    2600        62.275449   0.008220    0.209241    30.022609  
[37m[36mINFO[0m[0m 03/27 18:57:28 | 0.769402    0.778344    0.999289    0.969130    0.136203    1.000000    0.970660    0.997868    0.963675    1.000000    0.973054    0.769402    0.778344    2800        67.065868   0.005026    0.209664    30.878560  
[37m[36mINFO[0m[0m 03/27 18:59:01 | 0.739822    0.751592    0.998705    0.965158    0.142307    0.998780    0.960880    0.997335    0.961538    1.000000    0.973054    0.739822    0.751592    3000        71.856287   0.004387    0.210086    30.042872  
[37m[36mINFO[0m[0m 03/27 19:00:35 | 0.794529    0.794904    0.998831    0.964320    0.152536    0.997559    0.951100    0.998934    0.965812    1.000000    0.976048    0.794529    0.794904    3200        76.646707   0.004071    0.209254    30.625392  
[37m[36mINFO[0m[0m 03/27 19:02:10 | 0.748728    0.757962    0.999416    0.976397    0.111928    0.998780    0.970660    0.999467    0.976496    1.000000    0.982036    0.748728    0.757962    3400        81.437126   0.006455    0.208023    31.948912  
[37m[36mINFO[0m[0m 03/27 19:03:43 | 0.758588    0.759236    0.999086    0.969842    0.135995    0.999390    0.970660    0.997868    0.965812    1.000000    0.973054    0.758588    0.759236    3600        86.227545   0.005252    0.209500    30.404723  
[37m[36mINFO[0m[0m 03/27 19:05:17 | 0.746183    0.751592    0.998556    0.973629    0.128661    0.996949    0.965770    0.999467    0.970085    0.999251    0.985030    0.746183    0.751592    3800        91.017964   0.002181    0.207946    30.848690  
[37m[36mINFO[0m[0m 03/27 19:06:52 | 0.767812    0.784713    0.996934    0.970369    0.118560    0.993899    0.955990    0.998401    0.970085    0.998503    0.985030    0.767812    0.784713    4000        95.808383   0.006696    0.208177    32.091486  
[37m[36mINFO[0m[0m 03/27 19:08:27 | 0.785305    0.782166    0.999822    0.973343    0.135828    1.000000    0.965770    0.999467    0.972222    1.000000    0.982036    0.785305    0.782166    4200        100.598802  0.002995    0.210060    31.827268  
[37m[36mINFO[0m[0m 03/27 19:10:02 | 0.785305    0.793631    0.998628    0.977681    0.126152    0.996949    0.970660    0.998934    0.974359    1.000000    0.988024    0.785305    0.793631    4400        105.389222  0.005282    0.211345    31.707391  
[37m[36mINFO[0m[0m 03/27 19:11:36 | 0.726145    0.761783    0.995132    0.957865    0.188442    0.993289    0.936430    0.993603    0.955128    0.998503    0.982036    0.726145    0.761783    4600        110.179641  0.004318    0.209628    30.449772  
[37m[36mINFO[0m[0m 03/27 19:13:10 | 0.778626    0.785987    0.999212    0.977152    0.104909    0.998170    0.968215    0.999467    0.972222    1.000000    0.991018    0.778626    0.785987    4800        114.970060  0.003875    0.209358    31.290910  
[37m[36mINFO[0m[0m 03/27 19:14:45 | 0.760496    0.774522    0.999060    0.969576    0.105460    0.998780    0.960880    0.998401    0.965812    1.000000    0.982036    0.760496    0.774522    5000        119.760479  0.006071    0.212101    31.344823  
[37m[36mINFO[0m[0m 03/27 19:15:07 | Cumulative gradient change saved at train_output/PACS/ERM/[3]/250327_18-34-47_GENIE_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 19:15:08 | ---
[37m[36mINFO[0m[0m 03/27 19:15:08 | test-domain validation(oracle) = 79.453%
[37m[36mINFO[0m[0m 03/27 19:15:08 | training-domain validation(iid) = 78.531%
[37m[36mINFO[0m[0m 03/27 19:15:08 | last = 76.050%
[37m[36mINFO[0m[0m 03/27 19:15:08 | last (inD) = 96.958%
[37m[36mINFO[0m[0m 03/27 19:15:08 | training-domain validation (iid, inD) = 97.768%
[37m[36mINFO[0m[0m 03/27 19:15:08 | === Summary ===
[37m[36mINFO[0m[0m 03/27 19:15:08 | Command: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 3 --dataset PACS
[37m[36mINFO[0m[0m 03/27 19:15:08 | Unique name: 250327_18-34-47_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 19:15:08 | Out path: train_output/PACS/ERM/[3]/250327_18-34-47_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 19:15:08 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 19:15:08 | Dataset: PACS
