[37m[36mINFO[0m[0m 03/25 11:52:28 | Command :: /jsm0707/GENIE/train_all.py resnet50_Sign_GENIE config/resnet50_Sign_GENIE.yaml --algorithm ERM --test_envs 0 1 3 --dataset PACS --trial_seed 2 --hparams_seed 5
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_Sign_GENIE.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 5
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_Sign_GENIE
	out_dir: train_output/PACS/ERM/[0, 1, 3]/250325_11-52-28_resnet50_Sign_GENIE
	out_root: train_output/PACS/ERM/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 2
	unique_name: 250325_11-52-28_resnet50_Sign_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: sign_genie
	freeze_bn: False
	pretrained: True
	lr: 0.0002982828768978312
	batch_size: 9
	weight_decay: 0.0003180957897867641
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/25 11:52:28 | n_steps = 5001
[37m[36mINFO[0m[0m 03/25 11:52:28 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/25 11:52:28 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/25 11:52:28 | 
[37m[36mINFO[0m[0m 03/25 11:52:29 | Testenv name escaping te_A_C_S -> te_A_C_S
[37m[36mINFO[0m[0m 03/25 11:52:29 | Test envs = [0, 1, 3], name = te_A_C_S
[37m[36mINFO[0m[0m 03/25 11:52:29 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 03/25 11:52:29 | Batch sizes for each domain: [0, 0, 9, 0] (total=9)
[37m[36mINFO[0m[0m 03/25 11:52:29 | steps-per-epoch for each domain: 148.44 -> min = 148.44
[37m[36mINFO[0m[0m 03/25 11:52:29 | # of params = 23522375
[37m[36mINFO[0m[0m 03/25 11:52:52 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/25 11:52:52 | 0.096890    0.092032    0.256737    0.218563    2.524480    0.146431    0.136919    0.124200    0.117521    0.256737    0.218563    0.020038    0.021656    0           0.000000    1.829893    1.013501    22.005497  
[37m[36mINFO[0m[0m 03/25 11:53:41 | 0.468316    0.485313    0.840569    0.829341    0.400230    0.503966    0.533007    0.405117    0.433761    0.840569    0.829341    0.495865    0.489172    200         1.347305    0.638667    0.127004    23.460632  
[37m[36mINFO[0m[0m 03/25 11:54:34 | 0.424566    0.436325    0.921407    0.934132    0.258427    0.540574    0.569682    0.304371    0.335470    0.921407    0.934132    0.428753    0.403822    400         2.694611    0.305607    0.147569    23.259032  
[37m[36mINFO[0m[0m 03/25 11:55:25 | 0.400335    0.402601    0.943114    0.919162    0.232248    0.608298    0.579462    0.277186    0.316239    0.943114    0.919162    0.315522    0.312102    600         4.041916    0.200625    0.139916    22.678963  
[37m[36mINFO[0m[0m 03/25 11:56:11 | 0.386810    0.393506    0.967066    0.934132    0.209506    0.593045    0.603912    0.299574    0.326923    0.967066    0.934132    0.267812    0.249682    800         5.389222    0.103325    0.119439    22.404352  
[37m[36mINFO[0m[0m 03/25 11:57:00 | 0.473455    0.489285    0.964072    0.955090    0.151103    0.579012    0.596577    0.435501    0.457265    0.964072    0.955090    0.405852    0.414013    1000        6.736527    0.091507    0.132870    22.473949  
[37m[36mINFO[0m[0m 03/25 11:57:49 | 0.494352    0.495620    0.913174    0.883234    0.516532    0.498475    0.491443    0.453092    0.480769    0.913174    0.883234    0.531489    0.514650    1200        8.083832    0.115094    0.132428    22.049139  
[37m[36mINFO[0m[0m 03/25 11:58:35 | 0.391825    0.403683    0.976048    0.928144    0.179806    0.632703    0.633252    0.277186    0.314103    0.976048    0.928144    0.265585    0.263694    1400        9.431138    0.185747    0.123874    22.011192  
[37m[36mINFO[0m[0m 03/25 11:59:26 | 0.469676    0.499119    0.952096    0.898204    0.292150    0.578401    0.616137    0.451493    0.485043    0.952096    0.898204    0.379135    0.396178    1600        10.778443   0.106868    0.131236    24.135441  
[37m[36mINFO[0m[0m 03/25 12:00:17 | 0.411894    0.431475    0.929641    0.916168    0.329839    0.546065    0.589242    0.355011    0.380342    0.929641    0.916168    0.334606    0.324841    1800        12.125749   0.068658    0.141249    22.941363  
[37m[36mINFO[0m[0m 03/25 12:01:03 | 0.376562    0.383287    0.967814    0.940120    0.142161    0.524710    0.530562    0.348614    0.363248    0.967814    0.940120    0.256361    0.256051    2000        13.473054   0.118710    0.114156    23.211790  
[37m[36mINFO[0m[0m 03/25 12:01:51 | 0.467101    0.492483    0.991018    0.949102    0.123206    0.574741    0.618582    0.406716    0.442308    0.991018    0.949102    0.419847    0.416561    2200        14.820359   0.082784    0.120907    23.584929  
[37m[36mINFO[0m[0m 03/25 12:02:45 | 0.390733    0.401916    0.967814    0.940120    0.234395    0.561318    0.594132    0.342751    0.356838    0.967814    0.940120    0.268130    0.254777    2400        16.167665   0.085760    0.148490    24.367529  
[37m[36mINFO[0m[0m 03/25 12:03:34 | 0.478713    0.492203    0.986527    0.955090    0.154754    0.572910    0.596577    0.422708    0.431624    0.986527    0.955090    0.440522    0.448408    2600        17.514970   0.150065    0.133639    22.828289  
[37m[36mINFO[0m[0m 03/25 12:04:26 | 0.409655    0.429735    0.930389    0.904192    0.239874    0.450885    0.462103    0.400853    0.455128    0.930389    0.904192    0.377226    0.371975    2800        18.862275   0.143135    0.135566    23.967833  
[37m[36mINFO[0m[0m 03/25 12:05:15 | 0.394342    0.411237    0.995509    0.958084    0.147325    0.610738    0.635697    0.323561    0.354701    0.995509    0.958084    0.248728    0.243312    3000        20.209581   0.063463    0.138037    22.160868  
[37m[36mINFO[0m[0m 03/25 12:06:03 | 0.362217    0.366118    0.991766    0.961078    0.092270    0.620500    0.640587    0.263859    0.269231    0.991766    0.961078    0.202290    0.188535    3200        21.556886   0.120748    0.130629    21.407216  
