[37m[36mINFO[0m[0m 03/27 17:54:17 | Command :: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 2 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: GENIE_sharpness
	out_dir: train_output/PACS/ERM/[2]/250327_17-54-17_GENIE_sharpness
	out_root: train_output/PACS/ERM/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 0
	unique_name: 250327_17-54-17_GENIE_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/27 17:54:17 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 17:54:17 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 17:54:17 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 17:54:17 | 
[37m[36mINFO[0m[0m 03/27 17:54:17 | Testenv name escaping te_P -> te_P
[37m[36mINFO[0m[0m 03/27 17:54:17 | Test envs = [2], name = te_P
[37m[36mINFO[0m[0m 03/27 17:54:17 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 03/27 17:54:17 | Batch sizes for each domain: [32, 32, 0, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 17:54:17 | steps-per-epoch for each domain: 51.22, 58.62, 98.25 -> min = 51.22
[37m[36mINFO[0m[0m 03/27 17:54:18 | # of params = 23522375
[37m[36mINFO[0m[0m 03/27 17:54:49 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 17:54:49 | 0.167665    0.185629    0.201773    0.206748    1.881724    0.213545    0.229829    0.215885    0.228632    0.167665    0.185629    0.175891    0.161783    0           0.000000    1.977479    1.005232    30.638906  
[37m[36mINFO[0m[0m 03/27 17:56:25 | 0.974551    0.976048    0.950814    0.942073    0.165745    0.955461    0.946210    0.948827    0.961538    0.974551    0.976048    0.948155    0.918471    200         3.904820    0.439743    0.209899    32.951624  
[37m[36mINFO[0m[0m 03/27 17:58:00 | 0.964820    0.958084    0.962058    0.946559    0.167295    0.967053    0.948655    0.964286    0.957265    0.964820    0.958084    0.954835    0.933758    400         7.809640    0.140870    0.210532    30.625557  
[37m[36mINFO[0m[0m 03/27 17:59:34 | 0.972305    0.964072    0.975939    0.956374    0.130841    0.986577    0.955990    0.976546    0.961538    0.972305    0.964072    0.964695    0.951592    600         11.714460   0.084142    0.208356    31.578587  
[37m[36mINFO[0m[0m 03/27 18:01:08 | 0.974551    0.964072    0.981713    0.951497    0.169013    0.992068    0.951100    0.987740    0.965812    0.974551    0.964072    0.965331    0.937580    800         15.619280   0.053012    0.207844    31.033005  
[37m[36mINFO[0m[0m 03/27 18:02:41 | 0.976048    0.976048    0.989049    0.947594    0.176812    0.989018    0.936430    0.994670    0.963675    0.976048    0.976048    0.983461    0.942675    1000        19.524100   0.049115    0.209195    30.540234  
[37m[36mINFO[0m[0m 03/27 18:04:15 | 0.963323    0.952096    0.982295    0.957189    0.171534    0.984137    0.948655    0.973881    0.963675    0.963323    0.952096    0.988868    0.959236    1200        23.428920   0.029305    0.208822    31.030916  
[37m[36mINFO[0m[0m 03/27 18:05:51 | 0.967814    0.955090    0.991372    0.951908    0.185211    0.993899    0.941320    0.988806    0.961538    0.967814    0.955090    0.991412    0.952866    1400        27.333740   0.027348    0.209032    32.359962  
[37m[36mINFO[0m[0m 03/27 18:07:26 | 0.977545    0.964072    0.993005    0.964572    0.153126    0.998780    0.958435    0.992004    0.972222    0.977545    0.964072    0.988232    0.963057    1600        31.238560   0.018604    0.209724    32.102750  
[37m[36mINFO[0m[0m 03/27 18:08:59 | 0.976796    0.964072    0.996966    0.961284    0.131371    0.996949    0.958435    0.998401    0.957265    0.976796    0.964072    0.995547    0.968153    1800        35.143380   0.019462    0.208411    30.403303  
[37m[36mINFO[0m[0m 03/27 18:10:36 | 0.975299    0.961078    0.997487    0.966811    0.141748    0.997559    0.960880    0.998401    0.976496    0.975299    0.961078    0.996501    0.963057    2000        39.048200   0.014504    0.207722    33.892379  
[37m[36mINFO[0m[0m 03/27 18:12:12 | 0.977545    0.970060    0.995869    0.962277    0.150396    0.992068    0.951100    0.998401    0.976496    0.977545    0.970060    0.997137    0.959236    2200        42.953020   0.017406    0.209030    33.212255  
[37m[36mINFO[0m[0m 03/27 18:13:47 | 0.975299    0.970060    0.996702    0.965400    0.126709    0.998170    0.960880    0.998934    0.978632    0.975299    0.970060    0.993003    0.956688    2400        46.857840   0.014499    0.207354    31.796340  
[37m[36mINFO[0m[0m 03/27 18:15:20 | 0.979042    0.952096    0.998630    0.966030    0.147513    0.999390    0.955990    1.000000    0.976496    0.979042    0.952096    0.996501    0.965605    2600        50.762660   0.010538    0.207702    30.230669  
[37m[36mINFO[0m[0m 03/27 18:16:56 | 0.980539    0.973054    0.998178    0.962633    0.168652    0.998780    0.955990    0.998934    0.976496    0.980539    0.973054    0.996819    0.955414    2800        54.667480   0.012192    0.210659    33.509083  
[37m[36mINFO[0m[0m 03/27 18:18:33 | 0.976796    0.967066    0.996290    0.961969    0.155967    0.994509    0.953545    0.996269    0.980769    0.976796    0.967066    0.998092    0.951592    3000        58.572300   0.013385    0.209745    33.239061  
[37m[36mINFO[0m[0m 03/27 18:20:07 | 0.963323    0.961078    0.995456    0.959072    0.185326    0.995729    0.955990    0.994136    0.965812    0.963323    0.961078    0.996501    0.955414    3200        62.477120   0.012674    0.210210    30.818494  
[37m[36mINFO[0m[0m 03/27 18:21:41 | 0.976048    0.964072    0.997229    0.961976    0.158982    0.998170    0.951100    0.997335    0.967949    0.976048    0.964072    0.996183    0.966879    3400        66.381940   0.007883    0.210050    30.654509  
[37m[36mINFO[0m[0m 03/27 18:23:18 | 0.977545    0.973054    0.999195    0.961209    0.158504    0.999390    0.955990    0.999467    0.972222    0.977545    0.973054    0.998728    0.955414    3600        70.286760   0.008226    0.210149    33.430265  
[37m[36mINFO[0m[0m 03/27 18:24:53 | 0.977545    0.976048    0.998636    0.971427    0.148198    0.998780    0.965770    0.998401    0.982906    0.977545    0.976048    0.998728    0.965605    3800        74.191580   0.005316    0.208738    31.985904  
[37m[36mINFO[0m[0m 03/27 18:26:29 | 0.970060    0.955090    0.997310    0.967626    0.135698    0.997559    0.963325    0.997868    0.976496    0.970060    0.955090    0.996501    0.963057    4000        78.096400   0.011002    0.210026    33.463098  
[37m[36mINFO[0m[0m 03/27 18:28:04 | 0.979790    0.973054    0.998602    0.961531    0.178407    0.998780    0.953545    0.998934    0.974359    0.979790    0.973054    0.998092    0.956688    4200        82.001220   0.003780    0.208751    31.107100  
[37m[36mINFO[0m[0m 03/27 18:29:38 | 0.974551    0.955090    0.994335    0.959613    0.207090    0.997559    0.958435    0.994670    0.970085    0.974551    0.955090    0.990776    0.950318    4400        85.906040   0.004374    0.208261    31.409876  
[37m[36mINFO[0m[0m 03/27 18:31:11 | 0.975299    0.976048    0.999335    0.967832    0.154985    0.999390    0.968215    0.998934    0.972222    0.975299    0.976048    0.999682    0.963057    4600        89.810860   0.008219    0.209285    30.406653  
[37m[36mINFO[0m[0m 03/27 18:32:45 | 0.966317    0.952096    0.996484    0.966400    0.225341    0.995729    0.960880    0.996269    0.982906    0.966317    0.952096    0.997455    0.955414    4800        93.715680   0.002448    0.207961    31.245721  
[37m[36mINFO[0m[0m 03/27 18:34:21 | 0.971557    0.961078    0.998309    0.965756    0.164573    0.999390    0.965770    0.998401    0.978632    0.971557    0.961078    0.997137    0.952866    5000        97.620500   0.005903    0.210023    32.065462  
[37m[36mINFO[0m[0m 03/27 18:34:42 | Cumulative gradient change saved at train_output/PACS/ERM/[2]/250327_17-54-17_GENIE_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 18:34:43 | ---
[37m[36mINFO[0m[0m 03/27 18:34:43 | test-domain validation(oracle) = 97.455%
[37m[36mINFO[0m[0m 03/27 18:34:43 | training-domain validation(iid) = 97.754%
[37m[36mINFO[0m[0m 03/27 18:34:43 | last = 97.156%
[37m[36mINFO[0m[0m 03/27 18:34:43 | last (inD) = 96.576%
[37m[36mINFO[0m[0m 03/27 18:34:43 | training-domain validation (iid, inD) = 97.143%
[37m[36mINFO[0m[0m 03/27 18:34:44 | === Summary ===
[37m[36mINFO[0m[0m 03/27 18:34:44 | Command: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 2 --dataset PACS
[37m[36mINFO[0m[0m 03/27 18:34:44 | Unique name: 250327_17-54-17_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 18:34:44 | Out path: train_output/PACS/ERM/[2]/250327_17-54-17_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 18:34:44 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 18:34:44 | Dataset: PACS
