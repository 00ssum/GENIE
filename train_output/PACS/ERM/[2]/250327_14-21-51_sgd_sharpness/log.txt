[37m[36mINFO[0m[0m 03/27 14:21:51 | Command :: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 2 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: sgd_sharpness
	out_dir: train_output/PACS/ERM/[2]/250327_14-21-51_sgd_sharpness
	out_root: train_output/PACS/ERM/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 0
	unique_name: 250327_14-21-51_sgd_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/27 14:21:51 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 14:21:51 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 14:21:51 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 14:21:51 | 
[37m[36mINFO[0m[0m 03/27 14:21:51 | Testenv name escaping te_P -> te_P
[37m[36mINFO[0m[0m 03/27 14:21:51 | Test envs = [2], name = te_P
[37m[36mINFO[0m[0m 03/27 14:21:51 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 03/27 14:21:51 | Batch sizes for each domain: [32, 32, 0, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 14:21:51 | steps-per-epoch for each domain: 51.22, 58.62, 98.25 -> min = 51.22
[37m[36mINFO[0m[0m 03/27 14:21:52 | # of params = 23522375
[37m[36mINFO[0m[0m 03/27 14:22:25 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 14:22:25 | 0.063623    0.062874    0.144075    0.137563    1.970511    0.171446    0.185819    0.186034    0.151709    0.063623    0.062874    0.074746    0.075159    0           0.000000    1.977479    0.913533    31.612255  
[37m[36mINFO[0m[0m 03/27 14:23:54 | 0.098054    0.098802    0.217565    0.218439    1.879001    0.203173    0.222494    0.247868    0.220085    0.098054    0.098802    0.201654    0.212739    200         3.904820    1.922221    0.175865    32.428940  
[37m[36mINFO[0m[0m 03/27 14:25:22 | 0.129491    0.128743    0.250682    0.243677    1.813618    0.233679    0.244499    0.293177    0.252137    0.129491    0.128743    0.225191    0.234395    400         7.809640    1.847594    0.175622    32.306628  
[37m[36mINFO[0m[0m 03/27 14:26:49 | 0.163174    0.170659    0.319475    0.312821    1.741647    0.314826    0.293399    0.354478    0.341880    0.163174    0.170659    0.289122    0.303185    600         11.714460   1.779446    0.175211    30.452739  
[37m[36mINFO[0m[0m 03/27 14:28:19 | 0.263473    0.245509    0.404378    0.432640    1.657981    0.399634    0.403423    0.421642    0.435897    0.263473    0.245509    0.391858    0.458599    800         15.619280   1.703545    0.176500    33.311986  
[37m[36mINFO[0m[0m 03/27 14:29:46 | 0.415419    0.416168    0.474102    0.493847    1.555180    0.480781    0.469438    0.489872    0.500000    0.415419    0.416168    0.451654    0.512102    1000        19.524100   1.607755    0.174695    31.466988  
[37m[36mINFO[0m[0m 03/27 14:31:14 | 0.578593    0.562874    0.529170    0.548348    1.433544    0.538743    0.537897    0.557036    0.555556    0.578593    0.562874    0.491730    0.551592    1200        23.428920   1.497008    0.175734    31.192455  
[37m[36mINFO[0m[0m 03/27 14:32:41 | 0.654192    0.646707    0.574475    0.583585    1.304302    0.585113    0.574572    0.607143    0.594017    0.654192    0.646707    0.531170    0.582166    1400        27.333740   1.375303    0.177183    31.259755  
[37m[36mINFO[0m[0m 03/27 14:34:08 | 0.711078    0.688623    0.621080    0.635587    1.164657    0.643685    0.640587    0.650853    0.655983    0.711078    0.688623    0.568702    0.610191    1600        31.238560   1.234433    0.175792    30.526301  
[37m[36mINFO[0m[0m 03/27 14:35:36 | 0.753743    0.721557    0.671585    0.688870    1.023189    0.700427    0.723716    0.687100    0.700855    0.753743    0.721557    0.627226    0.642038    1800        35.143380   1.100965    0.175269    31.366042  
[37m[36mINFO[0m[0m 03/27 14:37:04 | 0.802395    0.763473    0.700307    0.725572    0.897805    0.738255    0.765281    0.705224    0.728632    0.802395    0.763473    0.657443    0.682803    2000        39.048200   0.975625    0.175618    31.558702  
[37m[36mINFO[0m[0m 03/27 14:38:32 | 0.830090    0.793413    0.737398    0.750577    0.792053    0.776693    0.792176    0.743070    0.750000    0.830090    0.793413    0.692430    0.709554    2200        42.953020   0.860966    0.176032    32.130544  
[37m[36mINFO[0m[0m 03/27 14:40:00 | 0.875000    0.832335    0.762083    0.766110    0.704092    0.812691    0.806846    0.761727    0.762821    0.875000    0.832335    0.711832    0.728662    2400        46.857840   0.768822    0.175362    30.940217  
[37m[36mINFO[0m[0m 03/27 14:41:27 | 0.891467    0.853293    0.779191    0.784307    0.645899    0.831605    0.833741    0.775053    0.777778    0.891467    0.853293    0.730916    0.741401    2600        50.762660   0.689827    0.174993    31.286177  
[37m[36mINFO[0m[0m 03/27 14:42:58 | 0.906437    0.880240    0.792561    0.801470    0.585534    0.841977    0.853301    0.784115    0.790598    0.906437    0.880240    0.751590    0.760510    2800        54.667480   0.640103    0.175592    34.155756  
[37m[36mINFO[0m[0m 03/27 14:44:24 | 0.919162    0.877246    0.805734    0.824639    0.537781    0.841367    0.880196    0.793710    0.814103    0.919162    0.877246    0.782125    0.779618    3000        58.572300   0.578215    0.175101    30.086793  
[37m[36mINFO[0m[0m 03/27 14:45:52 | 0.924401    0.889222    0.819431    0.834741    0.504271    0.863941    0.887531    0.806503    0.820513    0.924401    0.889222    0.787850    0.796178    3200        62.477120   0.543432    0.175987    31.801383  
[37m[36mINFO[0m[0m 03/27 14:47:20 | 0.933383    0.898204    0.830191    0.844671    0.472625    0.869433    0.897311    0.819296    0.829060    0.933383    0.898204    0.801845    0.807643    3400        66.381940   0.515800    0.176292    31.011275  
[37m[36mINFO[0m[0m 03/27 14:48:46 | 0.940868    0.904192    0.835105    0.854698    0.442187    0.874924    0.902200    0.824094    0.850427    0.940868    0.904192    0.806298    0.811465    3600        70.286760   0.479827    0.176252    30.350550  
[37m[36mINFO[0m[0m 03/27 14:50:16 | 0.944611    0.916168    0.838807    0.863676    0.418509    0.876144    0.909535    0.828891    0.861111    0.944611    0.916168    0.811387    0.820382    3800        74.191580   0.469402    0.176289    33.564127  
[37m[36mINFO[0m[0m 03/27 14:51:44 | 0.952096    0.931138    0.855280    0.866957    0.404000    0.887736    0.916870    0.843817    0.854701    0.952096    0.931138    0.834288    0.829299    4000        78.096400   0.440376    0.176139    31.047537  
[37m[36mINFO[0m[0m 03/27 14:53:13 | 0.953593    0.934132    0.854808    0.871333    0.386140    0.885906    0.919315    0.841684    0.865385    0.953593    0.934132    0.836832    0.829299    4200        82.001220   0.418252    0.175844    32.979254  
[37m[36mINFO[0m[0m 03/27 14:54:41 | 0.958832    0.946108    0.865675    0.875223    0.362470    0.898719    0.914425    0.854478    0.865385    0.958832    0.946108    0.843830    0.845860    4400        85.906040   0.400230    0.175757    31.587788  
[37m[36mINFO[0m[0m 03/27 14:56:09 | 0.960329    0.946108    0.866415    0.880586    0.360178    0.894448    0.916870    0.858742    0.873932    0.960329    0.946108    0.846056    0.850955    4600        89.810860   0.384063    0.175711    31.855213  
[37m[36mINFO[0m[0m 03/27 14:57:36 | 0.963323    0.952096    0.875363    0.887133    0.338463    0.904210    0.916870    0.860874    0.891026    0.963323    0.952096    0.861005    0.853503    4800        93.715680   0.373427    0.175798    31.049273  
[37m[36mINFO[0m[0m 03/27 14:59:04 | 0.967814    0.958084    0.877890    0.890003    0.334248    0.908481    0.914425    0.865139    0.893162    0.967814    0.958084    0.860051    0.862420    5000        97.620500   0.355927    0.175741    31.133132  
[37m[36mINFO[0m[0m 03/27 14:59:25 | Cumulative gradient change saved at train_output/PACS/ERM/[2]/250327_14-21-51_sgd_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 14:59:26 | ---
[37m[36mINFO[0m[0m 03/27 14:59:26 | test-domain validation(oracle) = 96.781%
[37m[36mINFO[0m[0m 03/27 14:59:26 | training-domain validation(iid) = 96.781%
[37m[36mINFO[0m[0m 03/27 14:59:26 | last = 96.781%
[37m[36mINFO[0m[0m 03/27 14:59:26 | last (inD) = 89.000%
[37m[36mINFO[0m[0m 03/27 14:59:26 | training-domain validation (iid, inD) = 89.000%
[37m[36mINFO[0m[0m 03/27 14:59:26 | === Summary ===
[37m[36mINFO[0m[0m 03/27 14:59:26 | Command: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 2 --dataset PACS
[37m[36mINFO[0m[0m 03/27 14:59:26 | Unique name: 250327_14-21-51_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 14:59:26 | Out path: train_output/PACS/ERM/[2]/250327_14-21-51_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 14:59:26 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 14:59:26 | Dataset: PACS
