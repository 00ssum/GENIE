[37m[36mINFO[0m[0m 03/27 13:44:15 | Command :: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 1 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: sgd_sharpness
	out_dir: train_output/PACS/ERM/[1]/250327_13-44-15_sgd_sharpness
	out_root: train_output/PACS/ERM/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250327_13-44-15_sgd_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/27 13:44:15 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 13:44:15 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 13:44:15 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 13:44:15 | 
[37m[36mINFO[0m[0m 03/27 13:44:15 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/27 13:44:15 | Test envs = [1], name = te_C
[37m[36mINFO[0m[0m 03/27 13:44:15 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 03/27 13:44:15 | Batch sizes for each domain: [32, 0, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 13:44:15 | steps-per-epoch for each domain: 51.22, 41.75, 98.25 -> min = 41.75
[37m[36mINFO[0m[0m 03/27 13:44:16 | # of params = 23522375
[37m[36mINFO[0m[0m 03/27 13:44:49 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 13:44:49 | 0.161514    0.151709    0.100800    0.107136    2.013914    0.170226    0.183374    0.161514    0.151709    0.058383    0.062874    0.073791    0.075159    0           0.000000    1.975161    0.936618    32.600345  
[37m[36mINFO[0m[0m 03/27 13:46:19 | 0.199360    0.200855    0.213250    0.219286    1.890276    0.209274    0.227384    0.199360    0.200855    0.252994    0.221557    0.177481    0.208917    200         4.790419    1.943030    0.177552    32.707450  
[37m[36mINFO[0m[0m 03/27 13:47:47 | 0.261194    0.256410    0.319495    0.311204    1.785334    0.300793    0.303178    0.261194    0.256410    0.416916    0.392216    0.240776    0.238217    400         9.580838    1.829109    0.176996    31.088161  
[37m[36mINFO[0m[0m 03/27 13:49:14 | 0.331557    0.337607    0.439569    0.437861    1.663392    0.418548    0.405868    0.331557    0.337607    0.591317    0.568862    0.308842    0.338854    600         14.371257   1.714728    0.176879    30.335911  
[37m[36mINFO[0m[0m 03/27 13:50:42 | 0.400853    0.405983    0.574876    0.581713    1.507906    0.541794    0.533007    0.400853    0.405983    0.755988    0.739521    0.426845    0.472611    800         19.161677   1.576725    0.180730    30.815790  
[37m[36mINFO[0m[0m 03/27 13:52:11 | 0.436034    0.435897    0.626831    0.647617    1.328809    0.610128    0.621027    0.436034    0.435897    0.813623    0.817365    0.456743    0.504459    1000        23.952096   1.408461    0.176373    32.377968  
[37m[36mINFO[0m[0m 03/27 13:53:38 | 0.463753    0.465812    0.683832    0.704138    1.136739    0.675412    0.716381    0.463753    0.465812    0.877994    0.862275    0.498092    0.533758    1200        28.742515   1.224402    0.179441    30.446868  
[37m[36mINFO[0m[0m 03/27 13:55:08 | 0.492537    0.491453    0.727692    0.757218    0.950945    0.732764    0.792176    0.492537    0.491453    0.904192    0.892216    0.546120    0.587261    1400        33.532934   1.031337    0.175830    32.989905  
[37m[36mINFO[0m[0m 03/27 13:56:35 | 0.520256    0.544872    0.768799    0.795673    0.786703    0.777913    0.831296    0.520256    0.544872    0.933383    0.925150    0.595102    0.630573    1600        38.323353   0.875232    0.177621    30.744615  
[37m[36mINFO[0m[0m 03/27 13:58:02 | 0.543177    0.559829    0.799046    0.825063    0.651131    0.806589    0.858191    0.543177    0.559829    0.950599    0.943114    0.639949    0.673885    1800        43.113772   0.735363    0.177652    30.447008  
[37m[36mINFO[0m[0m 03/27 13:59:31 | 0.552772    0.579060    0.826806    0.844171    0.548949    0.826724    0.870416    0.552772    0.579060    0.958084    0.955090    0.695611    0.707006    2000        47.904192   0.620072    0.175723    32.272063  
[37m[36mINFO[0m[0m 03/27 14:00:58 | 0.560768    0.585470    0.848128    0.866033    0.469308    0.850519    0.887531    0.560768    0.585470    0.969311    0.964072    0.724555    0.746497    2200        52.694611   0.526927    0.176801    30.809401  
[37m[36mINFO[0m[0m 03/27 14:02:27 | 0.564499    0.566239    0.858229    0.868832    0.424345    0.865162    0.880196    0.564499    0.566239    0.965569    0.967066    0.743957    0.759236    2400        57.485030   0.469401    0.176483    31.904377  
[37m[36mINFO[0m[0m 03/27 14:03:53 | 0.566098    0.576923    0.869847    0.878003    0.379145    0.866992    0.892421    0.566098    0.576923    0.971557    0.967066    0.770992    0.774522    2600        62.275449   0.421019    0.175773    30.268463  
[37m[36mINFO[0m[0m 03/27 14:05:20 | 0.571429    0.581197    0.877535    0.887815    0.341134    0.873703    0.899756    0.571429    0.581197    0.974551    0.970060    0.784351    0.793631    2800        67.065868   0.383903    0.175637    30.572653  
[37m[36mINFO[0m[0m 03/27 14:06:47 | 0.576226    0.585470    0.883775    0.894609    0.319929    0.879195    0.899756    0.576226    0.585470    0.971557    0.970060    0.800573    0.814013    3000        71.856287   0.355227    0.175884    30.491843  
[37m[36mINFO[0m[0m 03/27 14:08:14 | 0.582623    0.589744    0.894311    0.897123    0.299986    0.889567    0.902200    0.582623    0.589744    0.975299    0.970060    0.818066    0.819108    3200        76.646707   0.333199    0.176141    30.364011  
[37m[36mINFO[0m[0m 03/27 14:09:41 | 0.599680    0.608974    0.897179    0.903997    0.283311    0.894448    0.907090    0.599680    0.608974    0.976796    0.973054    0.820293    0.831847    3400        81.437126   0.313013    0.176360    30.663435  
[37m[36mINFO[0m[0m 03/27 14:11:08 | 0.591684    0.602564    0.904194    0.911054    0.267366    0.896278    0.909535    0.591684    0.602564    0.979790    0.979042    0.836514    0.844586    3600        86.227545   0.298616    0.176193    30.692681  
[37m[36mINFO[0m[0m 03/27 14:12:36 | 0.598614    0.598291    0.909766    0.912995    0.255787    0.906650    0.911980    0.598614    0.598291    0.977545    0.976048    0.845102    0.850955    3800        91.017964   0.283014    0.176040    31.329562  
[37m[36mINFO[0m[0m 03/27 14:14:05 | 0.601279    0.602564    0.911837    0.915967    0.243080    0.912752    0.911980    0.601279    0.602564    0.978293    0.976048    0.844466    0.859873    4000        95.808383   0.266950    0.176608    32.342175  
[37m[36mINFO[0m[0m 03/27 14:15:33 | 0.598081    0.589744    0.912349    0.918056    0.238965    0.910921    0.914425    0.598081    0.589744    0.975299    0.976048    0.850827    0.863694    4200        100.598802  0.251597    0.177271    31.322383  
[37m[36mINFO[0m[0m 03/27 14:17:01 | 0.597548    0.583333    0.916841    0.916885    0.229240    0.917023    0.907090    0.597548    0.583333    0.982036    0.976048    0.851463    0.867516    4400        105.389222  0.245018    0.176473    31.603871  
[37m[36mINFO[0m[0m 03/27 14:18:28 | 0.609808    0.596154    0.921116    0.921670    0.225731    0.914582    0.909535    0.609808    0.596154    0.982036    0.979042    0.866730    0.876433    4600        110.179641  0.232131    0.179296    30.684194  
[37m[36mINFO[0m[0m 03/27 14:19:57 | 0.611407    0.596154    0.922248    0.924001    0.217648    0.917023    0.914425    0.611407    0.596154    0.982036    0.976048    0.867684    0.881529    4800        114.970060  0.232821    0.175869    32.142079  
[37m[36mINFO[0m[0m 03/27 14:21:25 | 0.604478    0.579060    0.921747    0.924999    0.217031    0.912752    0.914425    0.604478    0.579060    0.983533    0.979042    0.868957    0.881529    5000        119.760479  0.218907    0.176042    31.703702  
[37m[36mINFO[0m[0m 03/27 14:21:46 | Cumulative gradient change saved at train_output/PACS/ERM/[1]/250327_13-44-15_sgd_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 14:21:47 | ---
[37m[36mINFO[0m[0m 03/27 14:21:47 | test-domain validation(oracle) = 59.968%
[37m[36mINFO[0m[0m 03/27 14:21:47 | training-domain validation(iid) = 60.448%
[37m[36mINFO[0m[0m 03/27 14:21:47 | last = 60.448%
[37m[36mINFO[0m[0m 03/27 14:21:47 | last (inD) = 92.500%
[37m[36mINFO[0m[0m 03/27 14:21:47 | training-domain validation (iid, inD) = 92.500%
[37m[36mINFO[0m[0m 03/27 14:21:48 | === Summary ===
[37m[36mINFO[0m[0m 03/27 14:21:48 | Command: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 1 --dataset PACS
[37m[36mINFO[0m[0m 03/27 14:21:48 | Unique name: 250327_13-44-15_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 14:21:48 | Out path: train_output/PACS/ERM/[1]/250327_13-44-15_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 14:21:48 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 14:21:48 | Dataset: PACS
