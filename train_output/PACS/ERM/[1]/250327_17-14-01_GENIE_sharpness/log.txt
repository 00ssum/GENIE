[37m[36mINFO[0m[0m 03/27 17:14:01 | Command :: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 1 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: GENIE_sharpness
	out_dir: train_output/PACS/ERM/[1]/250327_17-14-01_GENIE_sharpness
	out_root: train_output/PACS/ERM/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250327_17-14-01_GENIE_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/27 17:14:01 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 17:14:01 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 17:14:01 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 17:14:01 | 
[37m[36mINFO[0m[0m 03/27 17:14:01 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/27 17:14:01 | Test envs = [1], name = te_C
[37m[36mINFO[0m[0m 03/27 17:14:01 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 03/27 17:14:01 | Batch sizes for each domain: [32, 0, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 17:14:01 | steps-per-epoch for each domain: 51.22, 41.75, 98.25 -> min = 41.75
[37m[36mINFO[0m[0m 03/27 17:14:02 | # of params = 23522375
[37m[36mINFO[0m[0m 03/27 17:14:35 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 17:14:35 | 0.248934    0.264957    0.271637    0.271134    1.864688    0.237340    0.264059    0.248934    0.264957    0.370509    0.335329    0.207061    0.214013    0           0.000000    1.975161    0.997895    32.358834  
[37m[36mINFO[0m[0m 03/27 17:16:10 | 0.735075    0.779915    0.952521    0.949272    0.144125    0.951190    0.941320    0.735075    0.779915    0.986527    0.988024    0.919847    0.918471    200         4.790419    0.300146    0.211379    30.891938  
[37m[36mINFO[0m[0m 03/27 17:17:44 | 0.759062    0.794872    0.977227    0.953911    0.125629    0.972544    0.951100    0.759062    0.794872    0.994760    0.973054    0.964377    0.937580    400         9.580838    0.081800    0.210413    31.440312  
[37m[36mINFO[0m[0m 03/27 17:19:18 | 0.781983    0.826923    0.978815    0.960302    0.151258    0.970104    0.951100    0.781983    0.826923    0.994012    0.982036    0.972328    0.947771    600         14.371257   0.047763    0.209215    30.375895  
[37m[36mINFO[0m[0m 03/27 17:20:52 | 0.762260    0.803419    0.984609    0.963974    0.112616    0.990238    0.963325    0.762260    0.803419    0.994760    0.991018    0.968830    0.937580    800         19.161677   0.041028    0.209798    31.094871  
[37m[36mINFO[0m[0m 03/27 17:22:27 | 0.771855    0.811966    0.990447    0.966097    0.126579    0.993289    0.963325    0.771855    0.811966    1.000000    0.991018    0.978053    0.943949    1000        23.952096   0.030832    0.210354    32.013345  
[37m[36mINFO[0m[0m 03/27 17:24:01 | 0.817164    0.839744    0.993192    0.973030    0.097322    0.991458    0.973105    0.817164    0.839744    0.999251    0.988024    0.988868    0.957962    1200        28.742515   0.029180    0.211357    30.569485  
[37m[36mINFO[0m[0m 03/27 17:25:36 | 0.742004    0.790598    0.996129    0.965122    0.136929    0.996339    0.951100    0.742004    0.790598    1.000000    0.985030    0.992048    0.959236    1400        33.532934   0.020949    0.210251    31.486185  
[37m[36mINFO[0m[0m 03/27 17:27:10 | 0.754264    0.771368    0.993450    0.968038    0.120273    0.995729    0.965770    0.754264    0.771368    0.999251    0.988024    0.985369    0.950318    1600        38.323353   0.019509    0.209055    30.684529  
[37m[36mINFO[0m[0m 03/27 17:28:44 | 0.802772    0.837607    0.996058    0.970951    0.116501    0.993899    0.960880    0.802772    0.837607    1.000000    0.994012    0.994275    0.957962    1800        43.113772   0.021265    0.210197    30.846712  
[37m[36mINFO[0m[0m 03/27 17:30:19 | 0.755864    0.797009    0.996266    0.965949    0.137964    0.996339    0.963325    0.755864    0.797009    0.998503    0.988024    0.993957    0.946497    2000        47.904192   0.010622    0.208935    31.431302  
[37m[36mINFO[0m[0m 03/27 17:31:52 | 0.776652    0.835470    0.997885    0.960727    0.161749    0.996949    0.951100    0.776652    0.835470    0.999251    0.982036    0.997455    0.949045    2200        52.694611   0.009920    0.210655    29.713487  
[37m[36mINFO[0m[0m 03/27 17:33:26 | 0.789446    0.805556    0.998542    0.970012    0.145988    0.998170    0.965770    0.789446    0.805556    1.000000    0.985030    0.997455    0.959236    2400        57.485030   0.010460    0.210557    30.594726  
[37m[36mINFO[0m[0m 03/27 17:34:59 | 0.785714    0.824786    0.998029    0.972857    0.115477    0.996949    0.965770    0.785714    0.824786    1.000000    0.991018    0.997137    0.961783    2600        62.275449   0.008463    0.208617    30.511113  
[37m[36mINFO[0m[0m 03/27 17:36:35 | 0.786247    0.820513    0.997619    0.965571    0.150947    0.998170    0.958435    0.786247    0.820513    0.998503    0.979042    0.996183    0.959236    2800        67.065868   0.007227    0.210917    32.770295  
[37m[36mINFO[0m[0m 03/27 17:38:10 | 0.688166    0.732906    0.986095    0.941976    0.263914    0.988408    0.943765    0.688166    0.732906    0.998503    0.970060    0.971374    0.912102    3000        71.856287   0.008839    0.210397    31.021448  
[37m[36mINFO[0m[0m 03/27 17:39:45 | 0.787846    0.818376    0.998754    0.963848    0.122217    0.998170    0.951100    0.787846    0.818376    1.000000    0.985030    0.998092    0.955414    3200        76.646707   0.012303    0.210977    32.507762  
[37m[36mINFO[0m[0m 03/27 17:41:19 | 0.809701    0.835470    0.998716    0.972467    0.144758    0.998170    0.963325    0.809701    0.835470    0.999251    0.991018    0.998728    0.963057    3400        81.437126   0.008687    0.211443    30.731693  
[37m[36mINFO[0m[0m 03/27 17:42:54 | 0.808635    0.837607    0.997579    0.966510    0.148917    0.998780    0.953545    0.808635    0.837607    1.000000    0.988024    0.993957    0.957962    3600        86.227545   0.006004    0.211324    30.959197  
[37m[36mINFO[0m[0m 03/27 17:44:28 | 0.774520    0.805556    0.999160    0.969290    0.134755    0.999390    0.965770    0.774520    0.805556    1.000000    0.979042    0.998092    0.963057    3800        91.017964   0.004935    0.210203    30.308437  
[37m[36mINFO[0m[0m 03/27 17:46:02 | 0.804904    0.831197    0.998593    0.970871    0.124303    0.999390    0.955990    0.804904    0.831197    0.999251    0.991018    0.997137    0.965605    4000        95.808383   0.008375    0.208708    31.594400  
[37m[36mINFO[0m[0m 03/27 17:47:37 | 0.817164    0.837607    0.998020    0.969336    0.117184    0.997559    0.973105    0.817164    0.837607    1.000000    0.982036    0.996501    0.952866    4200        100.598802  0.007165    0.211016    31.500414  
[37m[36mINFO[0m[0m 03/27 17:49:11 | 0.792111    0.814103    0.999682    0.971159    0.120576    1.000000    0.965770    0.792111    0.814103    1.000000    0.991018    0.999046    0.956688    4400        105.389222  0.005035    0.207981    31.223977  
[37m[36mINFO[0m[0m 03/27 17:50:44 | 0.814499    0.850427    0.998064    0.968897    0.130700    0.994509    0.955990    0.814499    0.850427    1.000000    0.994012    0.999682    0.956688    4600        110.179641  0.006019    0.209249    29.780763  
[37m[36mINFO[0m[0m 03/27 17:52:17 | 0.787313    0.816239    0.999691    0.968565    0.125392    0.999390    0.958435    0.787313    0.816239    1.000000    0.988024    0.999682    0.959236    4800        114.970060  0.004027    0.208530    29.811063  
[37m[36mINFO[0m[0m 03/27 17:53:50 | 0.804904    0.829060    0.999576    0.977391    0.113293    1.000000    0.975550    0.804904    0.829060    1.000000    0.991018    0.998728    0.965605    5000        119.760479  0.003123    0.208294    30.725480  
[37m[36mINFO[0m[0m 03/27 17:54:11 | Cumulative gradient change saved at train_output/PACS/ERM/[1]/250327_17-14-01_GENIE_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 17:54:13 | ---
[37m[36mINFO[0m[0m 03/27 17:54:13 | test-domain validation(oracle) = 81.450%
[37m[36mINFO[0m[0m 03/27 17:54:13 | training-domain validation(iid) = 80.490%
[37m[36mINFO[0m[0m 03/27 17:54:13 | last = 80.490%
[37m[36mINFO[0m[0m 03/27 17:54:13 | last (inD) = 97.739%
[37m[36mINFO[0m[0m 03/27 17:54:13 | training-domain validation (iid, inD) = 97.739%
[37m[36mINFO[0m[0m 03/27 17:54:13 | === Summary ===
[37m[36mINFO[0m[0m 03/27 17:54:13 | Command: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 1 --dataset PACS
[37m[36mINFO[0m[0m 03/27 17:54:13 | Unique name: 250327_17-14-01_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 17:54:13 | Out path: train_output/PACS/ERM/[1]/250327_17-14-01_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 17:54:13 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 17:54:13 | Dataset: PACS
