[37m[36mINFO[0m[0m 03/05 16:49:06 | Command :: /jsm0707/GENIE/train_all.py B_vit_alg_ERM config/clip_vitb16_GENIE.yaml --trial_seed 2 --hparams_seed 9 --algorithm ERM --test_envs 0 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_GENIE.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 9
	in_domain: False
	model_save: None
	mpa: False
	name: B_vit_alg_ERM
	out_dir: train_output/PACS/ERM/[0]/250305_16-49-06_B_vit_alg_ERM
	out_root: train_output/PACS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 2
	unique_name: 250305_16-49-06_B_vit_alg_ERM
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 2.716671579524612e-05
	batch_size: 29
	weight_decay: 1.71368232883332e-06
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: clip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/05 16:49:06 | n_steps = 5001
[37m[36mINFO[0m[0m 03/05 16:49:06 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/05 16:49:06 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/05 16:49:06 | 
[37m[36mINFO[0m[0m 03/05 16:49:06 | Testenv name escaping te_A -> te_A
[37m[36mINFO[0m[0m 03/05 16:49:06 | Test envs = [0], name = te_A
[37m[36mINFO[0m[0m 03/05 16:49:06 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/05 16:49:06 | Batch sizes for each domain: [0, 29, 29, 29] (total=87)
[37m[36mINFO[0m[0m 03/05 16:49:06 | steps-per-epoch for each domain: 64.69, 46.07, 108.41 -> min = 46.07
[37m[36mINFO[0m[0m 03/05 16:49:09 | # of params = 86196231
[37m[36mINFO[0m[0m 03/05 16:49:41 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/05 16:49:41 | 0.176327    0.176039    0.171656    0.158581    1.932282    0.176327    0.176039    0.195629    0.155983    0.125000    0.119760    0.194338    0.200000    0           0.000000    1.954243    1.324965    31.151598  
[37m[36mINFO[0m[0m 03/05 16:51:31 | 0.352044    0.371638    0.553144    0.601265    1.085930    0.352044    0.371638    0.513859    0.574786    0.687874    0.724551    0.457697    0.504459    200         4.341317    1.670388    0.394245    31.332150  
[37m[36mINFO[0m[0m 03/05 16:53:22 | 0.545455    0.530562    0.769707    0.800274    0.550849    0.545455    0.530562    0.775586    0.820513    0.916168    0.919162    0.617366    0.661146    400         8.682635    0.743834    0.396535    31.375317  
[37m[36mINFO[0m[0m 03/05 16:55:12 | 0.546065    0.537897    0.855627    0.848637    0.429064    0.546065    0.537897    0.903518    0.905983    0.922904    0.943114    0.740458    0.696815    600         13.023952   0.479546    0.394611    30.769842  
[37m[36mINFO[0m[0m 03/05 16:57:02 | 0.538743    0.574572    0.887726    0.855226    0.409720    0.538743    0.574572    0.924840    0.899573    0.962575    0.922156    0.775763    0.743949    800         17.365269   0.361844    0.396878    31.132602  
[37m[36mINFO[0m[0m 03/05 16:58:52 | 0.506406    0.530562    0.892846    0.856206    0.406664    0.506406    0.530562    0.907249    0.884615    0.972305    0.931138    0.798982    0.752866    1000        21.706587   0.307932    0.395033    30.496838  
