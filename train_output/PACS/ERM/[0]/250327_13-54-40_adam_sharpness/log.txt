[37m[36mINFO[0m[0m 03/27 13:54:40 | Command :: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 0 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: adam_sharpness
	out_dir: train_output/PACS/ERM/[0]/250327_13-54-40_adam_sharpness
	out_root: train_output/PACS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250327_13-54-40_adam_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/27 13:54:40 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 13:54:40 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 13:54:40 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 13:54:40 | 
[37m[36mINFO[0m[0m 03/27 13:54:40 | Testenv name escaping te_A -> te_A
[37m[36mINFO[0m[0m 03/27 13:54:40 | Test envs = [0], name = te_A
[37m[36mINFO[0m[0m 03/27 13:54:40 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/27 13:54:40 | Batch sizes for each domain: [0, 32, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 13:54:40 | steps-per-epoch for each domain: 58.62, 41.75, 98.25 -> min = 41.75
[37m[36mINFO[0m[0m 03/27 13:54:41 | # of params = 23522375
[37m[36mINFO[0m[0m 03/27 13:55:13 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 13:55:13 | 0.257474    0.237164    0.291493    0.285536    1.839943    0.257474    0.237164    0.266525    0.297009    0.377994    0.344311    0.229962    0.215287    0           0.000000    1.997718    0.955009    31.629142  
[37m[36mINFO[0m[0m 03/27 13:56:43 | 0.732764    0.753056    0.918279    0.917049    0.260839    0.732764    0.753056    0.905117    0.914530    0.969311    0.955090    0.880407    0.881529    200         4.790419    0.269159    0.179026    32.762896  
[37m[36mINFO[0m[0m 03/27 13:58:13 | 0.847468    0.841076    0.978265    0.968202    0.124782    0.847468    0.841076    0.972814    0.970085    0.992515    0.988024    0.969466    0.946497    400         9.580838    0.083232    0.179910    32.563662  
[37m[36mINFO[0m[0m 03/27 13:59:41 | 0.824893    0.863081    0.985516    0.966770    0.125809    0.824893    0.863081    0.989872    0.972222    0.996257    0.979042    0.970420    0.949045    600         14.371257   0.054128    0.179772    30.473193  
[37m[36mINFO[0m[0m 03/27 14:01:10 | 0.842587    0.853301    0.987640    0.965781    0.110648    0.842587    0.853301    0.989339    0.970085    0.997754    0.982036    0.975827    0.945223    800         19.161677   0.037048    0.180161    31.526950  
[37m[36mINFO[0m[0m 03/27 14:02:38 | 0.866992    0.875306    0.993227    0.964633    0.133743    0.866992    0.875306    0.994670    0.965812    0.995509    0.979042    0.989504    0.949045    1000        23.952096   0.035810    0.179430    30.904709  
[37m[36mINFO[0m[0m 03/27 14:04:07 | 0.862721    0.865526    0.993262    0.969902    0.115251    0.862721    0.865526    0.990938    0.974359    0.997754    0.985030    0.991094    0.950318    1200        28.742515   0.025471    0.179915    31.777481  
[37m[36mINFO[0m[0m 03/27 14:05:36 | 0.838316    0.860636    0.995507    0.973027    0.104499    0.838316    0.860636    0.998401    0.982906    0.999251    0.982036    0.988868    0.954140    1400        33.532934   0.022263    0.179171    31.362917  
[37m[36mINFO[0m[0m 03/27 14:07:03 | 0.841977    0.860636    0.992975    0.961780    0.156797    0.841977    0.860636    0.990938    0.972222    0.996257    0.964072    0.991730    0.949045    1600        38.323353   0.023238    0.179046    30.263361  
[37m[36mINFO[0m[0m 03/27 14:08:32 | 0.841367    0.828851    0.995180    0.963060    0.134864    0.841367    0.828851    0.993603    0.961538    0.999251    0.976048    0.992684    0.951592    1800        43.113772   0.019056    0.180330    32.060771  
[37m[36mINFO[0m[0m 03/27 14:10:01 | 0.868212    0.894866    0.995145    0.969454    0.108504    0.868212    0.894866    0.996269    0.970085    0.997754    0.979042    0.991412    0.959236    2000        47.904192   0.016655    0.179072    32.054186  
[37m[36mINFO[0m[0m 03/27 14:11:31 | 0.859060    0.860636    0.996919    0.969468    0.106365    0.859060    0.860636    0.997868    0.976496    0.999251    0.979042    0.993639    0.952866    2200        52.694611   0.021948    0.178158    33.031792  
[37m[36mINFO[0m[0m 03/27 14:13:00 | 0.858450    0.858191    0.995785    0.967052    0.124121    0.858450    0.858191    0.995736    0.961538    0.999251    0.988024    0.992366    0.951592    2400        57.485030   0.015855    0.177781    31.281157  
[37m[36mINFO[0m[0m 03/27 14:14:26 | 0.777303    0.772616    0.989030    0.953526    0.181518    0.777303    0.772616    0.990405    0.961538    0.988772    0.955090    0.987913    0.943949    2600        62.275449   0.019607    0.177750    29.998203  
[37m[36mINFO[0m[0m 03/27 14:15:55 | 0.896278    0.885086    0.998865    0.968010    0.137693    0.896278    0.885086    0.998934    0.970085    0.999251    0.967066    0.998410    0.966879    2800        67.065868   0.011772    0.177553    31.607584  
[37m[36mINFO[0m[0m 03/27 14:17:22 | 0.807200    0.814181    0.994818    0.970304    0.131259    0.807200    0.814181    0.994670    0.970085    0.995509    0.979042    0.994275    0.961783    3000        71.856287   0.014543    0.178127    30.024227  
[37m[36mINFO[0m[0m 03/27 14:18:49 | 0.815131    0.831296    0.996520    0.958917    0.169823    0.815131    0.831296    0.994670    0.957265    0.997754    0.964072    0.997137    0.955414    3200        76.646707   0.019602    0.178937    30.758855  
[37m[36mINFO[0m[0m 03/27 14:20:17 | 0.799878    0.836186    0.994499    0.954494    0.200313    0.799878    0.836186    0.998934    0.967949    0.992515    0.940120    0.992048    0.955414    3400        81.437126   0.013870    0.177971    30.397008  
[37m[36mINFO[0m[0m 03/27 14:21:46 | 0.788896    0.823961    0.983452    0.944844    0.252855    0.788896    0.823961    0.987740    0.942308    0.979790    0.952096    0.982824    0.940127    3600        86.227545   0.016928    0.178346    32.145443  
[37m[36mINFO[0m[0m 03/27 14:23:14 | 0.782794    0.772616    0.982742    0.935060    0.233230    0.782794    0.772616    0.980277    0.927350    0.996257    0.958084    0.971692    0.919745    3800        91.017964   0.020341    0.179287    30.475217  
[37m[36mINFO[0m[0m 03/27 14:24:44 | 0.762050    0.767726    0.978512    0.933936    0.289982    0.762050    0.767726    0.986674    0.955128    0.983533    0.937126    0.965331    0.909554    4000        95.808383   0.014262    0.180517    33.167556  
[37m[36mINFO[0m[0m 03/27 14:26:13 | 0.846248    0.838631    0.995601    0.963917    0.139830    0.846248    0.838631    0.990938    0.955128    1.000000    0.985030    0.995865    0.951592    4200        100.598802  0.013969    0.178473    31.788697  
[37m[36mINFO[0m[0m 03/27 14:27:41 | 0.829164    0.821516    0.990302    0.950857    0.196954    0.829164    0.821516    0.995736    0.950855    0.997754    0.973054    0.977417    0.928662    4400        105.389222  0.012174    0.177262    31.725913  
[37m[36mINFO[0m[0m 03/27 14:29:12 | 0.795607    0.809291    0.997343    0.962347    0.181851    0.795607    0.809291    0.997868    0.959402    0.999251    0.976048    0.994911    0.951592    4600        110.179641  0.010338    0.178256    33.711793  
[37m[36mINFO[0m[0m 03/27 14:30:41 | 0.805979    0.831296    0.999220    0.971431    0.114235    0.805979    0.831296    1.000000    0.974359    0.999251    0.973054    0.998410    0.966879    4800        114.970060  0.014191    0.179384    31.760714  
[37m[36mINFO[0m[0m 03/27 14:32:10 | 0.807810    0.823961    0.997555    0.963327    0.136086    0.807810    0.823961    0.998934    0.967949    0.998503    0.964072    0.995229    0.957962    5000        119.760479  0.009811    0.179118    32.324068  
[37m[36mINFO[0m[0m 03/27 14:32:32 | Cumulative gradient change saved at train_output/PACS/ERM/[0]/250327_13-54-40_adam_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 14:32:33 | ---
[37m[36mINFO[0m[0m 03/27 14:32:33 | test-domain validation(oracle) = 86.821%
[37m[36mINFO[0m[0m 03/27 14:32:33 | training-domain validation(iid) = 83.832%
[37m[36mINFO[0m[0m 03/27 14:32:33 | last = 80.781%
[37m[36mINFO[0m[0m 03/27 14:32:33 | last (inD) = 96.333%
[37m[36mINFO[0m[0m 03/27 14:32:33 | training-domain validation (iid, inD) = 97.303%
[37m[36mINFO[0m[0m 03/27 14:32:33 | === Summary ===
[37m[36mINFO[0m[0m 03/27 14:32:33 | Command: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 0 --dataset PACS
[37m[36mINFO[0m[0m 03/27 14:32:33 | Unique name: 250327_13-54-40_adam_sharpness
[37m[36mINFO[0m[0m 03/27 14:32:33 | Out path: train_output/PACS/ERM/[0]/250327_13-54-40_adam_sharpness
[37m[36mINFO[0m[0m 03/27 14:32:33 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 14:32:33 | Dataset: PACS
