[37m[36mINFO[0m[0m 03/03 22:57:05 | Command :: /jsm0707/GENIE/train_all.py clip_vitb16_sgd config/clip_vitb16_sgd.yaml --algorithm ERM --test_envs 0 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_sgd.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: clip_vitb16_sgd
	out_dir: train_output/PACS/ERM/[0]/250303_22-57-05_clip_vitb16_sgd
	out_root: train_output/PACS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250303_22-57-05_clip_vitb16_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: clip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/03 22:57:05 | n_steps = 5001
[37m[36mINFO[0m[0m 03/03 22:57:05 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/03 22:57:05 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/03 22:57:05 | 
[37m[36mINFO[0m[0m 03/03 22:57:05 | Testenv name escaping te_A -> te_A
[37m[36mINFO[0m[0m 03/03 22:57:05 | Test envs = [0], name = te_A
[37m[36mINFO[0m[0m 03/03 22:57:05 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/03 22:57:05 | Batch sizes for each domain: [0, 32, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/03 22:57:05 | steps-per-epoch for each domain: 58.62, 41.75, 98.25 -> min = 41.75
[37m[36mINFO[0m[0m 03/03 22:57:08 | # of params = 86196231
[37m[36mINFO[0m[0m 03/03 22:57:40 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/03 22:57:40 | 0.106162    0.097800    0.126077    0.138732    1.934505    0.106162    0.097800    0.131130    0.132479    0.133234    0.185629    0.113868    0.098089    0           0.000000    1.922687    1.083564    31.665230  
[37m[36mINFO[0m[0m 03/03 22:59:33 | 0.241001    0.232274    0.503599    0.508417    1.712601    0.241001    0.232274    0.387527    0.403846    0.571108    0.523952    0.552163    0.597452    200         4.790419    1.824758    0.404846    32.121051  
[37m[36mINFO[0m[0m 03/03 23:01:27 | 0.537523    0.599022    0.816848    0.786022    1.467679    0.537523    0.599022    0.722281    0.730769    0.899701    0.781437    0.828562    0.845860    400         9.580838    1.592740    0.415183    30.449759  
[37m[36mINFO[0m[0m 03/03 23:03:22 | 0.767541    0.794621    0.920646    0.911792    1.204703    0.767541    0.794621    0.905650    0.908120    0.981287    0.952096    0.875000    0.875159    600         14.371257   1.331539    0.418905    31.574638  
[37m[36mINFO[0m[0m 03/03 23:05:17 | 0.910921    0.924205    0.954018    0.958144    0.952101    0.910921    0.924205    0.960021    0.959402    0.987275    0.994012    0.914758    0.921019    800         19.161677   1.063580    0.418840    30.891111  
[37m[36mINFO[0m[0m 03/03 23:07:09 | 0.949969    0.951100    0.967532    0.972920    0.743177    0.949969    0.951100    0.971748    0.978632    0.997006    1.000000    0.933842    0.940127    1000        23.952096   0.833380    0.410574    30.396021  
[37m[36mINFO[0m[0m 03/03 23:09:04 | 0.962172    0.965770    0.972942    0.979303    0.578938    0.962172    0.965770    0.975480    0.985043    0.995509    1.000000    0.947837    0.952866    1200        28.742515   0.646045    0.417540    30.733478  
[37m[36mINFO[0m[0m 03/03 23:10:59 | 0.963392    0.975550    0.975811    0.981440    0.470627    0.963392    0.975550    0.976546    0.991453    0.997006    1.000000    0.953880    0.952866    1400        33.532934   0.507476    0.422595    31.108444  
[37m[36mINFO[0m[0m 03/03 23:12:55 | 0.968883    0.958435    0.981413    0.981577    0.377771    0.968883    0.958435    0.985608    0.989316    0.997754    1.000000    0.960878    0.955414    1600        38.323353   0.409731    0.423512    31.377172  
[37m[36mINFO[0m[0m 03/03 23:14:50 | 0.968273    0.965770    0.984422    0.983988    0.315147    0.968273    0.965770    0.987207    0.991453    0.998503    1.000000    0.967557    0.960510    1800        43.113772   0.334062    0.417947    30.881368  
[37m[36mINFO[0m[0m 03/03 23:16:45 | 0.969494    0.965770    0.985791    0.986398    0.268896    0.969494    0.965770    0.985608    0.993590    0.996257    1.000000    0.975509    0.965605    2000        47.904192   0.369804    0.418009    31.884821  
[37m[36mINFO[0m[0m 03/03 23:18:40 | 0.967663    0.965770    0.985302    0.985974    0.234973    0.967663    0.965770    0.985075    0.993590    0.998503    1.000000    0.972328    0.964331    2200        52.694611   0.241614    0.415567    31.328056  
[37m[36mINFO[0m[0m 03/03 23:20:34 | 0.971324    0.968215    0.987253    0.988809    0.205312    0.971324    0.968215    0.990405    0.995726    0.997754    1.000000    0.973601    0.970701    2400        57.485030   0.208816    0.415259    30.688091  
[37m[36mINFO[0m[0m 03/03 23:22:27 | 0.971934    0.968215    0.989592    0.988384    0.184119    0.971934    0.968215    0.992537    0.995726    0.998503    1.000000    0.977735    0.969427    2600        62.275449   0.184998    0.408821    31.752057  
[37m[36mINFO[0m[0m 03/03 23:24:20 | 0.968883    0.965770    0.989807    0.988384    0.165855    0.968883    0.965770    0.993070    0.995726    0.999251    1.000000    0.977099    0.969427    2800        67.065868   0.165721    0.414131    30.322287  
[37m[36mINFO[0m[0m 03/03 23:26:16 | 0.970104    0.965770    0.991400    0.988384    0.152306    0.970104    0.965770    0.993603    0.995726    1.000000    1.000000    0.980598    0.969427    3000        71.856287   0.149986    0.418386    32.258773  
[37m[36mINFO[0m[0m 03/03 23:28:09 | 0.965223    0.958435    0.990976    0.991507    0.137224    0.965223    0.958435    0.994670    1.000000    0.999251    1.000000    0.979008    0.974522    3200        76.646707   0.136304    0.413042    30.299346  
[37m[36mINFO[0m[0m 03/03 23:30:05 | 0.967053    0.965770    0.992816    0.991083    0.128562    0.967053    0.965770    0.995736    1.000000    0.999251    1.000000    0.983461    0.973248    3400        81.437126   0.123617    0.416064    32.669984  
[37m[36mINFO[0m[0m 03/03 23:31:58 | 0.968273    0.968215    0.991220    0.989946    0.122805    0.968273    0.968215    0.993603    0.997863    0.998503    1.000000    0.981552    0.971975    3600        86.227545   0.114222    0.413347    30.832944  
[37m[36mINFO[0m[0m 03/03 23:33:53 | 0.966443    0.965770    0.993489    0.989809    0.114908    0.966443    0.965770    0.997868    1.000000    0.998503    1.000000    0.984097    0.969427    3800        91.017964   0.105456    0.415888    31.484868  
[37m[36mINFO[0m[0m 03/03 23:35:47 | 0.962172    0.965770    0.994229    0.991083    0.107030    0.962172    0.965770    0.995203    1.000000    0.999251    1.000000    0.988232    0.973248    4000        95.808383   0.093887    0.415670    31.011235  
[37m[36mINFO[0m[0m 03/03 23:37:42 | 0.965223    0.965770    0.994263    0.991507    0.100678    0.965223    0.965770    0.994670    1.000000    0.999251    1.000000    0.988868    0.974522    4200        100.598802  0.090449    0.413257    32.540020  
