[37m[36mINFO[0m[0m 03/18 14:14:42 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 0 --dataset PACS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: PACS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/PACS/ERM/[0]/250318_14-14-42_resnet50_GENIE
	out_root: train_output/PACS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250318_14-14-42_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[PACS] #envs=4, #classes=7
	env0: A (#2048)
	env1: C (#2344)
	env2: P (#1670)
	env3: S (#3929)

[37m[36mINFO[0m[0m 03/18 14:14:42 | n_steps = 5001
[37m[36mINFO[0m[0m 03/18 14:14:42 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/18 14:14:42 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/18 14:14:42 | 
[37m[36mINFO[0m[0m 03/18 14:14:42 | Testenv name escaping te_A -> te_A
[37m[36mINFO[0m[0m 03/18 14:14:42 | Test envs = [0], name = te_A
[37m[36mINFO[0m[0m 03/18 14:14:42 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/18 14:14:42 | Batch sizes for each domain: [0, 32, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/18 14:14:42 | steps-per-epoch for each domain: 58.62, 41.75, 98.25 -> min = 41.75
[37m[36mINFO[0m[0m 03/18 14:14:43 | # of params = 23522375
[37m[36mINFO[0m[0m 03/18 14:15:16 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/18 14:15:16 | 0.214155    0.215159    0.269215    0.262249    1.858444    0.214155    0.215159    0.244670    0.286325    0.362275    0.323353    0.200700    0.177070    0           0.000000    1.997718    0.995949    32.490914  
[37m[36mINFO[0m[0m 03/18 14:16:34 | 0.826724    0.845966    0.958171    0.958682    0.134787    0.826724    0.845966    0.943497    0.952991    0.991766    0.988024    0.939249    0.935032    200         4.790419    0.334542    0.212057    34.808955  
[37m[36mINFO[0m[0m 03/18 14:17:49 | 0.874314    0.889976    0.971646    0.958984    0.123417    0.874314    0.889976    0.965885    0.961538    0.993263    0.988024    0.955789    0.927389    400         9.580838    0.095340    0.218511    31.920293  
[37m[36mINFO[0m[0m 03/18 14:19:17 | 0.834045    0.858191    0.972175    0.962662    0.115553    0.834045    0.858191    0.971215    0.974359    0.989521    0.976048    0.955789    0.937580    600         14.371257   0.069440    0.282979    30.949784  
[37m[36mINFO[0m[0m 03/18 14:20:43 | 0.821843    0.811736    0.984539    0.963203    0.129168    0.821843    0.811736    0.977079    0.948718    0.996257    0.988024    0.980280    0.952866    800         19.161677   0.042540    0.271781    32.439434  
[37m[36mINFO[0m[0m 03/18 14:21:59 | 0.856620    0.850856    0.993016    0.969464    0.094428    0.856620    0.850856    0.992537    0.967949    0.997006    0.985030    0.989504    0.955414    1000        23.952096   0.033430    0.218551    32.044882  
[37m[36mINFO[0m[0m 03/18 14:23:32 | 0.859060    0.858191    0.992669    0.966081    0.119333    0.859060    0.858191    0.997335    0.974359    0.996257    0.985030    0.984415    0.938854    1200        28.742515   0.023562    0.305425    31.801458  
[37m[36mINFO[0m[0m 03/18 14:24:55 | 0.849908    0.860636    0.996705    0.975309    0.111417    0.849908    0.860636    0.995203    0.980769    1.000000    0.991018    0.994911    0.954140    1400        33.532934   0.019995    0.244280    34.015149  
[37m[36mINFO[0m[0m 03/18 14:26:11 | 0.763880    0.789731    0.990658    0.964220    0.128336    0.763880    0.789731    0.991471    0.967949    0.988772    0.982036    0.991730    0.942675    1600        38.323353   0.017783    0.224025    31.171910  
[37m[36mINFO[0m[0m 03/18 14:27:42 | 0.886516    0.870416    0.998547    0.972164    0.102101    0.886516    0.870416    0.997868    0.976496    1.000000    0.982036    0.997774    0.957962    1800        43.113772   0.014408    0.302508    30.631774  
[37m[36mINFO[0m[0m 03/18 14:28:58 | 0.826724    0.843521    0.997836    0.973436    0.125597    0.826724    0.843521    0.996802    0.972222    0.999251    0.985030    0.997455    0.963057    2000        47.904192   0.010302    0.225342    31.224950  
[37m[36mINFO[0m[0m 03/18 14:30:23 | 0.849298    0.860636    0.997768    0.970742    0.111146    0.849298    0.860636    0.997868    0.976496    0.999251    0.979042    0.996183    0.956688    2200        52.694611   0.012055    0.260745    32.142311  
[37m[36mINFO[0m[0m 03/18 14:31:50 | 0.841977    0.838631    0.998513    0.972585    0.115246    0.841977    0.838631    0.998401    0.967949    1.000000    0.988024    0.997137    0.961783    2400        57.485030   0.009692    0.285991    30.525064  
[37m[36mINFO[0m[0m 03/18 14:33:05 | 0.879805    0.875306    0.998441    0.969441    0.120095    0.879805    0.875306    0.998934    0.963675    0.999251    0.979042    0.997137    0.965605    2600        62.275449   0.007845    0.218035    31.070172  
[37m[36mINFO[0m[0m 03/18 14:34:36 | 0.862721    0.882641    0.995572    0.964498    0.143906    0.862721    0.882641    0.995736    0.972222    0.999251    0.976048    0.991730    0.945223    2800        67.065868   0.008089    0.297212    31.757283  
[37m[36mINFO[0m[0m 03/18 14:35:58 | 0.864552    0.880196    0.998581    0.969180    0.122374    0.864552    0.880196    0.997335    0.974359    1.000000    0.979042    0.998410    0.954140    3000        71.856287   0.008172    0.255913    30.888026  
[37m[36mINFO[0m[0m 03/18 14:37:14 | 0.795607    0.804401    0.996633    0.967179    0.157348    0.795607    0.804401    0.994670    0.961538    1.000000    0.982036    0.995229    0.957962    3200        76.646707   0.004251    0.223694    30.514083  
[37m[36mINFO[0m[0m 03/18 14:38:47 | 0.866382    0.877751    0.998547    0.970476    0.127349    0.866382    0.877751    0.997868    0.974359    1.000000    0.988024    0.997774    0.949045    3400        81.437126   0.007991    0.309293    31.501874  
[37m[36mINFO[0m[0m 03/18 14:40:04 | 0.854179    0.848411    0.998865    0.966918    0.147486    0.854179    0.848411    0.998934    0.972222    0.999251    0.982036    0.998410    0.946497    3600        86.227545   0.004483    0.228342    31.315805  
[37m[36mINFO[0m[0m 03/18 14:41:24 | 0.862111    0.867971    0.998120    0.969180    0.161099    0.862111    0.867971    0.997335    0.974359    0.999251    0.979042    0.997774    0.954140    3800        91.017964   0.001781    0.245671    31.181807  
[37m[36mINFO[0m[0m 03/18 14:42:56 | 0.837706    0.860636    0.996177    0.957260    0.235702    0.837706    0.860636    0.998934    0.952991    0.998503    0.985030    0.991094    0.933758    4000        95.808383   0.004036    0.303054    30.852203  
[37m[36mINFO[0m[0m 03/18 14:44:11 | 0.877974    0.887531    0.998759    0.967468    0.150639    0.877974    0.887531    0.997868    0.967949    1.000000    0.979042    0.998410    0.955414    4200        100.598802  0.004736    0.220144    31.108750  
[37m[36mINFO[0m[0m 03/18 14:45:38 | 0.870653    0.872861    0.999220    0.972998    0.109990    0.870653    0.872861    1.000000    0.965812    0.999251    0.985030    0.998410    0.968153    4400        105.389222  0.014758    0.281579    30.639778  
[37m[36mINFO[0m[0m 03/18 14:47:04 | 0.897498    0.907090    0.999539    0.972726    0.140952    0.897498    0.907090    0.998934    0.974359    1.000000    0.982036    0.999682    0.961783    4600        110.179641  0.001752    0.270018    31.679663  
[37m[36mINFO[0m[0m 03/18 14:48:21 | 0.858450    0.870416    0.998263    0.963474    0.166750    0.858450    0.870416    0.997335    0.963675    1.000000    0.970060    0.997455    0.956688    4800        114.970060  0.004049    0.218622    33.330461  
[37m[36mINFO[0m[0m 03/18 14:49:53 | 0.890787    0.899756    0.999788    0.973276    0.135209    0.890787    0.899756    1.000000    0.970085    1.000000    0.979042    0.999364    0.970701    5000        119.760479  0.003145    0.308181    31.169694  
[37m[36mINFO[0m[0m 03/18 14:49:54 | Cumulative gradient change saved at train_output/PACS/ERM/[0]/250318_14-14-42_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/18 14:49:55 | ---
[37m[36mINFO[0m[0m 03/18 14:49:55 | test-domain validation(oracle) = 89.750%
[37m[36mINFO[0m[0m 03/18 14:49:55 | training-domain validation(iid) = 84.991%
[37m[36mINFO[0m[0m 03/18 14:49:55 | last = 89.079%
[37m[36mINFO[0m[0m 03/18 14:49:55 | last (inD) = 97.328%
[37m[36mINFO[0m[0m 03/18 14:49:55 | training-domain validation (iid, inD) = 97.531%
[37m[36mINFO[0m[0m 03/18 14:49:55 | === Summary ===
[37m[36mINFO[0m[0m 03/18 14:49:55 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 0 --dataset PACS
[37m[36mINFO[0m[0m 03/18 14:49:55 | Unique name: 250318_14-14-42_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 14:49:55 | Out path: train_output/PACS/ERM/[0]/250318_14-14-42_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 14:49:55 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/18 14:49:55 | Dataset: PACS
