[37m[36mINFO[0m[0m 03/11 19:28:20 | Command :: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 0 --dataset TerraIncognita --trial_seed 0 --hparams_seed 12
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 12
	in_domain: False
	model_save: None
	mpa: False
	name: clip_vitb16_GENIE
	out_dir: train_output/TerraIncognita/ERM/[0]/250311_19-28-20_clip_vitb16_GENIE
	out_root: train_output/TerraIncognita/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250311_19-28-20_clip_vitb16_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.00012111338094255871
	batch_size: 9
	weight_decay: 1.821145631101786e-06
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: openclip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 03/11 19:28:20 | n_steps = 5001
[37m[36mINFO[0m[0m 03/11 19:28:20 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/11 19:28:20 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/11 19:28:20 | 
[37m[36mINFO[0m[0m 03/11 19:28:20 | Testenv name escaping te_L100 -> te_L100
[37m[36mINFO[0m[0m 03/11 19:28:20 | Test envs = [0], name = te_L100
[37m[36mINFO[0m[0m 03/11 19:28:20 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/11 19:28:20 | Batch sizes for each domain: [0, 9, 9, 9] (total=27)
[37m[36mINFO[0m[0m 03/11 19:28:20 | steps-per-epoch for each domain: 865.44, 352.89, 523.00 -> min = 352.89
[37m[36mINFO[0m[0m 03/11 19:28:23 | # of params = 86197770
[37m[36mINFO[0m[0m 03/11 19:31:12 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/11 19:31:12 | 0.009755    0.018987    0.145390    0.131232    2.326637    0.009755    0.018987    0.002054    0.002568    0.273929    0.235516    0.160187    0.155612    0           0.000000    2.377684    1.622702    167.966284 
[37m[36mINFO[0m[0m 03/11 19:34:32 | 0.202742    0.212025    0.424942    0.457578    1.561513    0.202742    0.212025    0.552703    0.608115    0.323992    0.353904    0.398130    0.410714    200         0.566751    1.757352    0.163745    167.238300 
[37m[36mINFO[0m[0m 03/11 19:37:48 | 0.212497    0.216245    0.417428    0.439526    1.542083    0.212497    0.216245    0.534600    0.572162    0.339736    0.347607    0.377948    0.398810    400         1.133501    1.644122    0.152056    165.366064 
[37m[36mINFO[0m[0m 03/11 19:40:59 | 0.199578    0.204641    0.445740    0.486023    1.488782    0.199578    0.204641    0.572988    0.617874    0.337846    0.395466    0.426386    0.444728    600         1.700252    1.567336    0.143982    162.277571 
[37m[36mINFO[0m[0m 03/11 19:44:22 | 0.299499    0.312236    0.450515    0.472687    1.492490    0.299499    0.312236    0.575427    0.595788    0.365239    0.400504    0.410877    0.421769    800         2.267003    1.559914    0.177481    167.686900 
[37m[36mINFO[0m[0m 03/11 19:47:54 | 0.212233    0.233122    0.450622    0.474323    1.502126    0.212233    0.233122    0.568751    0.592193    0.375000    0.400504    0.408116    0.430272    1000        2.833753    1.537893    0.222900    167.469296 
[37m[36mINFO[0m[0m 03/11 19:51:33 | 0.206960    0.219409    0.463630    0.505482    1.423409    0.206960    0.219409    0.598023    0.626091    0.359257    0.429471    0.433610    0.460884    1200        3.400504    1.519066    0.217642    174.851047 
[37m[36mINFO[0m[0m 03/11 19:55:10 | 0.210124    0.214135    0.451700    0.473142    1.477527    0.210124    0.214135    0.594043    0.619414    0.345718    0.379093    0.415339    0.420918    1400        3.967254    1.479065    0.215724    173.793278 
[37m[36mINFO[0m[0m 03/11 19:58:45 | 0.305299    0.312236    0.440151    0.464616    1.559318    0.305299    0.312236    0.561561    0.587571    0.340365    0.387909    0.418526    0.418367    1600        4.534005    1.474230    0.216837    172.200101 
[37m[36mINFO[0m[0m 03/11 20:02:10 | 0.223833    0.240506    0.475751    0.511241    1.397950    0.223833    0.240506    0.595455    0.638932    0.398615    0.448363    0.433185    0.446429    1800        5.100756    1.511165    0.197028    165.648394 
[37m[36mINFO[0m[0m 03/11 20:05:27 | 0.218561    0.229958    0.497349    0.520275    1.365215    0.218561    0.229958    0.618308    0.651772    0.443955    0.463476    0.429785    0.445578    2000        5.667506    1.453090    0.165132    163.158329 
[37m[36mINFO[0m[0m 03/11 20:08:45 | 0.198260    0.207806    0.489782    0.487596    1.334369    0.198260    0.207806    0.619592    0.647663    0.415932    0.394207    0.433822    0.420918    2200        6.234257    1.439461    0.154504    167.871815 
[37m[36mINFO[0m[0m 03/11 20:12:05 | 0.212760    0.221519    0.491924    0.512373    1.307841    0.212760    0.221519    0.608165    0.645095    0.415302    0.431990    0.452305    0.460034    2400        6.801008    1.404542    0.151534    169.530564 
[37m[36mINFO[0m[0m 03/11 20:15:25 | 0.205642    0.215190    0.514161    0.531300    1.329065    0.205642    0.215190    0.608807    0.629173    0.468199    0.498741    0.465477    0.465986    2600        7.367758    1.402330    0.180082    163.642918 
[37m[36mINFO[0m[0m 03/11 20:18:52 | 0.214342    0.223629    0.516914    0.530946    1.271811    0.214342    0.223629    0.607909    0.623010    0.483942    0.498741    0.458891    0.471088    2800        7.934509    1.361480    0.216918    164.226305 
[37m[36mINFO[0m[0m 03/11 20:22:25 | 0.216715    0.227848    0.521345    0.542704    1.255816    0.216715    0.227848    0.623828    0.643041    0.464106    0.503778    0.476099    0.481293    3000        8.501259    1.343397    0.222959    167.504981 
[37m[36mINFO[0m[0m 03/11 20:25:56 | 0.203269    0.212025    0.485439    0.495152    1.268588    0.203269    0.212025    0.626396    0.643554    0.386965    0.361461    0.442957    0.480442    3200        9.068010    1.328002    0.219481    167.938715 
[37m[36mINFO[0m[0m 03/11 20:29:21 | 0.290272    0.309072    0.480649    0.481940    1.299425    0.290272    0.309072    0.618308    0.643041    0.414673    0.395466    0.408965    0.407313    3400        9.634761    1.325840    0.187872    167.123966 
[37m[36mINFO[0m[0m 03/11 20:32:45 | 0.211442    0.218354    0.525114    0.556244    1.270950    0.211442    0.218354    0.632174    0.654340    0.456234    0.505038    0.486934    0.509354    3600        10.201511   1.280732    0.178026    168.618096 
[37m[36mINFO[0m[0m 03/11 20:36:04 | 0.196414    0.207806    0.541459    0.559624    1.204724    0.196414    0.207806    0.636282    0.654854    0.509446    0.550378    0.478649    0.473639    3800        10.768262   1.310334    0.163880    165.679198 
[37m[36mINFO[0m[0m 03/11 20:39:34 | 0.206169    0.215190    0.519162    0.515694    1.249669    0.206169    0.215190    0.630505    0.667180    0.468514    0.415617    0.458466    0.464286    4000        11.335013   1.247675    0.189796    171.807086 
[37m[36mINFO[0m[0m 03/11 20:43:00 | 0.214869    0.225738    0.549735    0.568126    1.204351    0.214869    0.225738    0.646810    0.662558    0.521411    0.521411    0.480986    0.520408    4200        11.901763   1.228183    0.167163    173.303647 
[37m[36mINFO[0m[0m 03/11 20:46:14 | 0.221197    0.232068    0.566427    0.572064    1.171137    0.221197    0.232068    0.657337    0.665126    0.545025    0.562972    0.496919    0.488095    4400        12.468514   1.218169    0.145442    164.453915 
[37m[36mINFO[0m[0m 03/11 20:49:51 | 0.197996    0.206751    0.568772    0.574477    1.163929    0.197996    0.206751    0.660290    0.671289    0.541247    0.545340    0.504780    0.506803    4600        13.035264   1.203379    0.217636    173.647646 
[37m[36mINFO[0m[0m 03/11 20:53:25 | 0.214606    0.222574    0.575696    0.581227    1.151209    0.214606    0.222574    0.664399    0.692861    0.551322    0.549118    0.511366    0.501701    4800        13.602015   1.183743    0.219199    169.795042 
