[37m[36mINFO[0m[0m 03/12 12:32:06 | Command :: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 0 --dataset TerraIncognita --trial_seed 1 --hparams_seed 16
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 16
	in_domain: False
	model_save: None
	mpa: False
	name: clip_vitb16_GENIE
	out_dir: train_output/TerraIncognita/ERM/[0]/250312_12-32-06_clip_vitb16_GENIE
	out_root: train_output/TerraIncognita/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 1
	unique_name: 250312_12-32-06_clip_vitb16_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.3198033474735314e-05
	batch_size: 10
	weight_decay: 0.001844990356992375
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: openclip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 03/12 12:32:07 | n_steps = 5001
[37m[36mINFO[0m[0m 03/12 12:32:07 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/12 12:32:07 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/12 12:32:07 | 
[37m[36mINFO[0m[0m 03/12 12:32:07 | Testenv name escaping te_L100 -> te_L100
[37m[36mINFO[0m[0m 03/12 12:32:07 | Test envs = [0], name = te_L100
[37m[36mINFO[0m[0m 03/12 12:32:07 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/12 12:32:07 | Batch sizes for each domain: [0, 10, 10, 10] (total=30)
[37m[36mINFO[0m[0m 03/12 12:32:07 | steps-per-epoch for each domain: 778.90, 317.60, 470.70 -> min = 317.60
[37m[36mINFO[0m[0m 03/12 12:32:09 | # of params = 86197770
[37m[36mINFO[0m[0m 03/12 12:34:54 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/12 12:34:54 | 0.011600    0.015823    0.107710    0.096476    2.319183    0.011600    0.015823    0.002054    0.003595    0.187657    0.157431    0.133418    0.128401    0           0.000000    2.403039    1.579595    162.731646 
[37m[36mINFO[0m[0m 03/12 12:38:32 | 0.208806    0.184599    0.427692    0.454843    1.552295    0.208806    0.184599    0.565028    0.574217    0.344773    0.372796    0.373274    0.417517    200         0.629723    1.663726    0.255171    166.782850 
[37m[36mINFO[0m[0m 03/12 12:42:09 | 0.220406    0.197257    0.455009    0.469393    1.485362    0.220406    0.197257    0.580306    0.577298    0.356423    0.358942    0.428298    0.471939    400         1.259446    1.567715    0.246779    168.119884 
[37m[36mINFO[0m[0m 03/12 12:45:46 | 0.219615    0.197257    0.459397    0.485588    1.443660    0.219615    0.197257    0.584799    0.585516    0.351071    0.394207    0.442320    0.477041    600         1.889169    1.529881    0.252893    166.290323 
[37m[36mINFO[0m[0m 03/12 12:49:19 | 0.214079    0.188819    0.486648    0.502941    1.376966    0.214079    0.188819    0.604057    0.594248    0.400819    0.430730    0.455067    0.483844    800         2.518892    1.459501    0.251320    162.537461 
[37m[36mINFO[0m[0m 03/12 12:52:53 | 0.207751    0.185654    0.479001    0.495400    1.352526    0.207751    0.185654    0.586982    0.573703    0.393892    0.426952    0.456129    0.485544    1000        3.148615    1.410796    0.257126    162.791821 
[37m[36mINFO[0m[0m 03/12 12:56:29 | 0.213551    0.194093    0.482558    0.492422    1.350171    0.213551    0.194093    0.588907    0.582435    0.399874    0.411839    0.458891    0.482993    1200        3.778338    1.411744    0.245424    167.324325 
[37m[36mINFO[0m[0m 03/12 13:00:00 | 0.179014    0.165612    0.476966    0.501471    1.453458    0.179014    0.165612    0.568622    0.564972    0.425693    0.481108    0.436584    0.458333    1400        4.408060    1.388638    0.245764    161.388754 
[37m[36mINFO[0m[0m 03/12 13:03:35 | 0.214869    0.193038    0.484336    0.509633    1.287337    0.214869    0.193038    0.610091    0.608629    0.375315    0.450882    0.467601    0.469388    1600        5.037783    1.322960    0.246576    165.633122 
[37m[36mINFO[0m[0m 03/12 13:07:11 | 0.211442    0.190928    0.502817    0.534893    1.255171    0.211442    0.190928    0.626396    0.640986    0.415302    0.479849    0.466752    0.483844    1800        5.667506    1.303939    0.249387    166.536957 
[37m[36mINFO[0m[0m 03/12 13:10:44 | 0.212497    0.183544    0.528299    0.549628    1.218588    0.212497    0.183544    0.609321    0.614278    0.490554    0.520151    0.485022    0.514456    2000        6.297229    1.268396    0.249254    163.155350 
[37m[36mINFO[0m[0m 03/12 13:14:20 | 0.162932    0.139241    0.544845    0.550698    1.203361    0.162932    0.139241    0.643985    0.635850    0.490869    0.526448    0.499681    0.489796    2200        6.926952    1.237397    0.248498    166.077264 
[37m[36mINFO[0m[0m 03/12 13:17:59 | 0.135249    0.137131    0.552762    0.571999    1.191476    0.135249    0.137131    0.661702    0.675912    0.510076    0.523929    0.486509    0.516156    2400        7.556675    1.213142    0.241348    170.781034 
[37m[36mINFO[0m[0m 03/12 13:21:34 | 0.220406    0.198312    0.539726    0.546288    1.178827    0.220406    0.198312    0.640647    0.649718    0.457179    0.450882    0.521351    0.538265    2600        8.186398    1.192554    0.243482    166.116866 
[37m[36mINFO[0m[0m 03/12 13:24:57 | 0.147640    0.149789    0.573026    0.571881    1.175057    0.147640    0.149789    0.664784    0.670776    0.534005    0.549118    0.520289    0.495748    2800        8.816121    1.200365    0.172835    168.190120 
[37m[36mINFO[0m[0m 03/12 13:28:11 | 0.141313    0.138186    0.573040    0.587712    1.122471    0.141313    0.138186    0.649121    0.659476    0.552897    0.573048    0.517102    0.530612    3000        9.445844    1.174996    0.168023    161.120636 
[37m[36mINFO[0m[0m 03/12 13:31:29 | 0.168468    0.162447    0.588579    0.592373    1.113077    0.168468    0.162447    0.680318    0.699024    0.542821    0.544081    0.542596    0.534014    3200        10.075567   1.120943    0.166726    164.590613 
[37m[36mINFO[0m[0m 03/12 13:34:48 | 0.058529    0.050633    0.587664    0.601641    1.090961    0.058529    0.050633    0.630376    0.637904    0.574937    0.605793    0.557680    0.561224    3400        10.705290   1.147765    0.164229    165.525906 
