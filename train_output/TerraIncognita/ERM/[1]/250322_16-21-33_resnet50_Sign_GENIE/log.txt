[37m[36mINFO[0m[0m 03/22 16:21:33 | Command :: /jsm0707/GENIE/train_all.py resnet50_Sign_GENIE config/resnet50_Sign_GENIE.yaml --algorithm ERM --test_envs 1 --dataset TerraIncognita --trial_seed 0 --hparams_seed 4
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_Sign_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 4
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_Sign_GENIE
	out_dir: train_output/TerraIncognita/ERM/[1]/250322_16-21-33_resnet50_Sign_GENIE
	out_root: train_output/TerraIncognita/ERM/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250322_16-21-33_resnet50_Sign_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: sign_genie
	freeze_bn: False
	pretrained: True
	lr: 2.0793009436922532e-05
	batch_size: 30
	weight_decay: 0.0007980067844361917
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 03/22 16:21:33 | n_steps = 5001
[37m[36mINFO[0m[0m 03/22 16:21:33 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/22 16:21:33 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/22 16:21:33 | 
[37m[36mINFO[0m[0m 03/22 16:21:33 | Testenv name escaping te_L38 -> te_L38
[37m[36mINFO[0m[0m 03/22 16:21:33 | Test envs = [1], name = te_L38
[37m[36mINFO[0m[0m 03/22 16:21:33 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 03/22 16:21:33 | Batch sizes for each domain: [30, 0, 30, 30] (total=90)
[37m[36mINFO[0m[0m 03/22 16:21:33 | steps-per-epoch for each domain: 126.43, 105.87, 156.90 -> min = 105.87
[37m[36mINFO[0m[0m 03/22 16:21:34 | # of params = 23528522
[37m[36mINFO[0m[0m 03/22 16:24:20 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/22 16:24:20 | 0.019258    0.023112    0.284710    0.289972    2.046840    0.522278    0.520042    0.019258    0.023112    0.096033    0.103275    0.235819    0.246599    0           0.000000    2.437533    1.105174    164.597162 
[37m[36mINFO[0m[0m 03/22 16:27:42 | 0.089485    0.099127    0.575855    0.569347    1.154716    0.736356    0.710970    0.089485    0.099127    0.506612    0.496222    0.484597    0.500850    200         1.889169    1.407649    0.231092    156.582921 
[37m[36mINFO[0m[0m 03/22 16:31:11 | 0.270638    0.289163    0.715984    0.720737    0.794993    0.843396    0.835443    0.270638    0.289163    0.665932    0.681360    0.638623    0.645408    400         3.778338    1.015529    0.223629    163.421980 
[37m[36mINFO[0m[0m 03/22 16:34:40 | 0.350751    0.359527    0.758580    0.750783    0.691900    0.868442    0.851266    0.350751    0.359527    0.716625    0.721662    0.690673    0.679422    600         5.667506    0.771103    0.237656    162.337200 
[37m[36mINFO[0m[0m 03/22 16:38:12 | 0.356015    0.369286    0.775547    0.774027    0.610623    0.886370    0.860759    0.356015    0.369286    0.748111    0.765743    0.692161    0.695578    800         7.556675    0.662239    0.224691    166.425027 
[37m[36mINFO[0m[0m 03/22 16:41:38 | 0.438054    0.448896    0.810851    0.796340    0.568133    0.895597    0.866034    0.438054    0.448896    0.781486    0.764484    0.755471    0.758503    1000        9.445844    0.581850    0.227526    160.581697 
[37m[36mINFO[0m[0m 03/22 16:45:10 | 0.365644    0.378531    0.826577    0.826039    0.516582    0.908779    0.894515    0.365644    0.378531    0.807620    0.812343    0.763331    0.771259    1200        11.335013   0.527473    0.227370    166.540866 
[37m[36mINFO[0m[0m 03/22 16:48:32 | 0.415714    0.428351    0.833054    0.815382    0.501110    0.904034    0.891350    0.415714    0.428351    0.802267    0.770781    0.792862    0.784014    1400        13.224181   0.497973    0.224079    157.557403 
[37m[36mINFO[0m[0m 03/22 16:52:01 | 0.354603    0.382126    0.834874    0.823914    0.516048    0.911152    0.882911    0.354603    0.382126    0.807620    0.802267    0.785851    0.786565    1600        15.113350   0.474105    0.227052    162.965860 
[37m[36mINFO[0m[0m 03/22 16:55:27 | 0.435101    0.444273    0.855889    0.842615    0.465330    0.929080    0.909283    0.435101    0.444273    0.827456    0.804786    0.811132    0.813776    1800        17.002519   0.429450    0.222636    161.683852 
[37m[36mINFO[0m[0m 03/22 16:59:04 | 0.370266    0.383154    0.859647    0.836993    0.459936    0.927762    0.895570    0.370266    0.383154    0.831549    0.818640    0.819630    0.796769    2000        18.891688   0.405426    0.231363    171.084839 
[37m[36mINFO[0m[0m 03/22 17:02:34 | 0.303890    0.325116    0.863315    0.840498    0.456697    0.918534    0.895570    0.303890    0.325116    0.853904    0.827456    0.817506    0.798469    2200        20.780856   0.390179    0.233522    162.705296 
[37m[36mINFO[0m[0m 03/22 17:06:10 | 0.402876    0.417565    0.873358    0.850731    0.418293    0.935144    0.927215    0.402876    0.417565    0.860202    0.835013    0.824729    0.789966    2400        22.670025   0.369964    0.233033    169.518319 
[37m[36mINFO[0m[0m 03/22 17:09:42 | 0.396071    0.422188    0.876180    0.852332    0.400785    0.935407    0.909283    0.396071    0.422188    0.867128    0.852645    0.826004    0.795068    2600        24.559194   0.354024    0.235271    165.592906 
[37m[36mINFO[0m[0m 03/22 17:13:15 | 0.311593    0.324602    0.889925    0.867118    0.375889    0.943053    0.922996    0.311593    0.324602    0.882872    0.850126    0.843850    0.828231    2800        26.448363   0.337999    0.234877    165.647380 
[37m[36mINFO[0m[0m 03/22 17:16:49 | 0.375786    0.413970    0.892403    0.860032    0.388114    0.949380    0.922996    0.375786    0.413970    0.896725    0.850126    0.831103    0.806973    3000        28.337531   0.305571    0.248017    164.047612 
[37m[36mINFO[0m[0m 03/22 17:20:15 | 0.313134    0.330765    0.880733    0.860573    0.398301    0.947535    0.926160    0.313134    0.330765    0.863350    0.837531    0.831315    0.818027    3200        30.226700   0.309312    0.244030    157.695511 
[37m[36mINFO[0m[0m 03/22 17:23:42 | 0.477468    0.490498    0.897364    0.873914    0.379548    0.953071    0.924051    0.477468    0.490498    0.884761    0.867758    0.854260    0.829932    3400        32.115869   0.300664    0.231215    160.160995 
[37m[36mINFO[0m[0m 03/22 17:27:14 | 0.417640    0.429379    0.895499    0.877283    0.370524    0.950699    0.940928    0.417640    0.429379    0.878778    0.865239    0.857021    0.825680    3600        34.005038   0.280605    0.238034    164.325285 
