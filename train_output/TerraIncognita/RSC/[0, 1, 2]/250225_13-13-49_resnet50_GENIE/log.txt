[37m[36mINFO[0m[0m 02/25 13:13:49 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 1 2 --dataset TerraIncognita --trial_seed 1 --hparams_seed 12
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 12
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/RSC/[0, 1, 2]/250225_13-13-49_resnet50_GENIE
	out_root: train_output/TerraIncognita/RSC/[0, 1, 2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 2]
	trial_seed: 1
	unique_name: 250225_13-13-49_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 3.0065625733392338e-05
	batch_size: 34
	weight_decay: 4.89485556145273e-06
	rsc_f_drop_factor: 0.23818860955359455
	rsc_b_drop_factor: 0.46131243462788474
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/25 13:13:49 | n_steps = 5001
[37m[36mINFO[0m[0m 02/25 13:13:49 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/25 13:13:49 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/25 13:13:49 | 
[37m[36mINFO[0m[0m 02/25 13:13:49 | Testenv name escaping te_L100_L38_L43 -> te_L100_L38_L43
[37m[36mINFO[0m[0m 02/25 13:13:49 | Test envs = [0, 1, 2], name = te_L100_L38_L43
[37m[36mINFO[0m[0m 02/25 13:13:49 | Train environments: [3], Test environments: [0, 1, 2]
[37m[36mINFO[0m[0m 02/25 13:13:49 | Batch sizes for each domain: [0, 0, 0, 34] (total=34)
[37m[36mINFO[0m[0m 02/25 13:13:49 | steps-per-epoch for each domain: 138.44 -> min = 138.44
[37m[36mINFO[0m[0m 02/25 13:13:50 | # of params = 23528522
[37m[36mINFO[0m[0m 02/25 13:16:30 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/25 13:16:30 | 0.226456    0.235068    0.273210    0.244048    2.204095    0.513314    0.539030    0.019643    0.022599    0.146411    0.143577    0.273210    0.244048    0           0.000000    3.907873    1.119818    158.922904 
[37m[36mINFO[0m[0m 02/25 13:19:51 | 0.255714    0.268834    0.342469    0.330782    1.646682    0.514105    0.540084    0.020670    0.022085    0.232368    0.244332    0.342469    0.330782    200         1.444657    1.962614    0.233279    154.470983 
[37m[36mINFO[0m[0m 02/25 13:23:29 | 0.262221    0.279003    0.344381    0.341837    1.608845    0.514105    0.540084    0.020670    0.023626    0.251889    0.273300    0.344381    0.341837    400         2.889314    1.837228    0.302168    157.160074 
[37m[36mINFO[0m[0m 02/25 13:26:52 | 0.258047    0.271867    0.349904    0.332483    1.554348    0.514105    0.540084    0.021055    0.023626    0.238980    0.251889    0.349904    0.332483    600         4.333971    1.762632    0.232273    156.151565 
[37m[36mINFO[0m[0m 02/25 13:30:32 | 0.320024    0.322309    0.439771    0.460884    1.516483    0.207751    0.180380    0.439979    0.428865    0.312343    0.357683    0.439771    0.460884    800         5.778628    1.651480    0.278857    164.091048 
[37m[36mINFO[0m[0m 02/25 13:33:52 | 0.270500    0.275718    0.441258    0.465136    1.454849    0.206697    0.181435    0.288997    0.282999    0.315806    0.362720    0.441258    0.465136    1000        7.223284    1.615200    0.246542    151.359786 
[37m[36mINFO[0m[0m 02/25 13:37:31 | 0.306758    0.307429    0.469089    0.485544    1.441670    0.225152    0.206751    0.372705    0.364150    0.322418    0.351385    0.469089    0.485544    1200        8.667941    1.584155    0.261282    166.972874 
[37m[36mINFO[0m[0m 02/25 13:40:54 | 0.306636    0.311200    0.466964    0.483844    1.401424    0.206433    0.179325    0.397355    0.387776    0.316121    0.366499    0.466964    0.483844    1400        10.112598   1.537974    0.246335    153.708909 
[37m[36mINFO[0m[0m 02/25 13:44:31 | 0.323260    0.329165    0.454642    0.486395    1.420688    0.205906    0.180380    0.434844    0.424242    0.329030    0.382872    0.454642    0.486395    1600        11.557255   1.534894    0.250534    166.143356 
[37m[36mINFO[0m[0m 02/25 13:48:06 | 0.328565    0.325962    0.473762    0.486395    1.397983    0.220142    0.197257    0.446912    0.435542    0.318640    0.345088    0.473762    0.486395    1800        13.001912   1.532733    0.265688    161.926626 
[37m[36mINFO[0m[0m 02/25 13:51:34 | 0.331838    0.331523    0.475462    0.483844    1.404149    0.220142    0.198312    0.446656    0.436055    0.328715    0.360202    0.475462    0.483844    2000        14.446569   1.514456    0.261532    156.034108 
[37m[36mINFO[0m[0m 02/25 13:55:08 | 0.324957    0.323158    0.483110    0.489796    1.406852    0.225679    0.205696    0.434330    0.423729    0.314861    0.340050    0.483110    0.489796    2200        15.891226   1.480940    0.271491    160.038290 
[37m[36mINFO[0m[0m 02/25 13:58:38 | 0.282913    0.270754    0.481836    0.494048    1.385548    0.222515    0.204641    0.312620    0.292758    0.313602    0.314861    0.481836    0.494048    2400        17.335883   1.479926    0.238604    162.071059 
[37m[36mINFO[0m[0m 02/25 14:02:22 | 0.332159    0.327061    0.484385    0.504252    1.365298    0.219351    0.195148    0.439594    0.428351    0.337531    0.357683    0.484385    0.504252    2600        18.780540   1.462082    0.311800    161.345256 
[37m[36mINFO[0m[0m 02/25 14:05:51 | 0.324202    0.322276    0.484597    0.497449    1.388706    0.210124    0.180380    0.431249    0.423729    0.331234    0.362720    0.484597    0.497449    2800        20.225197   1.451076    0.231522    162.373414 
[37m[36mINFO[0m[0m 02/25 14:09:29 | 0.332094    0.326573    0.482898    0.493197    1.369233    0.223042    0.196203    0.437283    0.428351    0.335957    0.355164    0.482898    0.493197    3000        21.669853   1.448729    0.279004    162.622629 
[37m[36mINFO[0m[0m 02/25 14:13:05 | 0.307439    0.306195    0.485872    0.500850    1.324195    0.218824    0.196203    0.369110    0.353364    0.334383    0.369018    0.485872    0.500850    3200        23.114510   1.440076    0.263977    163.536719 
[37m[36mINFO[0m[0m 02/25 14:16:21 | 0.292190    0.284826    0.496070    0.511054    1.335028    0.287635    0.281646    0.318783    0.304571    0.270151    0.268262    0.496070    0.511054    3400        24.559167   1.423638    0.235302    148.869647 
[37m[36mINFO[0m[0m 02/25 14:19:58 | 0.330296    0.326956    0.504143    0.503401    1.360264    0.224888    0.204641    0.424060    0.416025    0.341940    0.360202    0.504143    0.503401    3600        26.003824   1.385507    0.273610    161.659373 
[37m[36mINFO[0m[0m 02/25 14:23:26 | 0.319990    0.317593    0.516040    0.522109    1.329299    0.213551    0.193038    0.411093    0.397021    0.335327    0.362720    0.516040    0.522109    3800        27.448481   1.402680    0.245595    159.139996 
[37m[36mINFO[0m[0m 02/25 14:27:00 | 0.311664    0.310176    0.526025    0.529762    1.268361    0.161613    0.145570    0.434587    0.424756    0.338791    0.360202    0.526025    0.529762    4000        28.893138   1.379525    0.268674    159.975408 
