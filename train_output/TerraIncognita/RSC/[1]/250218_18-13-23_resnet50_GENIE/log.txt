[37m[36mINFO[0m[0m 02/18 18:13:23 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 1 --dataset TerraIncognita --trial_seed 0 --hparams_seed 1
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 1
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/RSC/[1]/250218_18-13-23_resnet50_GENIE
	out_root: train_output/TerraIncognita/RSC/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250218_18-13-23_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5.0781288859686544e-05
	batch_size: 44
	weight_decay: 0.00046410133598234803
	rsc_f_drop_factor: 0.2665134852412123
	rsc_b_drop_factor: 0.28056710878365126
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/18 18:13:24 | n_steps = 5001
[37m[36mINFO[0m[0m 02/18 18:13:24 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/18 18:13:24 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/18 18:13:24 | 
[37m[36mINFO[0m[0m 02/18 18:13:24 | Testenv name escaping te_L38 -> te_L38
[37m[36mINFO[0m[0m 02/18 18:13:24 | Test envs = [1], name = te_L38
[37m[36mINFO[0m[0m 02/18 18:13:24 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 02/18 18:13:24 | Batch sizes for each domain: [44, 0, 44, 44] (total=132)
[37m[36mINFO[0m[0m 02/18 18:13:24 | steps-per-epoch for each domain: 86.20, 72.18, 106.98 -> min = 72.18
[37m[36mINFO[0m[0m 02/18 18:13:26 | # of params = 23528522
[37m[36mINFO[0m[0m 02/18 18:16:18 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/18 18:16:18 | 0.019258    0.023112    0.284922    0.289972    2.131721    0.522278    0.520042    0.019258    0.023112    0.096033    0.103275    0.236456    0.246599    0           0.000000    3.347437    2.131327    170.388335 
[37m[36mINFO[0m[0m 02/18 18:20:44 | 0.025934    0.027221    0.356263    0.364510    1.647783    0.499605    0.546414    0.025934    0.027221    0.269207    0.239295    0.299979    0.307823    200         2.770781    1.892962    0.430660    179.852118 
[37m[36mINFO[0m[0m 02/18 18:24:52 | 0.043009    0.042116    0.448604    0.459820    1.424701    0.616135    0.608650    0.043009    0.042116    0.366814    0.401763    0.362864    0.369048    400         5.541562    1.721152    0.366894    174.720947 
[37m[36mINFO[0m[0m 02/18 18:29:13 | 0.027988    0.026708    0.416587    0.412723    1.399479    0.602426    0.583333    0.027988    0.026708    0.333123    0.327456    0.314213    0.327381    600         8.312343    1.565015    0.372153    185.961574 
[37m[36mINFO[0m[0m 02/18 18:33:28 | 0.043908    0.041089    0.463755    0.448923    1.337828    0.610071    0.594937    0.043908    0.041089    0.367128    0.356423    0.414064    0.395408    800         11.083123   1.521938    0.357252    183.885332 
[37m[36mINFO[0m[0m 02/18 18:37:49 | 0.047631    0.045711    0.506974    0.514860    1.245368    0.669127    0.681435    0.047631    0.045711    0.422859    0.429471    0.428936    0.433673    1000        13.853904   1.466619    0.477766    165.747232 
[37m[36mINFO[0m[0m 02/18 18:42:23 | 0.041982    0.046739    0.538518    0.515953    1.199213    0.710783    0.687764    0.041982    0.046739    0.434194    0.401763    0.470576    0.458333    1200        16.624685   1.418195    0.450440    183.273159 
[37m[36mINFO[0m[0m 02/18 18:46:32 | 0.033252    0.039034    0.588332    0.589680    1.082530    0.745584    0.742616    0.033252    0.039034    0.511020    0.494962    0.508392    0.531463    1400        19.395466   1.353647    0.367287    175.876472 
[37m[36mINFO[0m[0m 02/18 18:50:42 | 0.040442    0.043143    0.622727    0.621137    0.993485    0.746902    0.727848    0.040442    0.043143    0.555101    0.571788    0.566178    0.563776    1600        22.166247   1.263307    0.380409    173.452901 
[37m[36mINFO[0m[0m 02/18 18:55:06 | 0.074849    0.083719    0.642412    0.641097    0.918478    0.762457    0.775316    0.074849    0.083719    0.559509    0.565491    0.605269    0.582483    1800        24.937028   1.199984    0.401506    184.061485 
[37m[36mINFO[0m[0m 02/18 18:59:34 | 0.161381    0.173087    0.633889    0.631542    0.876164    0.756393    0.751055    0.161381    0.173087    0.547229    0.544081    0.598045    0.599490    2000        27.707809   1.133051    0.423676    183.513390 
[37m[36mINFO[0m[0m 02/18 19:03:53 | 0.217871    0.240883    0.697909    0.696611    0.798561    0.826786    0.819620    0.217871    0.240883    0.640428    0.648615    0.626514    0.621599    2200        30.478589   1.086832    0.405812    177.203876 
[37m[36mINFO[0m[0m 02/18 19:08:12 | 0.353319    0.373395    0.674697    0.668292    0.877625    0.719747    0.718354    0.353319    0.373395    0.665932    0.670025    0.638411    0.616497    2400        33.249370   1.032917    0.416651    176.109830 
[37m[36mINFO[0m[0m 02/18 19:12:26 | 0.137116    0.152542    0.746738    0.753464    0.712321    0.815713    0.812236    0.137116    0.152542    0.743388    0.764484    0.681113    0.683673    2600        36.020151   0.967377    0.398692    174.461243 
[37m[36mINFO[0m[0m 02/18 19:16:53 | 0.138914    0.155110    0.769463    0.770417    0.655420    0.847878    0.841772    0.138914    0.155110    0.752204    0.759446    0.708307    0.710034    2800        38.790932   0.899055    0.468055    172.762040 
[37m[36mINFO[0m[0m 02/18 19:21:21 | 0.302606    0.320493    0.778961    0.769273    0.642922    0.842341    0.824895    0.302606    0.320493    0.775189    0.772040    0.719354    0.710884    3000        41.561713   0.827462    0.497215    168.874588 
[37m[36mINFO[0m[0m 02/18 19:25:54 | 0.270253    0.286081    0.787621    0.782814    0.624467    0.859214    0.864979    0.270253    0.286081    0.777708    0.763224    0.725940    0.720238    3200        44.332494   0.805618    0.487264    175.298345 
[37m[36mINFO[0m[0m 02/18 19:30:23 | 0.299397    0.315357    0.801283    0.782516    0.648163    0.847878    0.836498    0.299397    0.315357    0.790302    0.767003    0.765668    0.744048    3400        47.103275   0.757264    0.458279    177.203292 
[37m[36mINFO[0m[0m 02/18 19:34:35 | 0.342406    0.362096    0.803822    0.803302    0.584667    0.848669    0.845992    0.342406    0.362096    0.823048    0.822418    0.739749    0.741497    3600        49.874055   0.733903    0.406878    171.358120 
[37m[36mINFO[0m[0m 02/18 19:38:54 | 0.194633    0.200822    0.814012    0.813201    0.542287    0.853414    0.848101    0.194633    0.200822    0.822103    0.826196    0.766518    0.765306    3800        52.644836   0.710916    0.403745    177.792419 
[37m[36mINFO[0m[0m 02/18 19:43:09 | 0.293362    0.306112    0.837359    0.829397    0.484670    0.872397    0.871308    0.293362    0.306112    0.842569    0.836272    0.797111    0.780612    4000        55.415617   0.708447    0.388628    177.646761 
[37m[36mINFO[0m[0m 02/18 19:47:32 | 0.291180    0.304571    0.812127    0.797442    0.680388    0.851305    0.853376    0.291180    0.304571    0.807935    0.779597    0.777140    0.759354    4200        58.186398   0.671053    0.420808    178.921529 
