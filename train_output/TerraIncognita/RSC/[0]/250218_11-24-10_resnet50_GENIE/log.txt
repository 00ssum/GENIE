[37m[36mINFO[0m[0m 02/18 11:24:10 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 --dataset TerraIncognita --trial_seed 0 --hparams_seed 3
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 3
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/RSC/[0]/250218_11-24-10_resnet50_GENIE
	out_root: train_output/TerraIncognita/RSC/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250218_11-24-10_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 6.166705009429313e-05
	batch_size: 14
	weight_decay: 3.4412812120883604e-06
	rsc_f_drop_factor: 0.2173803526378828
	rsc_b_drop_factor: 0.2877436135895483
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/18 11:24:10 | n_steps = 5001
[37m[36mINFO[0m[0m 02/18 11:24:10 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/18 11:24:10 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/18 11:24:10 | 
[37m[36mINFO[0m[0m 02/18 11:24:11 | Testenv name escaping te_L100 -> te_L100
[37m[36mINFO[0m[0m 02/18 11:24:11 | Test envs = [0], name = te_L100
[37m[36mINFO[0m[0m 02/18 11:24:11 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 02/18 11:24:11 | Batch sizes for each domain: [0, 14, 14, 14] (total=42)
[37m[36mINFO[0m[0m 02/18 11:24:11 | steps-per-epoch for each domain: 556.36, 226.86, 336.21 -> min = 226.86
[37m[36mINFO[0m[0m 02/18 11:24:12 | # of params = 23528522
[37m[36mINFO[0m[0m 02/18 11:27:14 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/18 11:27:14 | 0.204060    0.212025    0.304747    0.308719    2.124143    0.204060    0.212025    0.456670    0.486903    0.231738    0.229219    0.225834    0.210034    0           0.000000    3.151174    1.915724    180.445417 
[37m[36mINFO[0m[0m 02/18 11:31:05 | 0.210124    0.219409    0.402115    0.420899    1.494312    0.210124    0.219409    0.569264    0.595275    0.329030    0.374055    0.308052    0.293367    200         0.881612    1.932066    0.237632    183.312198 
[37m[36mINFO[0m[0m 02/18 11:34:53 | 0.210388    0.218354    0.410312    0.410239    1.426457    0.210388    0.218354    0.580049    0.600924    0.355793    0.357683    0.295092    0.272109    400         1.763224    1.712149    0.251486    177.683494 
[37m[36mINFO[0m[0m 02/18 11:38:45 | 0.211442    0.221519    0.451680    0.455131    1.341755    0.211442    0.221519    0.579792    0.604520    0.357997    0.347607    0.417251    0.413265    600         2.644836    1.608074    0.246630    182.614573 
[37m[36mINFO[0m[0m 02/18 11:42:21 | 0.206433    0.212025    0.457445    0.481962    1.312225    0.206433    0.212025    0.582360    0.606574    0.352330    0.397985    0.437646    0.441327    800         3.526448    1.541286    0.184175    178.731926 
[37m[36mINFO[0m[0m 02/18 11:46:14 | 0.215924    0.217300    0.498946    0.503753    1.268551    0.215924    0.217300    0.583387    0.610169    0.462846    0.454660    0.450605    0.446429    1000        4.408060    1.483676    0.250857    183.060703 
[37m[36mINFO[0m[0m 02/18 11:50:06 | 0.209597    0.215190    0.502484    0.514620    1.236878    0.209597    0.215190    0.581076    0.606574    0.430730    0.416877    0.495645    0.520408    1200        5.289673    1.449383    0.248216    182.796854 
[37m[36mINFO[0m[0m 02/18 11:53:56 | 0.180859    0.176160    0.581687    0.578097    1.142404    0.180859    0.176160    0.625241    0.619414    0.570214    0.560453    0.549607    0.554422    1400        6.171285    1.389540    0.171580    194.941249 
[37m[36mINFO[0m[0m 02/18 11:57:44 | 0.233061    0.233122    0.596432    0.600502    1.010504    0.233061    0.233122    0.669406    0.659990    0.546914    0.571788    0.572976    0.569728    1600        7.052897    1.311346    0.167960    194.456051 
[37m[36mINFO[0m[0m 02/18 12:01:28 | 0.330082    0.336498    0.625945    0.617901    0.969180    0.330082    0.336498    0.707921    0.695429    0.573992    0.557935    0.595921    0.600340    1800        7.934509    1.257971    0.172406    189.707278 
[37m[36mINFO[0m[0m 02/18 12:05:17 | 0.330873    0.330169    0.649497    0.649010    0.952169    0.330873    0.330169    0.711003    0.713405    0.616499    0.617128    0.620990    0.616497    2000        8.816121    1.196533    0.198234    189.179715 
[37m[36mINFO[0m[0m 02/18 12:09:07 | 0.247561    0.250000    0.645966    0.636517    0.938944    0.247561    0.250000    0.735268    0.717001    0.635390    0.637280    0.567240    0.555272    2200        9.697733    1.114896    0.176891    194.980678 
[37m[36mINFO[0m[0m 02/18 12:12:51 | 0.359610    0.390295    0.700687    0.725673    0.780743    0.359610    0.390295    0.759661    0.770930    0.681045    0.729219    0.661355    0.676871    2400        10.579345   1.088643    0.169430    190.166484 
[37m[36mINFO[0m[0m 02/18 12:16:37 | 0.616135    0.624473    0.719284    0.726078    0.751360    0.616135    0.624473    0.764540    0.781715    0.704975    0.712846    0.688337    0.683673    2600        11.460957   1.037673    0.194844    186.691151 
[37m[36mINFO[0m[0m 02/18 12:20:25 | 0.506987    0.508439    0.721922    0.732112    0.749158    0.506987    0.508439    0.763256    0.771443    0.708438    0.721662    0.694073    0.703231    2800        12.342569   0.976846    0.253272    177.388566 
[37m[36mINFO[0m[0m 02/18 12:24:12 | 0.397311    0.416667    0.690476    0.692151    0.795457    0.397311    0.416667    0.752728    0.761685    0.681990    0.688917    0.636711    0.625850    3000        13.224181   0.951781    0.247196    177.512625 
[37m[36mINFO[0m[0m 02/18 12:27:56 | 0.541524    0.546414    0.716000    0.717528    0.839208    0.541524    0.546414    0.756965    0.769389    0.717569    0.724181    0.673465    0.659014    3200        14.105793   0.918059    0.203206    183.019370 
[37m[36mINFO[0m[0m 02/18 12:31:43 | 0.455312    0.448312    0.753910    0.757834    0.704577    0.455312    0.448312    0.772243    0.777093    0.745277    0.748111    0.744211    0.748299    3400        14.987406   0.887295    0.250799    176.976301 
[37m[36mINFO[0m[0m 02/18 12:35:27 | 0.525705    0.535865    0.759221    0.763073    0.667459    0.525705    0.535865    0.785467    0.798665    0.749685    0.740554    0.742511    0.750000    3600        15.869018   0.864444    0.201508    184.348955 
[37m[36mINFO[0m[0m 02/18 12:39:05 | 0.506196    0.518987    0.743048    0.743276    0.734732    0.506196    0.518987    0.781230    0.788906    0.731108    0.731738    0.716805    0.709184    3800        16.750630   0.882511    0.184991    180.962133 
[37m[36mINFO[0m[0m 02/18 12:42:39 | 0.559188    0.555907    0.755330    0.765914    0.671499    0.559188    0.555907    0.778791    0.791988    0.767632    0.782116    0.719567    0.723639    4000        17.632242   0.842749    0.173443    179.235885 
[37m[36mINFO[0m[0m 02/18 12:46:28 | 0.577116    0.568565    0.764973    0.771098    0.654739    0.577116    0.568565    0.794069    0.804828    0.763224    0.769521    0.737625    0.738946    4200        18.513854   0.845163    0.205636    187.421813 
[37m[36mINFO[0m[0m 02/18 12:50:19 | 0.581070    0.568565    0.781962    0.783037    0.641701    0.581070    0.568565    0.797792    0.802260    0.791562    0.793451    0.756533    0.753401    4400        19.395466   0.797670    0.247165    181.753247 
[37m[36mINFO[0m[0m 02/18 12:54:01 | 0.436594    0.447257    0.777293    0.775032    0.650354    0.436594    0.447257    0.800873    0.809450    0.772985    0.773300    0.758020    0.742347    4600        20.277078   0.766916    0.180776    185.830655 
[37m[36mINFO[0m[0m 02/18 12:57:54 | 0.597153    0.602321    0.783964    0.772676    0.637122    0.597153    0.602321    0.800745    0.805342    0.793766    0.772040    0.757383    0.740646    4800        21.158690   0.747933    0.186089    195.673591 
