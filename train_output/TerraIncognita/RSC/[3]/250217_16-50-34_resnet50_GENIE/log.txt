[37m[36mINFO[0m[0m 02/17 16:50:34 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 3 --dataset TerraIncognita --trial_seed 1 --hparams_seed 18
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 18
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/RSC/[3]/250217_16-50-34_resnet50_GENIE
	out_root: train_output/TerraIncognita/RSC/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 1
	unique_name: 250217_16-50-34_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.0002805925701721472
	batch_size: 39
	weight_decay: 1.5909976529017505e-06
	rsc_f_drop_factor: 0.2169985151107509
	rsc_b_drop_factor: 0.17705472375070025
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/17 16:50:34 | n_steps = 5001
[37m[36mINFO[0m[0m 02/17 16:50:34 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/17 16:50:34 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/17 16:50:34 | 
[37m[36mINFO[0m[0m 02/17 16:50:34 | Testenv name escaping te_L46 -> te_L46
[37m[36mINFO[0m[0m 02/17 16:50:34 | Test envs = [3], name = te_L46
[37m[36mINFO[0m[0m 02/17 16:50:34 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 02/17 16:50:34 | Batch sizes for each domain: [39, 39, 39, 0] (total=117)
[37m[36mINFO[0m[0m 02/17 16:50:34 | steps-per-epoch for each domain: 97.26, 199.72, 81.44 -> min = 81.44
[37m[36mINFO[0m[0m 02/17 16:50:36 | # of params = 23528522
[37m[36mINFO[0m[0m 02/17 16:53:35 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/17 16:53:35 | 0.217336    0.229592    0.286166    0.283001    2.090735    0.208278    0.179325    0.461933    0.455573    0.188287    0.214106    0.217336    0.229592    0           0.000000    2.933114    1.158475    178.275639 
[37m[36mINFO[0m[0m 02/17 16:57:43 | 0.227746    0.244048    0.524368    0.514643    1.239633    0.686791    0.642405    0.574913    0.566513    0.311398    0.335013    0.227746    0.244048    200         2.455919    1.622929    0.355114    177.104778 
[37m[36mINFO[0m[0m 02/17 17:01:44 | 0.258126    0.268707    0.569829    0.581221    1.141967    0.658054    0.690928    0.608422    0.596816    0.443010    0.455919    0.258126    0.268707    400         4.911839    1.321634    0.315782    177.490924 
[37m[36mINFO[0m[0m 02/17 17:05:48 | 0.266624    0.267857    0.655220    0.656211    0.971564    0.771685    0.770042    0.640134    0.636877    0.553841    0.561713    0.266624    0.267857    600         7.367758    1.203355    0.370039    170.472293 
[37m[36mINFO[0m[0m 02/17 17:10:01 | 0.261100    0.256803    0.664096    0.660018    0.888427    0.812286    0.805907    0.650404    0.636364    0.529597    0.537783    0.261100    0.256803    800         9.823678    1.103719    0.385390    175.349004 
[37m[36mINFO[0m[0m 02/17 17:13:58 | 0.318887    0.317177    0.735239    0.727311    0.748922    0.849723    0.839662    0.715881    0.694915    0.640113    0.647355    0.318887    0.317177    1000        12.279597   0.927821    0.334238    170.770813 
[37m[36mINFO[0m[0m 02/17 17:18:11 | 0.330996    0.328231    0.755122    0.759928    0.635252    0.822041    0.820675    0.741815    0.732409    0.701511    0.726700    0.330996    0.328231    1200        14.735516   0.837961    0.386118    175.725780 
[37m[36mINFO[0m[0m 02/17 17:22:16 | 0.323136    0.321429    0.784004    0.792347    0.582949    0.875297    0.861814    0.759148    0.782229    0.717569    0.732997    0.323136    0.321429    1400        17.191436   0.741371    0.328254    179.283540 
[37m[36mINFO[0m[0m 02/17 17:26:19 | 0.288082    0.280612    0.808194    0.804352    0.559064    0.880042    0.862869    0.798947    0.805855    0.745592    0.744332    0.288082    0.280612    1600        19.647355   0.691114    0.321306    178.327483 
[37m[36mINFO[0m[0m 02/17 17:30:09 | 0.319312    0.321429    0.814049    0.815908    0.517451    0.889533    0.871308    0.802927    0.820750    0.749685    0.755668    0.319312    0.321429    1800        22.103275   0.627783    0.304422    169.354510 
[37m[36mINFO[0m[0m 02/17 17:34:17 | 0.372636    0.363946    0.830645    0.823568    0.474126    0.906407    0.885021    0.798690    0.804828    0.786839    0.780856    0.372636    0.363946    2000        24.559194   0.611505    0.353705    176.791168 
[37m[36mINFO[0m[0m 02/17 17:38:33 | 0.374973    0.369048    0.843295    0.847396    0.429755    0.910625    0.891350    0.820773    0.839753    0.798489    0.811083    0.374973    0.369048    2200        27.015113   0.568726    0.366876    183.140287 
[37m[36mINFO[0m[0m 02/17 17:42:48 | 0.332908    0.328231    0.854426    0.852156    0.444112    0.914579    0.907173    0.825652    0.838213    0.823048    0.811083    0.332908    0.328231    2400        29.471033   0.543354    0.370659    180.308914 
[37m[36mINFO[0m[0m 02/17 17:47:01 | 0.380285    0.379252    0.854291    0.852129    0.444935    0.911679    0.900844    0.824368    0.835645    0.826826    0.819899    0.380285    0.379252    2600        31.926952   0.528920    0.381400    177.295038 
[37m[36mINFO[0m[0m 02/17 17:51:14 | 0.355641    0.349490    0.865805    0.861012    0.460350    0.915370    0.906118    0.830659    0.846944    0.851385    0.829975    0.355641    0.349490    2800        34.382872   0.503293    0.353004    182.298086 
[37m[36mINFO[0m[0m 02/17 17:55:19 | 0.360102    0.356293    0.858651    0.858405    0.423757    0.916161    0.904008    0.812813    0.824859    0.846977    0.846348    0.360102    0.356293    3000        36.838791   0.479312    0.387974    166.977703 
[37m[36mINFO[0m[0m 02/17 17:59:31 | 0.371999    0.369048    0.874763    0.864840    0.414661    0.918007    0.890295    0.850173    0.865434    0.856108    0.838791    0.371999    0.369048    3200        39.294710   0.477329    0.385712    175.023203 
[37m[36mINFO[0m[0m 02/17 18:03:43 | 0.375611    0.362245    0.872474    0.875250    0.360787    0.926180    0.910338    0.854025    0.857730    0.837217    0.857683    0.375611    0.362245    3400        41.750630   0.461255    0.397515    172.995372 
[37m[36mINFO[0m[0m 02/17 18:08:01 | 0.340769    0.344388    0.885135    0.864631    0.405139    0.930398    0.909283    0.848119    0.840781    0.876889    0.843829    0.340769    0.344388    3600        44.206549   0.457226    0.393934    178.677241 
