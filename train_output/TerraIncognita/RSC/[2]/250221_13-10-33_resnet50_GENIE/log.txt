[37m[36mINFO[0m[0m 02/21 13:10:33 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 2 --dataset TerraIncognita --trial_seed 1 --hparams_seed 10
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 10
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/RSC/[2]/250221_13-10-33_resnet50_GENIE
	out_root: train_output/TerraIncognita/RSC/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 1
	unique_name: 250221_13-10-33_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 2.2128485336217652e-05
	batch_size: 9
	weight_decay: 0.00030907441430549757
	rsc_f_drop_factor: 0.11930737869274544
	rsc_b_drop_factor: 0.16188382214285657
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/21 13:10:33 | n_steps = 5001
[37m[36mINFO[0m[0m 02/21 13:10:33 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/21 13:10:33 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/21 13:10:33 | 
[37m[36mINFO[0m[0m 02/21 13:10:33 | Testenv name escaping te_L43 -> te_L43
[37m[36mINFO[0m[0m 02/21 13:10:33 | Test envs = [2], name = te_L43
[37m[36mINFO[0m[0m 02/21 13:10:33 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 02/21 13:10:33 | Batch sizes for each domain: [9, 9, 0, 9] (total=27)
[37m[36mINFO[0m[0m 02/21 13:10:33 | steps-per-epoch for each domain: 421.44, 865.44, 523.00 -> min = 421.44
[37m[36mINFO[0m[0m 02/21 13:10:35 | # of params = 23528522
[37m[36mINFO[0m[0m 02/21 13:13:51 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/21 13:13:51 | 0.098237    0.094458    0.258356    0.260020    2.041634    0.515950    0.541139    0.019900    0.022085    0.098237    0.094458    0.239218    0.216837    0           0.000000    2.809121    1.912788    193.961670 
[37m[36mINFO[0m[0m 02/21 13:17:42 | 0.185139    0.210327    0.475464    0.490554    1.447571    0.591089    0.610759    0.528951    0.515665    0.185139    0.210327    0.306352    0.345238    200         0.474558    1.731327    0.190202    193.272780 
[37m[36mINFO[0m[0m 02/21 13:21:37 | 0.329030    0.332494    0.484028    0.446502    1.333492    0.436594    0.300633    0.574657    0.578839    0.329030    0.332494    0.440833    0.460034    400         0.949117    1.484701    0.232613    188.944178 
[37m[36mINFO[0m[0m 02/21 13:25:31 | 0.184194    0.188917    0.532534    0.532684    1.349263    0.613235    0.612869    0.566055    0.560863    0.184194    0.188917    0.418313    0.424320    600         1.423675    1.346994    0.210006    192.001268 
[37m[36mINFO[0m[0m 02/21 13:29:14 | 0.281801    0.278338    0.566960    0.579535    1.190262    0.672291    0.694093    0.570548    0.557268    0.281801    0.278338    0.458041    0.487245    800         1.898234    1.307511    0.206749    180.926754 
[37m[36mINFO[0m[0m 02/21 13:32:50 | 0.290932    0.314861    0.579476    0.552676    1.150776    0.691801    0.694093    0.609192    0.588084    0.290932    0.314861    0.437434    0.375850    1000        2.372792    1.269866    0.215553    173.052851 
[37m[36mINFO[0m[0m 02/21 13:36:32 | 0.297544    0.322418    0.598770    0.605542    1.077360    0.714210    0.722574    0.602388    0.590652    0.297544    0.322418    0.479711    0.503401    1200        2.847350    1.211136    0.210040    180.446941 
[37m[36mINFO[0m[0m 02/21 13:40:13 | 0.263539    0.250630    0.615711    0.618788    1.055732    0.756921    0.761603    0.602003    0.594761    0.263539    0.250630    0.488209    0.500000    1400        3.321909    1.134827    0.212465    178.387468 
[37m[36mINFO[0m[0m 02/21 13:44:03 | 0.297229    0.295970    0.655093    0.654479    0.948860    0.802531    0.799578    0.626525    0.628146    0.297229    0.295970    0.536223    0.535714    1600        3.796467    1.107590    0.201698    189.304270 
[37m[36mINFO[0m[0m 02/21 13:47:47 | 0.282116    0.288413    0.635242    0.644623    1.046758    0.759557    0.786920    0.649249    0.668207    0.282116    0.288413    0.496919    0.478741    1800        4.271026    1.075444    0.196263    184.893169 
[37m[36mINFO[0m[0m 02/21 13:51:35 | 0.331234    0.358942    0.676297    0.685836    0.859914    0.845505    0.833333    0.702401    0.721623    0.331234    0.358942    0.480986    0.502551    2000        4.745584    0.985852    0.189481    189.888320 
[37m[36mINFO[0m[0m 02/21 13:55:11 | 0.418136    0.430730    0.727225    0.721039    0.783311    0.860005    0.842827    0.741045    0.751412    0.418136    0.430730    0.580625    0.568878    2200        5.220142    0.952765    0.179805    180.734584 
[37m[36mINFO[0m[0m 02/21 13:59:02 | 0.349181    0.363980    0.748004    0.747340    0.716357    0.846559    0.837553    0.756066    0.776066    0.349181    0.363980    0.641385    0.628401    2400        5.694701    0.842822    0.190528    192.124378 
[37m[36mINFO[0m[0m 02/21 14:02:38 | 0.424433    0.438287    0.732059    0.733486    0.750773    0.811495    0.803797    0.741173    0.752953    0.424433    0.438287    0.643510    0.643707    2600        6.169259    0.810025    0.186217    178.942499 
[37m[36mINFO[0m[0m 02/21 14:06:32 | 0.348866    0.345088    0.729171    0.735485    0.748365    0.823886    0.844937    0.720118    0.736518    0.348866    0.345088    0.643510    0.625000    2800        6.643818    0.798211    0.200656    193.556009 
[37m[36mINFO[0m[0m 02/21 14:10:21 | 0.420340    0.435768    0.724441    0.721774    0.763760    0.825204    0.834388    0.755809    0.766307    0.420340    0.435768    0.592309    0.564626    3000        7.118376    0.782719    0.187230    191.821363 
[37m[36mINFO[0m[0m 02/21 14:14:00 | 0.443955    0.453401    0.783049    0.773664    0.619522    0.880833    0.876582    0.784440    0.798151    0.443955    0.453401    0.683875    0.646259    3200        7.592934    0.749508    0.198447    179.394134 
[37m[36mINFO[0m[0m 02/21 14:17:44 | 0.357997    0.362720    0.742638    0.726777    0.750709    0.853941    0.849156    0.728977    0.725732    0.357997    0.362720    0.644997    0.605442    3400        8.067493    0.744059    0.191078    186.004116 
[37m[36mINFO[0m[0m 02/21 14:21:31 | 0.402708    0.411839    0.765782    0.751846    0.678202    0.858951    0.854430    0.753884    0.756549    0.402708    0.411839    0.684512    0.644558    3600        8.542051    0.723943    0.192553    188.678562 
[37m[36mINFO[0m[0m 02/21 14:25:16 | 0.479534    0.484887    0.766460    0.752412    0.696346    0.866069    0.856540    0.767493    0.771443    0.479534    0.484887    0.665817    0.629252    3800        9.016610    0.697004    0.181870    188.385321 
[37m[36mINFO[0m[0m 02/21 14:28:59 | 0.479849    0.494962    0.795480    0.777318    0.593169    0.880833    0.852321    0.776480    0.775552    0.479849    0.494962    0.729127    0.704082    4000        9.491168    0.686906    0.179404    186.578772 
[37m[36mINFO[0m[0m 02/21 14:32:44 | 0.443010    0.457179    0.788993    0.775073    0.615366    0.876351    0.859705    0.767236    0.775039    0.443010    0.457179    0.723391    0.690476    4200        9.965726    0.676923    0.185592    188.071417 
[37m[36mINFO[0m[0m 02/21 14:36:23 | 0.440806    0.460957    0.798852    0.789040    0.592627    0.886106    0.864979    0.788548    0.809964    0.440806    0.460957    0.721904    0.692177    4400        10.440285   0.631576    0.150793    189.227070 
