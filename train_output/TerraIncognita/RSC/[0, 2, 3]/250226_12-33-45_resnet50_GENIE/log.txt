[37m[36mINFO[0m[0m 02/26 12:33:45 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 2 3 --dataset TerraIncognita --trial_seed 0 --hparams_seed 0
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/RSC/[0, 2, 3]/250226_12-33-45_resnet50_GENIE
	out_root: train_output/TerraIncognita/RSC/[0, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 2, 3]
	trial_seed: 0
	unique_name: 250226_12-33-45_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	rsc_f_drop_factor: 0.3333333333333333
	rsc_b_drop_factor: 0.3333333333333333
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/26 12:33:45 | n_steps = 5001
[37m[36mINFO[0m[0m 02/26 12:33:45 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/26 12:33:45 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/26 12:33:45 | 
[37m[36mINFO[0m[0m 02/26 12:33:45 | Testenv name escaping te_L100_L43_L46 -> te_L100_L43_L46
[37m[36mINFO[0m[0m 02/26 12:33:45 | Test envs = [0, 2, 3], name = te_L100_L43_L46
[37m[36mINFO[0m[0m 02/26 12:33:45 | Train environments: [1], Test environments: [0, 2, 3]
[37m[36mINFO[0m[0m 02/26 12:33:45 | Batch sizes for each domain: [0, 32, 0, 0] (total=32)
[37m[36mINFO[0m[0m 02/26 12:33:45 | steps-per-epoch for each domain: 243.41 -> min = 243.41
[37m[36mINFO[0m[0m 02/26 12:33:47 | # of params = 23528522
[37m[36mINFO[0m[0m 02/26 12:36:37 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/26 12:36:37 | 0.203084    0.213519    0.455514    0.481253    1.882461    0.200633    0.208861    0.455514    0.481253    0.186398    0.221662    0.222222    0.210034    0           0.000000    3.553550    1.727355    169.055440 
[37m[36mINFO[0m[0m 02/26 12:40:26 | 0.217400    0.223077    0.574015    0.589625    1.093058    0.208015    0.216245    0.574015    0.589625    0.187972    0.219144    0.256214    0.233844    200         0.821672    1.544170    0.298594    169.005417 
[37m[36mINFO[0m[0m 02/26 12:44:24 | 0.213901    0.220605    0.575298    0.590652    1.061265    0.208542    0.213080    0.575298    0.590652    0.186083    0.219144    0.247079    0.229592    400         1.643343    1.358387    0.344896    169.126189 
[37m[36mINFO[0m[0m 02/26 12:48:04 | 0.202100    0.210207    0.582488    0.606574    1.069406    0.203796    0.207806    0.582488    0.606574    0.182620    0.217884    0.219885    0.204932    600         2.465015    1.338175    0.257739    168.011057 
[37m[36mINFO[0m[0m 02/26 12:51:49 | 0.187736    0.193071    0.588266    0.614792    1.065045    0.176378    0.175105    0.588266    0.614792    0.178841    0.217884    0.207988    0.186224    800         3.286686    1.321258    0.258928    173.182227 
[37m[36mINFO[0m[0m 02/26 12:55:29 | 0.198334    0.205217    0.611889    0.624037    1.055416    0.193778    0.189873    0.611889    0.624037    0.183249    0.219144    0.217973    0.206633    1000        4.108358    1.283996    0.267378    166.350809 
[37m[36mINFO[0m[0m 02/26 12:59:15 | 0.198390    0.206465    0.584799    0.607088    1.048842    0.199842    0.202532    0.584799    0.607088    0.178841    0.217884    0.216486    0.198980    1200        4.930030    1.275872    0.259105    174.781580 
[37m[36mINFO[0m[0m 02/26 13:02:53 | 0.203243    0.213031    0.614328    0.624037    1.088162    0.204587    0.209916    0.614328    0.624037    0.184194    0.219144    0.220948    0.210034    1400        5.751701    1.247697    0.251497    167.787585 
[37m[36mINFO[0m[0m 02/26 13:06:47 | 0.189891    0.196710    0.592759    0.609142    1.054785    0.163723    0.165612    0.592759    0.609142    0.178841    0.217884    0.227109    0.206633    1600        6.573373    1.238981    0.266576    180.597646 
[37m[36mINFO[0m[0m 02/26 13:10:50 | 0.190665    0.196304    0.618051    0.639959    1.011482    0.187714    0.185654    0.618051    0.639959    0.178841    0.217884    0.205439    0.185374    1800        7.395044    1.210060    0.242532    194.119152 
[37m[36mINFO[0m[0m 02/26 13:14:33 | 0.191617    0.198913    0.622159    0.639445    1.007180    0.188241    0.190928    0.622159    0.639445    0.179471    0.217884    0.207138    0.187925    2000        8.216716    1.205824    0.248557    173.064876 
[37m[36mINFO[0m[0m 02/26 13:18:18 | 0.174061    0.181027    0.637823    0.652799    0.993767    0.143686    0.148734    0.637823    0.652799    0.182620    0.216625    0.195878    0.177721    2200        9.038387    1.187993    0.249557    175.580145 
[37m[36mINFO[0m[0m 02/26 13:22:05 | 0.165200    0.172907    0.614585    0.627632    0.966868    0.147377    0.141350    0.614585    0.627632    0.173804    0.214106    0.174421    0.163265    2400        9.860059    1.146290    0.291967    168.519386 
[37m[36mINFO[0m[0m 02/26 13:25:49 | 0.078443    0.085381    0.667095    0.675398    0.919699    0.067229    0.070675    0.667095    0.675398    0.092254    0.125945    0.075844    0.059524    2600        10.681731   1.149374    0.314556    161.589325 
[37m[36mINFO[0m[0m 02/26 13:29:30 | 0.094729    0.092348    0.662858    0.684129    0.959669    0.122858    0.113924    0.662858    0.684129    0.063602    0.078086    0.097727    0.085034    2800        11.503402   1.136484    0.295192    161.826765 
[37m[36mINFO[0m[0m 02/26 13:33:21 | 0.077530    0.077515    0.728078    0.728300    0.838314    0.123385    0.120253    0.728078    0.728300    0.052267    0.062972    0.056936    0.049320    3000        12.325074   1.121650    0.331610    164.224523 
[37m[36mINFO[0m[0m 02/26 13:37:03 | 0.080685    0.079511    0.730004    0.733950    0.831690    0.128131    0.114979    0.730004    0.733950    0.092254    0.113350    0.021670    0.010204    3200        13.146745   1.081954    0.272242    167.840552 
[37m[36mINFO[0m[0m 02/26 13:40:39 | 0.053825    0.058502    0.659391    0.722650    0.898630    0.050620    0.058017    0.659391    0.722650    0.047544    0.055416    0.063310    0.062075    3400        13.968417   1.107294    0.275708    160.623014 
[37m[36mINFO[0m[0m 02/26 13:44:14 | 0.168346    0.173273    0.727821    0.732409    0.805727    0.182178    0.168776    0.727821    0.732409    0.180730    0.211587    0.142129    0.139456    3600        14.790089   0.940231    0.238669    167.469749 
[37m[36mINFO[0m[0m 02/26 13:47:56 | 0.105846    0.116755    0.746052    0.745763    0.749700    0.147640    0.147679    0.746052    0.745763    0.145466    0.183879    0.024432    0.018707    3800        15.611760   0.882840    0.237450    174.316668 
[37m[36mINFO[0m[0m 02/26 13:51:35 | 0.106277    0.115840    0.742586    0.738059    0.738177    0.143686    0.155063    0.742586    0.738059    0.124370    0.144836    0.050775    0.047619    4000        16.433432   0.929395    0.274975    163.640106 
[37m[36mINFO[0m[0m 02/26 13:55:11 | 0.139160    0.141045    0.751830    0.759117    0.734228    0.169259    0.159283    0.751830    0.759117    0.125000    0.147355    0.123221    0.116497    4200        17.255103   0.870037    0.235361    169.419786 
[37m[36mINFO[0m[0m 02/26 13:58:42 | 0.085639    0.090604    0.760817    0.765280    0.691334    0.102821    0.120253    0.760817    0.765280    0.081864    0.084383    0.072233    0.067177    4400        18.076775   0.853875    0.252747    160.089958 
[37m[36mINFO[0m[0m 02/26 14:02:18 | 0.169840    0.178949    0.744126    0.735490    0.699410    0.166359    0.174051    0.744126    0.735490    0.162154    0.190176    0.181007    0.172619    4600        18.898447   0.817062    0.244284    167.534964 
