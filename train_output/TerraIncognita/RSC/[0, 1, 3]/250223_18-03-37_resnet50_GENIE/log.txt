[37m[36mINFO[0m[0m 02/23 18:03:37 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 1 3 --dataset TerraIncognita --trial_seed 1 --hparams_seed 3
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 3
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/RSC/[0, 1, 3]/250223_18-03-37_resnet50_GENIE
	out_root: train_output/TerraIncognita/RSC/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 1
	unique_name: 250223_18-03-37_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.00018664964766736356
	batch_size: 13
	weight_decay: 0.0001969969539389593
	rsc_f_drop_factor: 0.39341808491196506
	rsc_b_drop_factor: 0.11306461290442926
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/23 18:03:37 | n_steps = 5001
[37m[36mINFO[0m[0m 02/23 18:03:37 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/23 18:03:37 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/23 18:03:37 | 
[37m[36mINFO[0m[0m 02/23 18:03:37 | Testenv name escaping te_L100_L38_L46 -> te_L100_L38_L46
[37m[36mINFO[0m[0m 02/23 18:03:37 | Test envs = [0, 1, 3], name = te_L100_L38_L46
[37m[36mINFO[0m[0m 02/23 18:03:37 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/23 18:03:37 | Batch sizes for each domain: [0, 0, 13, 0] (total=13)
[37m[36mINFO[0m[0m 02/23 18:03:37 | steps-per-epoch for each domain: 244.31 -> min = 244.31
[37m[36mINFO[0m[0m 02/23 18:03:38 | # of params = 23528522
[37m[36mINFO[0m[0m 02/23 18:06:25 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/23 18:06:25 | 0.259925    0.260020    0.098237    0.094458    2.289664    0.517005    0.541139    0.019515    0.022085    0.098237    0.094458    0.243255    0.216837    0           0.000000    3.422166    1.182453    165.985915 
[37m[36mINFO[0m[0m 02/23 18:09:38 | 0.325421    0.321241    0.334383    0.367758    1.614525    0.205642    0.178270    0.455771    0.447869    0.334383    0.367758    0.314850    0.337585    200         0.818640    1.890047    0.138839    165.252408 
[37m[36mINFO[0m[0m 02/23 18:12:50 | 0.294934    0.288422    0.344144    0.370277    1.554744    0.206960    0.180380    0.446270    0.439137    0.344144    0.370277    0.231570    0.245748    400         1.637280    1.767762    0.135688    164.771624 
[37m[36mINFO[0m[0m 02/23 18:15:59 | 0.336527    0.333211    0.370907    0.376574    1.509999    0.224888    0.203586    0.456670    0.455059    0.370907    0.376574    0.328022    0.340986    600         2.455919    1.728336    0.134319    162.173141 
[37m[36mINFO[0m[0m 02/23 18:19:04 | 0.286523    0.282628    0.349181    0.371537    1.496771    0.177432    0.152954    0.447169    0.440678    0.349181    0.371537    0.234969    0.254252    800         3.274559    1.730435    0.131272    158.423500 
[37m[36mINFO[0m[0m 02/23 18:22:12 | 0.288549    0.281876    0.351071    0.370277    1.484124    0.204587    0.179325    0.440750    0.432460    0.351071    0.370277    0.220310    0.233844    1000        4.093199    1.693688    0.136173    160.350928 
[37m[36mINFO[0m[0m 02/23 18:25:16 | 0.307067    0.297752    0.389169    0.442065    1.444967    0.208278    0.182489    0.450122    0.443760    0.389169    0.442065    0.262800    0.267007    1200        4.911839    1.656713    0.142587    155.947062 
[37m[36mINFO[0m[0m 02/23 18:28:33 | 0.256813    0.252126    0.425378    0.468514    1.415037    0.149222    0.133966    0.406856    0.404725    0.425378    0.468514    0.214362    0.217687    1400        5.730479    1.637503    0.155535    165.517258 
[37m[36mINFO[0m[0m 02/23 18:32:10 | 0.287147    0.279331    0.481108    0.516373    1.362906    0.206960    0.186709    0.429709    0.424242    0.481108    0.516373    0.224772    0.227041    1600        6.549118    1.596671    0.226005    172.307752 
[37m[36mINFO[0m[0m 02/23 18:35:34 | 0.200973    0.201024    0.449307    0.462217    1.387337    0.042974    0.043249    0.400822    0.404212    0.449307    0.462217    0.159125    0.155612    1800        7.367758    1.612779    0.177870    168.684061 
[37m[36mINFO[0m[0m 02/23 18:38:55 | 0.327280    0.318057    0.490554    0.528967    1.309764    0.219615    0.197257    0.458210    0.452491    0.490554    0.528967    0.304015    0.304422    2000        8.186398    1.522995    0.164179    167.756216 
[37m[36mINFO[0m[0m 02/23 18:42:18 | 0.222654    0.217749    0.535264    0.542821    1.261689    0.062747    0.063291    0.415073    0.408834    0.535264    0.542821    0.190142    0.181122    2200        9.005038    1.486788    0.150222    173.360042 
[37m[36mINFO[0m[0m 02/23 18:45:45 | 0.207484    0.210827    0.427582    0.520151    1.382210    0.010282    0.016878    0.436898    0.429379    0.427582    0.520151    0.175271    0.186224    2400        9.823678    1.449310    0.146783    177.694593 
[37m[36mINFO[0m[0m 02/23 18:49:08 | 0.178026    0.177379    0.536209    0.537783    1.185554    0.039810    0.051688    0.313262    0.308680    0.536209    0.537783    0.181007    0.171769    2600        10.642317   1.456053    0.137036    175.488210 
[37m[36mINFO[0m[0m 02/23 18:52:33 | 0.164304    0.163926    0.601071    0.647355    1.047233    0.064593    0.072785    0.235203    0.248074    0.601071    0.647355    0.193117    0.170918    2800        11.460957   1.343796    0.191796    165.898464 
[37m[36mINFO[0m[0m 02/23 18:55:51 | 0.206152    0.206673    0.526763    0.581864    1.218560    0.048247    0.065401    0.398126    0.388803    0.526763    0.581864    0.172084    0.165816    3000        12.279597   1.295406    0.157168    167.385788 
[37m[36mINFO[0m[0m 02/23 18:59:06 | 0.332619    0.328089    0.540302    0.550378    1.218746    0.224097    0.206751    0.461035    0.456086    0.540302    0.550378    0.312726    0.321429    3200        13.098237   1.357809    0.136171    167.325082 
[37m[36mINFO[0m[0m 02/23 19:02:25 | 0.067912    0.069542    0.429786    0.434509    1.298559    0.015028    0.024262    0.001541    0.001541    0.429786    0.434509    0.187168    0.182823    3400        13.916877   1.397701    0.132120    172.414765 
[37m[36mINFO[0m[0m 02/23 19:05:41 | 0.309710    0.302580    0.619332    0.636020    0.991358    0.181650    0.162447    0.446014    0.444273    0.619332    0.636020    0.301466    0.301020    3600        14.735516   1.303753    0.137612    168.884649 
[37m[36mINFO[0m[0m 02/23 19:08:57 | 0.320939    0.316340    0.652708    0.673804    0.915128    0.224097    0.205696    0.456798    0.455059    0.652708    0.673804    0.281921    0.288265    3800        15.554156   1.195843    0.134603    168.362132 
[37m[36mINFO[0m[0m 02/23 19:12:14 | 0.318935    0.308857    0.652078    0.688917    0.890897    0.204324    0.190928    0.426371    0.425270    0.652078    0.688917    0.326110    0.310374    4000        16.372796   1.201197    0.166087    164.509372 
[37m[36mINFO[0m[0m 02/23 19:15:49 | 0.276540    0.265378    0.637909    0.651134    0.981035    0.176114    0.162447    0.441905    0.438110    0.637909    0.651134    0.211600    0.195578    4200        17.191436   1.158447    0.220801    170.647329 
