[37m[36mINFO[0m[0m 02/23 17:58:42 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 1 3 --dataset TerraIncognita --trial_seed 2 --hparams_seed 6
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 6
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/RSC/[0, 1, 3]/250223_17-58-42_resnet50_GENIE
	out_root: train_output/TerraIncognita/RSC/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 2
	unique_name: 250223_17-58-42_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.00020671063342821416
	batch_size: 9
	weight_decay: 0.00021810605040355685
	rsc_f_drop_factor: 0.46042234920023606
	rsc_b_drop_factor: 0.4947336818366218
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/23 17:58:42 | n_steps = 5001
[37m[36mINFO[0m[0m 02/23 17:58:42 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/23 17:58:42 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/23 17:58:42 | 
[37m[36mINFO[0m[0m 02/23 17:58:42 | Testenv name escaping te_L100_L38_L46 -> te_L100_L38_L46
[37m[36mINFO[0m[0m 02/23 17:58:42 | Test envs = [0, 1, 3], name = te_L100_L38_L46
[37m[36mINFO[0m[0m 02/23 17:58:42 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/23 17:58:42 | Batch sizes for each domain: [0, 0, 9, 0] (total=9)
[37m[36mINFO[0m[0m 02/23 17:58:42 | steps-per-epoch for each domain: 352.89 -> min = 352.89
[37m[36mINFO[0m[0m 02/23 17:58:43 | # of params = 23528522
[37m[36mINFO[0m[0m 02/23 18:01:28 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/23 18:01:28 | 0.297093    0.282833    0.196788    0.180101    2.238501    0.203533    0.197257    0.464886    0.443760    0.196788    0.180101    0.222860    0.207483    0           0.000000    3.828201    1.381275    162.960527 
[37m[36mINFO[0m[0m 02/23 18:04:37 | 0.061546    0.055362    0.204030    0.170025    1.699374    0.035065    0.035865    0.019130    0.017976    0.204030    0.170025    0.130444    0.112245    200         0.566751    2.062712    0.127770    164.138541 
[37m[36mINFO[0m[0m 02/23 18:07:50 | 0.180799    0.178003    0.347292    0.356423    1.660051    0.010546    0.015823    0.369752    0.357473    0.347292    0.356423    0.162099    0.160714    400         1.133501    1.932736    0.129694    166.951917 
[37m[36mINFO[0m[0m 02/23 18:10:51 | 0.317046    0.300806    0.350441    0.335013    1.602209    0.202215    0.198312    0.463603    0.439651    0.350441    0.335013    0.285320    0.264456    600         1.700252    1.881610    0.151658    150.766009 
[37m[36mINFO[0m[0m 02/23 18:13:56 | 0.057976    0.059188    0.273615    0.282116    1.635322    0.010546    0.015823    0.001284    0.001027    0.273615    0.282116    0.162099    0.160714    800         2.267003    1.829177    0.142935    156.160939 
[37m[36mINFO[0m[0m 02/23 18:17:09 | 0.331530    0.319573    0.352960    0.361461    1.605542    0.201424    0.198312    0.465143    0.440678    0.352960    0.361461    0.328022    0.319728    1000        2.833753    1.834173    0.153619    162.348701 
[37m[36mINFO[0m[0m 02/23 18:20:16 | 0.057976    0.059188    0.273615    0.282116    1.646828    0.010546    0.015823    0.001284    0.001027    0.273615    0.282116    0.162099    0.160714    1200        3.400504    1.836439    0.151075    156.176032 
[37m[36mINFO[0m[0m 02/23 18:23:26 | 0.057976    0.059188    0.273615    0.282116    1.626882    0.010546    0.015823    0.001284    0.001027    0.273615    0.282116    0.162099    0.160714    1400        3.967254    1.814121    0.139346    162.612134 
[37m[36mINFO[0m[0m 02/23 18:26:36 | 0.057976    0.059188    0.273615    0.282116    1.614697    0.010546    0.015823    0.001284    0.001027    0.273615    0.282116    0.162099    0.160714    1600        4.534005    1.814179    0.128848    164.574037 
[37m[36mINFO[0m[0m 02/23 18:29:57 | 0.309803    0.294547    0.357683    0.338791    1.604123    0.186396    0.186709    0.457055    0.435028    0.357683    0.338791    0.285957    0.261905    1800        5.100756    1.803398    0.132924    173.754205 
[37m[36mINFO[0m[0m 02/23 18:33:21 | 0.057976    0.059188    0.273615    0.282116    1.594811    0.010546    0.015823    0.001284    0.001027    0.273615    0.282116    0.162099    0.160714    2000        5.667506    1.789502    0.115666    180.593383 
[37m[36mINFO[0m[0m 02/23 18:36:41 | 0.306693    0.290290    0.357683    0.336272    1.560353    0.202215    0.198312    0.446142    0.426810    0.357683    0.336272    0.271723    0.245748    2200        6.234257    1.771805    0.131730    174.128451 
[37m[36mINFO[0m[0m 02/23 18:39:59 | 0.057976    0.059188    0.273615    0.282116    1.573654    0.010546    0.015823    0.001284    0.001027    0.273615    0.282116    0.162099    0.160714    2400        6.801008    1.763449    0.134491    171.482313 
[37m[36mINFO[0m[0m 02/23 18:43:19 | 0.256794    0.243984    0.358942    0.340050    1.556242    0.101766    0.100211    0.445757    0.426810    0.358942    0.340050    0.222860    0.204932    2600        7.367758    1.776277    0.128649    174.383737 
[37m[36mINFO[0m[0m 02/23 18:46:49 | 0.057976    0.059188    0.273615    0.282116    1.565043    0.010546    0.015823    0.001284    0.001027    0.273615    0.282116    0.162099    0.160714    2800        7.934509    1.796681    0.161464    176.837605 
[37m[36mINFO[0m[0m 02/23 18:50:06 | 0.057976    0.059188    0.273615    0.282116    1.571185    0.010546    0.015823    0.001284    0.001027    0.273615    0.282116    0.162099    0.160714    3000        8.501259    1.763984    0.133682    171.054159 
[37m[36mINFO[0m[0m 02/23 18:53:17 | 0.057976    0.059188    0.273615    0.282116    1.574524    0.010546    0.015823    0.001284    0.001027    0.273615    0.282116    0.162099    0.160714    3200        9.068010    1.763787    0.121249    166.677514 
[37m[36mINFO[0m[0m 02/23 18:56:29 | 0.294904    0.279413    0.366184    0.346348    1.519152    0.202215    0.198312    0.449865    0.429892    0.366184    0.346348    0.232632    0.210034    3400        9.634761    1.742034    0.118307    168.328725 
[37m[36mINFO[0m[0m 02/23 18:59:48 | 0.294128    0.278498    0.375000    0.358942    1.507763    0.201687    0.198312    0.447426    0.426297    0.375000    0.358942    0.233270    0.210884    3600        10.201511   1.731878    0.126683    172.834285 
[37m[36mINFO[0m[0m 02/23 19:02:56 | 0.243018    0.237815    0.375315    0.376574    1.498032    0.163195    0.172996    0.401849    0.380586    0.375315    0.376574    0.164011    0.159864    3800        10.768262   1.735417    0.139177    160.390972 
[37m[36mINFO[0m[0m 02/23 19:06:14 | 0.057976    0.059188    0.274874    0.284635    1.561962    0.010546    0.015823    0.001284    0.001027    0.274874    0.284635    0.162099    0.160714    4000        11.335013   1.743323    0.144465    169.228389 
[37m[36mINFO[0m[0m 02/23 19:09:32 | 0.058105    0.059359    0.274559    0.282116    1.494521    0.010546    0.015823    0.001669    0.001541    0.274559    0.282116    0.162099    0.160714    4200        11.901763   1.727344    0.175708    163.334873 
[37m[36mINFO[0m[0m 02/23 19:13:03 | 0.296786    0.286043    0.444584    0.434509    1.526462    0.202215    0.196203    0.461035    0.439137    0.444584    0.434509    0.227109    0.222789    4400        12.468514   1.722078    0.195794    171.787373 
[37m[36mINFO[0m[0m 02/23 19:16:23 | 0.261269    0.253360    0.403023    0.387909    1.448269    0.177168    0.186709    0.441777    0.421161    0.403023    0.387909    0.164861    0.152211    4600        13.035264   1.708889    0.145049    170.579978 
