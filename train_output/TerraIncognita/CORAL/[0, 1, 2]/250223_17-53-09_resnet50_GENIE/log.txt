[37m[36mINFO[0m[0m 02/23 17:53:09 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 2 --dataset TerraIncognita --trial_seed 1 --hparams_seed 15
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 15
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/CORAL/[0, 1, 2]/250223_17-53-09_resnet50_GENIE
	out_root: train_output/TerraIncognita/CORAL/[0, 1, 2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 2]
	trial_seed: 1
	unique_name: 250223_17-53-09_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.7812453400894684e-05
	batch_size: 36
	weight_decay: 3.1651536009272826e-06
	mmd_gamma: 3.440541362473826
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/23 17:53:09 | n_steps = 5001
[37m[36mINFO[0m[0m 02/23 17:53:09 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/23 17:53:09 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/23 17:53:09 | 
[37m[36mINFO[0m[0m 02/23 17:53:09 | Testenv name escaping te_L100_L38_L43 -> te_L100_L38_L43
[37m[36mINFO[0m[0m 02/23 17:53:09 | Test envs = [0, 1, 2], name = te_L100_L38_L43
[37m[36mINFO[0m[0m 02/23 17:53:09 | Train environments: [3], Test environments: [0, 1, 2]
[37m[36mINFO[0m[0m 02/23 17:53:09 | Batch sizes for each domain: [0, 0, 0, 36] (total=36)
[37m[36mINFO[0m[0m 02/23 17:53:09 | steps-per-epoch for each domain: 130.75 -> min = 130.75
[37m[36mINFO[0m[0m 02/23 17:53:10 | # of params = 23528522
[37m[36mINFO[0m[0m 02/23 17:55:57 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/23 17:55:57 | 0.224228    0.236680    0.233694    0.193027    2.093382    0.503296    0.521097    0.019515    0.027735    0.149874    0.161209    0.233694    0.193027    0           0.000000    2.646195    0.000000    1.135454    165.660894 
[37m[36mINFO[0m[0m 02/23 17:59:42 | 0.347798    0.351020    0.468026    0.484694    1.403995    0.219351    0.190928    0.495956    0.495634    0.328086    0.366499    0.468026    0.484694    200         1.529637    1.460264    0.000000    0.301067    165.182789 
[37m[36mINFO[0m[0m 02/23 18:03:26 | 0.228874    0.241401    0.559167    0.552721    1.216780    0.231743    0.262658    0.120811    0.122753    0.334068    0.338791    0.559167    0.552721    400         3.059273    1.254717    0.000000    0.275511    168.937973 
[37m[36mINFO[0m[0m 02/23 18:07:09 | 0.361334    0.356666    0.648821    0.624150    0.986104    0.373847    0.367089    0.292335    0.283513    0.417821    0.419395    0.648821    0.624150    600         4.588910    0.954036    0.000000    0.257816    171.231283 
[37m[36mINFO[0m[0m 02/23 18:10:45 | 0.407926    0.404336    0.711918    0.679422    0.894842    0.373056    0.379747    0.359225    0.355932    0.491499    0.477330    0.711918    0.679422    800         6.118547    0.803119    0.000000    0.262715    163.470262 
[37m[36mINFO[0m[0m 02/23 18:14:30 | 0.385257    0.388750    0.756958    0.738946    0.727241    0.406011    0.426160    0.239055    0.238829    0.510705    0.501259    0.756958    0.738946    1000        7.648184    0.717947    0.000000    0.292191    165.908042 
[37m[36mINFO[0m[0m 02/23 18:18:03 | 0.390385    0.400037    0.756320    0.747449    0.695384    0.391774    0.413502    0.293234    0.300462    0.486146    0.486146    0.756320    0.747449    1200        9.177820    0.671504    0.000000    0.267182    160.310388 
[37m[36mINFO[0m[0m 02/23 18:21:31 | 0.398035    0.404611    0.785851    0.761905    0.677439    0.319536    0.305907    0.384645    0.387776    0.489924    0.520151    0.785851    0.761905    1400        10.707457   0.655942    0.000000    0.250338    157.231229 
[37m[36mINFO[0m[0m 02/23 18:24:58 | 0.420101    0.438860    0.804334    0.780612    0.640389    0.398102    0.408228    0.337014    0.344119    0.525189    0.564232    0.804334    0.780612    1600        12.237094   0.574450    0.000000    0.252354    156.577297 
[37m[36mINFO[0m[0m 02/23 18:28:42 | 0.398233    0.413830    0.791799    0.757653    0.681049    0.315845    0.313291    0.361535    0.367745    0.517317    0.560453    0.791799    0.757653    1800        13.766730   0.554708    0.000000    0.268258    170.467658 
[37m[36mINFO[0m[0m 02/23 18:32:50 | 0.345942    0.355571    0.814532    0.789966    0.605712    0.245979    0.266878    0.279882    0.280945    0.511965    0.518892    0.814532    0.789966    2000        15.296367   0.517457    0.000000    0.340856    180.267676 
[37m[36mINFO[0m[0m 02/23 18:36:44 | 0.404961    0.418657    0.829191    0.789116    0.587024    0.348537    0.366034    0.356272    0.355932    0.510076    0.534005    0.829191    0.789116    2200        16.826004   0.517296    0.000000    0.263023    180.864086 
[37m[36mINFO[0m[0m 02/23 18:40:29 | 0.423227    0.430318    0.836414    0.800170    0.609297    0.369101    0.389241    0.391449    0.379045    0.509131    0.522670    0.836414    0.800170    2400        18.355641   0.449928    0.000000    0.277905    169.973324 
[37m[36mINFO[0m[0m 02/23 18:44:31 | 0.393112    0.403980    0.839388    0.818027    0.556028    0.406538    0.441983    0.218642    0.206985    0.554156    0.562972    0.839388    0.818027    2600        19.885277   0.455542    0.000000    0.308478    179.683916 
[37m[36mINFO[0m[0m 02/23 18:48:24 | 0.410647    0.421198    0.845124    0.809524    0.550307    0.405484    0.430380    0.348183    0.353364    0.478275    0.479849    0.845124    0.809524    2800        21.414914   0.468860    0.000000    0.312601    170.426219 
[37m[36mINFO[0m[0m 02/23 18:52:17 | 0.457237    0.465693    0.853622    0.829082    0.511517    0.502241    0.528481    0.338298    0.333333    0.531171    0.535264    0.853622    0.829082    3000        22.944551   0.466787    0.000000    0.274592    178.269473 
[37m[36mINFO[0m[0m 02/23 18:56:07 | 0.422438    0.428189    0.864457    0.829932    0.499270    0.333509    0.343882    0.406727    0.401644    0.527078    0.539043    0.864457    0.829932    3200        24.474187   0.401439    0.000000    0.294029    170.878718 
[37m[36mINFO[0m[0m 02/23 18:59:47 | 0.377612    0.387160    0.861695    0.815476    0.523857    0.318745    0.342827    0.260881    0.250642    0.553212    0.568010    0.861695    0.815476    3400        26.003824   0.398836    0.000000    0.250957    170.538860 
[37m[36mINFO[0m[0m 02/23 19:03:35 | 0.425406    0.432447    0.878691    0.833333    0.503659    0.352228    0.366034    0.397227    0.398562    0.526763    0.532746    0.878691    0.833333    3600        27.533461   0.387895    0.000000    0.300407    167.465316 
[37m[36mINFO[0m[0m 02/23 19:07:29 | 0.441725    0.450581    0.876567    0.851190    0.481749    0.484577    0.506329    0.326743    0.332820    0.513854    0.512594    0.876567    0.851190    3800        29.063098   0.374461    0.000000    0.308777    171.917060 
[37m[36mINFO[0m[0m 02/23 19:11:14 | 0.328571    0.343738    0.852348    0.807823    0.536436    0.262853    0.291139    0.215304    0.224961    0.507557    0.515113    0.852348    0.807823    4000        30.592734   0.359394    0.000000    0.253308    174.338234 
[37m[36mINFO[0m[0m 02/23 19:15:01 | 0.399927    0.411989    0.895475    0.852041    0.440231    0.335882    0.367089    0.343433    0.334874    0.520466    0.534005    0.895475    0.852041    4200        32.122371   0.356043    0.000000    0.256618    175.829998 
