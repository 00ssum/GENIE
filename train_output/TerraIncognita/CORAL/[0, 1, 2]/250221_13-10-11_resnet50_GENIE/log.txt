[37m[36mINFO[0m[0m 02/21 13:10:11 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 2 --dataset TerraIncognita --trial_seed 0 --hparams_seed 1
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 1
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/CORAL/[0, 1, 2]/250221_13-10-11_resnet50_GENIE
	out_root: train_output/TerraIncognita/CORAL/[0, 1, 2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 2]
	trial_seed: 0
	unique_name: 250221_13-10-11_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5.0781288859686544e-05
	batch_size: 44
	weight_decay: 0.00046410133598234803
	mmd_gamma: 1.1642706271054615
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/21 13:10:11 | n_steps = 5001
[37m[36mINFO[0m[0m 02/21 13:10:11 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/21 13:10:11 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/21 13:10:11 | 
[37m[36mINFO[0m[0m 02/21 13:10:11 | Testenv name escaping te_L100_L38_L43 -> te_L100_L38_L43
[37m[36mINFO[0m[0m 02/21 13:10:11 | Test envs = [0, 1, 2], name = te_L100_L38_L43
[37m[36mINFO[0m[0m 02/21 13:10:11 | Train environments: [3], Test environments: [0, 1, 2]
[37m[36mINFO[0m[0m 02/21 13:10:11 | Batch sizes for each domain: [0, 0, 0, 44] (total=44)
[37m[36mINFO[0m[0m 02/21 13:10:11 | steps-per-epoch for each domain: 106.98 -> min = 106.98
[37m[36mINFO[0m[0m 02/21 13:10:13 | # of params = 23528522
[37m[36mINFO[0m[0m 02/21 13:13:09 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/21 13:13:09 | 0.280848    0.303925    0.222222    0.210034    2.214864    0.200633    0.208861    0.455514    0.481253    0.186398    0.221662    0.222222    0.210034    0           0.000000    2.508072    0.000000    1.436812    174.631033 
[37m[36mINFO[0m[0m 02/21 13:17:30 | 0.323933    0.341374    0.452093    0.471939    1.426509    0.219879    0.233122    0.435486    0.450950    0.316436    0.340050    0.452093    0.471939    200         1.869556    1.490820    0.000000    0.406280    180.539294 
[37m[36mINFO[0m[0m 02/21 13:21:29 | 0.239354    0.252453    0.568515    0.560374    1.098763    0.090166    0.112869    0.208499    0.218798    0.419395    0.425693    0.568515    0.560374    400         3.739112    1.230958    0.000000    0.382212    162.220799 
[37m[36mINFO[0m[0m 02/21 13:25:47 | 0.317564    0.320898    0.614617    0.642007    1.005276    0.529133    0.536920    0.083194    0.083205    0.340365    0.342569    0.614617    0.642007    600         5.608668    0.969201    0.000000    0.381929    181.406998 
[37m[36mINFO[0m[0m 02/21 13:29:54 | 0.353117    0.368545    0.740599    0.759354    0.699978    0.369892    0.380802    0.266915    0.284027    0.422544    0.440806    0.740599    0.759354    800         7.478224    0.805435    0.000000    0.345215    178.015665 
[37m[36mINFO[0m[0m 02/21 13:33:59 | 0.299818    0.312844    0.751434    0.761054    0.699297    0.221197    0.233122    0.204391    0.216744    0.473866    0.488665    0.751434    0.761054    1000        9.347780    0.741455    0.000000    0.328966    179.308671 
[37m[36mINFO[0m[0m 02/21 13:38:07 | 0.311591    0.306022    0.794349    0.803571    0.596882    0.250461    0.255274    0.191552    0.187982    0.492758    0.474811    0.794349    0.803571    1200        11.217336   0.639108    0.000000    0.329144    182.266056 
[37m[36mINFO[0m[0m 02/21 13:42:12 | 0.388546    0.394370    0.810070    0.806973    0.534906    0.329291    0.342827    0.361535    0.380586    0.474811    0.459698    0.810070    0.806973    1400        13.086892   0.585148    0.000000    0.322494    180.016439 
[37m[36mINFO[0m[0m 02/21 13:46:13 | 0.384314    0.385116    0.811982    0.794218    0.605103    0.388874    0.411392    0.279497    0.282999    0.484572    0.460957    0.811982    0.794218    1600        14.956448   0.572351    0.000000    0.336139    173.771656 
[37m[36mINFO[0m[0m 02/21 13:50:21 | 0.322783    0.327024    0.831315    0.792517    0.619830    0.144213    0.143460    0.305559    0.319979    0.518577    0.517632    0.831315    0.792517    1800        16.826004   0.516520    0.000000    0.314889    184.939979 
[37m[36mINFO[0m[0m 02/21 13:54:17 | 0.338982    0.359474    0.804546    0.781463    0.597736    0.171632    0.182489    0.359481    0.379558    0.485831    0.516373    0.804546    0.781463    2000        18.695560   0.480938    0.000000    0.320202    172.464456 
[37m[36mINFO[0m[0m 02/21 13:58:29 | 0.307278    0.314774    0.851710    0.836735    0.499434    0.123912    0.119198    0.264861    0.286081    0.533060    0.539043    0.851710    0.836735    2200        20.565116   0.481689    0.000000    0.348715    181.704315 
[37m[36mINFO[0m[0m 02/21 14:02:34 | 0.356797    0.381035    0.847461    0.813776    0.504432    0.306090    0.315401    0.278470    0.302517    0.485831    0.525189    0.847461    0.813776    2400        22.434672   0.442313    0.000000    0.366684    172.376388 
[37m[36mINFO[0m[0m 02/21 14:06:53 | 0.313776    0.330820    0.852985    0.825680    0.515607    0.150804    0.163502    0.316344    0.341551    0.474181    0.487406    0.852985    0.825680    2600        24.304228   0.417389    0.000000    0.390949    180.906607 
[37m[36mINFO[0m[0m 02/21 14:11:05 | 0.310861    0.318349    0.848736    0.840986    0.506341    0.201424    0.212025    0.224547    0.234206    0.506612    0.508816    0.848736    0.840986    2800        26.173784   0.393372    0.000000    0.340158    183.430053 
[37m[36mINFO[0m[0m 02/21 14:14:55 | 0.280276    0.300400    0.865944    0.863095    0.460219    0.095966    0.101266    0.253049    0.286081    0.491814    0.513854    0.865944    0.863095    3000        28.043340   0.357141    0.000000    0.300866    169.846671 
[37m[36mINFO[0m[0m 02/21 14:18:57 | 0.223195    0.227434    0.855322    0.818878    0.546435    0.088584    0.079114    0.097060    0.100668    0.483942    0.502519    0.855322    0.818878    3200        29.912896   0.369576    0.000000    0.334274    175.349692 
[37m[36mINFO[0m[0m 02/21 14:23:06 | 0.375908    0.394293    0.879754    0.827381    0.531820    0.232270    0.247890    0.370266    0.393426    0.525189    0.541562    0.879754    0.827381    3400        31.782452   0.343928    0.000000    0.373365    174.019413 
[37m[36mINFO[0m[0m 02/21 14:27:09 | 0.336720    0.347485    0.868919    0.834184    0.473674    0.192987    0.183544    0.277186    0.300976    0.539987    0.557935    0.868919    0.834184    3600        33.652008   0.335178    0.000000    0.319138    179.526359 
[37m[36mINFO[0m[0m 02/21 14:31:06 | 0.318176    0.337918    0.901636    0.875850    0.407734    0.194042    0.214135    0.276544    0.295840    0.483942    0.503778    0.901636    0.875850    3800        35.521564   0.315623    0.000000    0.328112    170.666714 
[37m[36mINFO[0m[0m 02/21 14:35:12 | 0.368693    0.384559    0.871680    0.829082    0.541816    0.269180    0.280591    0.324304    0.344119    0.512594    0.528967    0.871680    0.829082    4000        37.391120   0.302203    0.000000    0.354135    175.969031 
