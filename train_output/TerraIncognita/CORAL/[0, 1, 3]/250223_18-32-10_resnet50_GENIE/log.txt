[37m[36mINFO[0m[0m 02/23 18:32:10 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset TerraIncognita --trial_seed 1 --hparams_seed 13
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 13
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/CORAL/[0, 1, 3]/250223_18-32-10_resnet50_GENIE
	out_root: train_output/TerraIncognita/CORAL/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 1
	unique_name: 250223_18-32-10_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 6.984990754803203e-05
	batch_size: 13
	weight_decay: 2.888048423418205e-06
	mmd_gamma: 7.302291555616863
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/23 18:32:10 | n_steps = 5001
[37m[36mINFO[0m[0m 02/23 18:32:10 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/23 18:32:10 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/23 18:32:10 | 
[37m[36mINFO[0m[0m 02/23 18:32:10 | Testenv name escaping te_L100_L38_L46 -> te_L100_L38_L46
[37m[36mINFO[0m[0m 02/23 18:32:10 | Test envs = [0, 1, 3], name = te_L100_L38_L46
[37m[36mINFO[0m[0m 02/23 18:32:10 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/23 18:32:10 | Batch sizes for each domain: [0, 0, 13, 0] (total=13)
[37m[36mINFO[0m[0m 02/23 18:32:10 | steps-per-epoch for each domain: 244.31 -> min = 244.31
[37m[36mINFO[0m[0m 02/23 18:32:13 | # of params = 23528522
[37m[36mINFO[0m[0m 02/23 18:35:04 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/23 18:35:04 | 0.259925    0.260020    0.098237    0.094458    2.340777    0.517005    0.541139    0.019515    0.022085    0.098237    0.094458    0.243255    0.216837    0           0.000000    2.749227    0.000000    2.172222    168.624618 
[37m[36mINFO[0m[0m 02/23 18:38:26 | 0.335017    0.330880    0.386650    0.448363    1.505992    0.221988    0.198312    0.460778    0.452491    0.386650    0.448363    0.322286    0.341837    200         0.818640    1.684899    0.000000    0.162888    169.407043 
[37m[36mINFO[0m[0m 02/23 18:41:53 | 0.336476    0.334182    0.392317    0.443325    1.414928    0.221988    0.204641    0.460906    0.453518    0.392317    0.443325    0.326535    0.344388    400         1.637280    1.530487    0.000000    0.170809    173.281982 
[37m[36mINFO[0m[0m 02/23 18:45:28 | 0.339941    0.342002    0.435768    0.469773    1.382521    0.229634    0.213080    0.465143    0.461736    0.435768    0.469773    0.325048    0.351190    600         2.455919    1.437868    0.000000    0.158429    182.816620 
[37m[36mINFO[0m[0m 02/23 18:48:47 | 0.316740    0.312107    0.386965    0.403023    1.439058    0.211178    0.186709    0.460521    0.454545    0.386965    0.403023    0.278521    0.295068    800         3.274559    1.418628    0.000000    0.125674    174.272098 
[37m[36mINFO[0m[0m 02/23 18:52:05 | 0.313218    0.309551    0.535264    0.571788    1.221165    0.217770    0.194093    0.449737    0.441192    0.535264    0.571788    0.272148    0.293367    1000        4.093199    1.302597    0.000000    0.124838    173.031030 
[37m[36mINFO[0m[0m 02/23 18:55:23 | 0.214250    0.210907    0.548489    0.574307    1.158476    0.103348    0.103376    0.308255    0.296353    0.548489    0.574307    0.231145    0.232993    1200        4.911839    1.193595    0.000000    0.171159    163.974079 
[37m[36mINFO[0m[0m 02/23 18:58:46 | 0.254748    0.258488    0.579345    0.584383    1.095889    0.222515    0.226793    0.324817    0.312275    0.579345    0.584383    0.216911    0.236395    1400        5.730479    1.082037    0.000000    0.154187    171.580306 
[37m[36mINFO[0m[0m 02/23 19:02:00 | 0.223070    0.226498    0.668451    0.664987    0.925627    0.095439    0.093882    0.295673    0.292244    0.668451    0.664987    0.278096    0.293367    1600        6.549118    0.956140    0.000000    0.131383    167.802997 
[37m[36mINFO[0m[0m 02/23 19:05:18 | 0.128925    0.135078    0.653652    0.683879    0.835781    0.039283    0.048523    0.090641    0.088855    0.653652    0.683879    0.256851    0.267857    1800        7.367758    1.004747    0.000000    0.126800    172.996340 
[37m[36mINFO[0m[0m 02/23 19:08:29 | 0.300206    0.311927    0.656171    0.693955    0.861001    0.132085    0.149789    0.425215    0.426297    0.656171    0.693955    0.343318    0.359694    2000        8.186398    0.888412    0.000000    0.128716    165.433367 
[37m[36mINFO[0m[0m 02/23 19:11:45 | 0.274892    0.287538    0.691436    0.724181    0.733518    0.119431    0.143460    0.350879    0.349255    0.691436    0.724181    0.354366    0.369898    2200        9.005038    0.832522    0.000000    0.131472    169.141246 
[37m[36mINFO[0m[0m 02/23 19:14:56 | 0.296578    0.302845    0.730479    0.760705    0.674692    0.214606    0.222574    0.322249    0.319466    0.730479    0.760705    0.352879    0.366497    2400        9.823678    0.793704    0.000000    0.139130    163.783139 
