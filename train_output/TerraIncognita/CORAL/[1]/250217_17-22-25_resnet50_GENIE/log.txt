[37m[36mINFO[0m[0m 02/17 17:22:26 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 1 --dataset TerraIncognita --trial_seed 0 --hparams_seed 19
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: TerraIncognita
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 19
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/TerraIncognita/CORAL/[1]/250217_17-22-25_resnet50_GENIE
	out_root: train_output/TerraIncognita/CORAL/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250217_17-22-25_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5.513854977685438e-05
	batch_size: 27
	weight_decay: 0.0014556716107047517
	mmd_gamma: 6.828665286703169
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[TerraIncognita] #envs=4, #classes=10
	env0: L100 (#4741)
	env1: L38 (#9736)
	env2: L43 (#3970)
	env3: L46 (#5883)

[37m[36mINFO[0m[0m 02/17 17:22:26 | n_steps = 5001
[37m[36mINFO[0m[0m 02/17 17:22:26 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/17 17:22:26 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/17 17:22:26 | 
[37m[36mINFO[0m[0m 02/17 17:22:26 | Testenv name escaping te_L38 -> te_L38
[37m[36mINFO[0m[0m 02/17 17:22:26 | Test envs = [1], name = te_L38
[37m[36mINFO[0m[0m 02/17 17:22:26 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 02/17 17:22:26 | Batch sizes for each domain: [27, 0, 27, 27] (total=81)
[37m[36mINFO[0m[0m 02/17 17:22:26 | steps-per-epoch for each domain: 140.48, 117.63, 174.33 -> min = 117.63
[37m[36mINFO[0m[0m 02/17 17:22:28 | # of params = 23528522
[37m[36mINFO[0m[0m 02/17 17:25:29 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/17 17:25:29 | 0.019258    0.023112    0.284710    0.289972    2.078899    0.522278    0.520042    0.019258    0.023112    0.096033    0.103275    0.235819    0.246599    0           0.000000    2.366545    0.063460    1.195919    179.643191 
[37m[36mINFO[0m[0m 02/17 17:29:05 | 0.026448    0.027221    0.413232    0.418463    1.474298    0.599262    0.594937    0.026448    0.027221    0.289043    0.287154    0.351392    0.373299    200         1.700252    1.641873    0.004430    0.232877    169.735999 
[37m[36mINFO[0m[0m 02/17 17:32:47 | 0.061625    0.067283    0.508176    0.525993    1.343451    0.693646    0.718354    0.061625    0.067283    0.386650    0.405542    0.444232    0.454082    400         3.400504    1.433399    0.006297    0.243280    172.910443 
[37m[36mINFO[0m[0m 02/17 17:36:37 | 0.078444    0.092964    0.542326    0.568605    1.242441    0.716847    0.724684    0.078444    0.092964    0.465050    0.512594    0.445082    0.468537    600         5.100756    1.327323    0.006337    0.242955    182.061638 
[37m[36mINFO[0m[0m 02/17 17:40:21 | 0.085634    0.088855    0.591233    0.610147    1.186777    0.741629    0.734177    0.085634    0.088855    0.544710    0.552897    0.487359    0.543367    800         6.801008    1.237332    0.006008    0.240247    175.489211 
[37m[36mINFO[0m[0m 02/17 17:44:16 | 0.274105    0.298408    0.636921    0.632099    1.032143    0.747429    0.722574    0.274105    0.298408    0.608627    0.613350    0.554706    0.560374    1000        8.501259    1.074416    0.006816    0.219859    190.634650 
[37m[36mINFO[0m[0m 02/17 17:48:06 | 0.260624    0.266564    0.713965    0.729262    0.844761    0.833641    0.827004    0.260624    0.266564    0.667506    0.681360    0.640748    0.679422    1200        10.201511   0.915540    0.007404    0.235496    183.017937 
[37m[36mINFO[0m[0m 02/17 17:52:00 | 0.257029    0.268105    0.745304    0.750658    0.752092    0.820722    0.813291    0.257029    0.268105    0.731738    0.740554    0.683450    0.698129    1400        11.901763   0.771708    0.007593    0.246832    184.859164 
[37m[36mINFO[0m[0m 02/17 17:55:53 | 0.327000    0.333333    0.772240    0.770700    0.656047    0.872924    0.869198    0.327000    0.333333    0.746537    0.743073    0.697259    0.699830    1600        13.602015   0.702645    0.007294    0.286376    175.650422 
[37m[36mINFO[0m[0m 02/17 17:59:44 | 0.392733    0.396507    0.778839    0.773420    0.652573    0.869496    0.850211    0.392733    0.396507    0.758501    0.748111    0.708519    0.721939    1800        15.302267   0.655282    0.006892    0.306428    169.569090 
[37m[36mINFO[0m[0m 02/17 18:03:42 | 0.282193    0.289163    0.812263    0.793448    0.598990    0.894806    0.873418    0.282193    0.289163    0.778652    0.756927    0.763331    0.750000    2000        17.002519   0.581100    0.006486    0.317943    174.972003 
[37m[36mINFO[0m[0m 02/17 18:07:42 | 0.352292    0.361582    0.811705    0.800485    0.573976    0.888742    0.880802    0.352292    0.361582    0.793451    0.780856    0.752921    0.739796    2200        18.702771   0.541409    0.006069    0.309219    177.651262 
