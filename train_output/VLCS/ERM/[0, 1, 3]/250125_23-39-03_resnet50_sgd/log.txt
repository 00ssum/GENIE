[37m[36mINFO[0m[0m 01/25 23:39:03 | Command :: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm ERM --test_envs 0 1 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/VLCS/ERM/[0, 1, 3]/250125_23-39-03_resnet50_sgd
	out_root: train_output/VLCS/ERM/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 0
	unique_name: 250125_23-39-03_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 01/25 23:39:03 | n_steps = 5001
[37m[36mINFO[0m[0m 01/25 23:39:03 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 01/25 23:39:03 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 01/25 23:39:03 | 
[37m[36mINFO[0m[0m 01/25 23:39:03 | Testenv name escaping te_C_L_V -> te_C_L_V
[37m[36mINFO[0m[0m 01/25 23:39:03 | Test envs = [0, 1, 3], name = te_C_L_V
[37m[36mINFO[0m[0m 01/25 23:39:03 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 01/25 23:39:03 | Batch sizes for each domain: [0, 0, 32, 0] (total=32)
[37m[36mINFO[0m[0m 01/25 23:39:03 | steps-per-epoch for each domain: 82.06 -> min = 82.06
[37m[36mINFO[0m[0m 01/25 23:39:04 | # of params = 23518277
[37m[36mINFO[0m[0m 01/25 23:41:18 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 01/25 23:41:18 | 0.143102    0.121782    0.149657    0.167683    1.696149    0.091873    0.077739    0.193412    0.167608    0.149657    0.167683    0.144021    0.120000    0           0.000000    1.675835    0.920163    133.332606 
[37m[36mINFO[0m[0m 01/25 23:44:27 | 0.310145    0.320199    0.553694    0.608232    1.149557    0.147527    0.130742    0.461176    0.480226    0.553694    0.608232    0.321733    0.349630    200         2.437167    1.351906    0.260375    137.003753 
[37m[36mINFO[0m[0m 01/25 23:47:33 | 0.398232    0.422455    0.674791    0.704268    0.944998    0.273852    0.296820    0.507294    0.521657    0.674791    0.704268    0.413551    0.448889    400         4.874334    1.061673    0.248815    135.160249 
[37m[36mINFO[0m[0m 01/25 23:50:39 | 0.466016    0.465687    0.733054    0.724085    0.806819    0.415194    0.385159    0.523765    0.514124    0.733054    0.724085    0.459089    0.497778    600         7.311500    0.890858    0.250862    135.740081 
[37m[36mINFO[0m[0m 01/25 23:53:50 | 0.554435    0.562142    0.755141    0.766768    0.702097    0.611307    0.604240    0.540706    0.531073    0.755141    0.766768    0.511292    0.551111    800         9.748667    0.776724    0.265355    137.743317 
[37m[36mINFO[0m[0m 01/25 23:56:51 | 0.571477    0.574189    0.769231    0.775915    0.635158    0.633392    0.625442    0.544941    0.538606    0.769231    0.775915    0.536098    0.558519    1000        12.185834   0.674802    0.246139    131.609720 
[37m[36mINFO[0m[0m 01/25 23:59:54 | 0.597226    0.603884    0.781036    0.783537    0.596900    0.684629    0.678445    0.550588    0.548023    0.781036    0.783537    0.556461    0.585185    1200        14.623001   0.626394    0.243527    133.719401 
[37m[36mINFO[0m[0m 01/26 00:03:00 | 0.606598    0.614204    0.796268    0.780488    0.570370    0.694346    0.692580    0.560471    0.557439    0.796268    0.780488    0.564976    0.592593    1400        17.060168   0.577525    0.251652    135.974634 
[37m[36mINFO[0m[0m 01/26 00:06:05 | 0.625947    0.628169    0.800838    0.789634    0.552055    0.719965    0.724382    0.573647    0.563089    0.800838    0.789634    0.584228    0.597037    1600        19.497334   0.569183    0.249426    134.114230 
[37m[36mINFO[0m[0m 01/26 00:09:06 | 0.630639    0.629305    0.801599    0.798780    0.546081    0.730565    0.727915    0.565647    0.555556    0.801599    0.798780    0.595705    0.604444    1800        21.934501   0.549152    0.255076    130.086071 
[37m[36mINFO[0m[0m 01/26 00:12:05 | 0.643291    0.642692    0.806169    0.801829    0.532592    0.742933    0.734982    0.584941    0.585687    0.806169    0.801829    0.601999    0.607407    2000        24.371668   0.537286    0.245917    129.778316 
[37m[36mINFO[0m[0m 01/26 00:15:09 | 0.648381    0.647440    0.819117    0.807927    0.528016    0.752650    0.745583    0.584941    0.581921    0.819117    0.807927    0.607553    0.614815    2200        26.808835   0.511661    0.245355    134.482391 
[37m[36mINFO[0m[0m 01/26 00:18:16 | 0.637576    0.636282    0.821021    0.800305    0.528420    0.736749    0.720848    0.576941    0.568738    0.821021    0.800305    0.599037    0.619259    2400        29.246002   0.495209    0.252598    136.575630 
[37m[36mINFO[0m[0m 01/26 00:21:22 | 0.644005    0.643469    0.822163    0.803354    0.522226    0.742049    0.724382    0.588706    0.583804    0.822163    0.803354    0.601259    0.622222    2600        31.683168   0.481070    0.247508    136.112766 
[37m[36mINFO[0m[0m 01/26 00:24:26 | 0.653403    0.653013    0.831683    0.807927    0.517935    0.756184    0.734982    0.593882    0.598870    0.831683    0.807927    0.610144    0.625185    2800        34.120335   0.477612    0.259024    132.037670 
[37m[36mINFO[0m[0m 01/26 00:27:26 | 0.648430    0.651489    0.821401    0.804878    0.519497    0.750883    0.734982    0.588706    0.591337    0.821401    0.804878    0.605702    0.628148    3000        36.557502   0.483534    0.241798    131.922962 
[37m[36mINFO[0m[0m 01/26 00:30:30 | 0.643934    0.646968    0.831683    0.804878    0.519491    0.737633    0.724382    0.591059    0.591337    0.831683    0.804878    0.603110    0.625185    3200        38.994669   0.468180    0.252467    133.630381 
[37m[36mINFO[0m[0m 01/26 00:33:30 | 0.657424    0.659171    0.834730    0.807927    0.515267    0.755300    0.738516    0.604235    0.606403    0.834730    0.807927    0.612736    0.632593    3400        41.431835   0.458138    0.247648    129.935542 
[37m[36mINFO[0m[0m 01/26 00:36:39 | 0.652116    0.652611    0.835491    0.806402    0.516103    0.750000    0.734982    0.594353    0.593220    0.835491    0.806402    0.611996    0.629630    3600        43.869002   0.462689    0.265027    135.971482 
[37m[36mINFO[0m[0m 01/26 00:39:44 | 0.657120    0.656243    0.839299    0.806402    0.512304    0.751767    0.734982    0.606118    0.602637    0.839299    0.806402    0.613476    0.631111    3800        46.306169   0.439788    0.256448    133.995767 
[37m[36mINFO[0m[0m 01/26 00:42:56 | 0.651907    0.655842    0.840061    0.809451    0.516849    0.750883    0.734982    0.592471    0.596987    0.840061    0.809451    0.612366    0.635556    4000        48.743336   0.434450    0.268838    137.446899 
[37m[36mINFO[0m[0m 01/26 00:46:02 | 0.654938    0.654917    0.847296    0.810976    0.512758    0.743816    0.727915    0.612706    0.610169    0.847296    0.810976    0.608293    0.626667    4200        51.180503   0.446085    0.248402    136.349854 
[37m[36mINFO[0m[0m 01/26 00:49:06 | 0.655489    0.656526    0.844250    0.814024    0.517572    0.752650    0.738516    0.598118    0.596987    0.844250    0.814024    0.615698    0.634074    4400        53.617669   0.446831    0.247768    134.354640 
[37m[36mINFO[0m[0m 01/26 00:52:09 | 0.652569    0.655143    0.846915    0.806402    0.513009    0.735866    0.727915    0.613176    0.606403    0.846915    0.806402    0.608663    0.631111    4600        56.054836   0.448269    0.252597    132.095125 
[37m[36mINFO[0m[0m 01/26 00:55:13 | 0.658553    0.659114    0.846915    0.812500    0.511998    0.748233    0.734982    0.615059    0.608286    0.846915    0.812500    0.612366    0.634074    4800        58.492003   0.429248    0.248341    134.840890 
[37m[36mINFO[0m[0m 01/26 00:58:15 | 0.655253    0.657520    0.845773    0.803354    0.511613    0.741166    0.727915    0.617412    0.612053    0.845773    0.803354    0.607183    0.632593    5000        60.929170   0.427747    0.248229    132.146394 
[37m[36mINFO[0m[0m 01/26 00:58:15 | Cumulative gradient change saved at train_output/VLCS/ERM/[0, 1, 3]/250125_23-39-03_resnet50_sgd/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 01/26 00:58:16 | ---
[37m[36mINFO[0m[0m 01/26 00:58:16 | test-domain validation(oracle) = 65.742%
[37m[36mINFO[0m[0m 01/26 00:58:16 | training-domain validation(iid) = 65.549%
[37m[36mINFO[0m[0m 01/26 00:58:16 | last = 65.525%
[37m[36mINFO[0m[0m 01/26 00:58:16 | last (inD) = 80.335%
[37m[36mINFO[0m[0m 01/26 00:58:16 | training-domain validation (iid, inD) = 81.402%
[37m[36mINFO[0m[0m 01/26 00:58:16 | === Summary ===
[37m[36mINFO[0m[0m 01/26 00:58:16 | Command: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm ERM --test_envs 0 1 3 --dataset VLCS
[37m[36mINFO[0m[0m 01/26 00:58:16 | Unique name: 250125_23-39-03_resnet50_sgd
[37m[36mINFO[0m[0m 01/26 00:58:16 | Out path: train_output/VLCS/ERM/[0, 1, 3]/250125_23-39-03_resnet50_sgd
[37m[36mINFO[0m[0m 01/26 00:58:16 | Algorithm: ERM
[37m[36mINFO[0m[0m 01/26 00:58:16 | Dataset: VLCS
