[37m[36mINFO[0m[0m 03/25 09:53:02 | Command :: /jsm0707/GENIE/train_all.py resnet50_Sign_GENIE config/resnet50_Sign_GENIE.yaml --algorithm ERM --test_envs 0 2 3 --dataset VLCS --trial_seed 1 --hparams_seed 9
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_Sign_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 9
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_Sign_GENIE
	out_dir: train_output/VLCS/ERM/[0, 2, 3]/250325_09-53-02_resnet50_Sign_GENIE
	out_root: train_output/VLCS/ERM/[0, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 2, 3]
	trial_seed: 1
	unique_name: 250325_09-53-02_resnet50_Sign_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: sign_genie
	freeze_bn: False
	pretrained: True
	lr: 2.7532223448653515e-05
	batch_size: 36
	weight_decay: 0.007511403320794324
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/25 09:53:02 | n_steps = 5001
[37m[36mINFO[0m[0m 03/25 09:53:02 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/25 09:53:02 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/25 09:53:02 | 
[37m[36mINFO[0m[0m 03/25 09:53:02 | Testenv name escaping te_C_S_V -> te_C_S_V
[37m[36mINFO[0m[0m 03/25 09:53:02 | Test envs = [0, 2, 3], name = te_C_S_V
[37m[36mINFO[0m[0m 03/25 09:53:02 | Train environments: [1], Test environments: [0, 2, 3]
[37m[36mINFO[0m[0m 03/25 09:53:02 | Batch sizes for each domain: [0, 36, 0, 0] (total=36)
[37m[36mINFO[0m[0m 03/25 09:53:02 | steps-per-epoch for each domain: 59.03 -> min = 59.03
[37m[36mINFO[0m[0m 03/25 09:53:03 | # of params = 23518277
[37m[36mINFO[0m[0m 03/25 09:55:01 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/25 09:55:01 | 0.451366    0.454204    0.466824    0.461394    1.125122    0.540636    0.558304    0.466824    0.461394    0.375476    0.416159    0.437986    0.388148    0           0.000000    1.704425    1.754501    116.650062 
[37m[36mINFO[0m[0m 03/25 10:01:14 | 0.653017    0.653773    0.770353    0.749529    0.671952    0.836572    0.833922    0.770353    0.749529    0.490861    0.525915    0.631618    0.601481    200         3.388235    0.736607    1.261559    120.063445 
[37m[36mINFO[0m[0m 03/25 10:07:28 | 0.662219    0.647624    0.774588    0.749529    0.697142    0.853357    0.826855    0.774588    0.749529    0.520564    0.542683    0.612736    0.573333    400         6.776471    0.594674    1.265228    121.693444 
[37m[36mINFO[0m[0m 03/25 10:13:40 | 0.677409    0.666115    0.835294    0.774011    0.627495    0.855124    0.837456    0.835294    0.774011    0.524752    0.557927    0.652351    0.602963    600         10.164706   0.542768    1.267462    118.312631 
[37m[36mINFO[0m[0m 03/25 10:19:49 | 0.631468    0.623116    0.863059    0.764595    0.640882    0.583922    0.561837    0.863059    0.764595    0.621478    0.661585    0.689004    0.645926    800         13.552941   0.441029    1.250317    118.737598 
[37m[36mINFO[0m[0m 03/25 10:26:01 | 0.703142    0.685185    0.881882    0.768362    0.797482    0.856890    0.816254    0.881882    0.768362    0.574638    0.606707    0.677897    0.632593    1000        16.941176   0.380981    1.265907    119.308903 
[37m[36mINFO[0m[0m 03/25 10:32:10 | 0.686702    0.677746    0.886588    0.764595    0.775280    0.818021    0.777385    0.886588    0.764595    0.571592    0.615854    0.670492    0.640000    1200        20.329412   0.341805    1.250290    118.857233 
[37m[36mINFO[0m[0m 03/25 10:38:24 | 0.518444    0.518232    0.827294    0.719397    0.903542    0.350707    0.328622    0.827294    0.719397    0.649276    0.663110    0.555350    0.562963    1400        23.717647   0.295248    1.271771    119.602196 
[37m[36mINFO[0m[0m 03/25 10:44:42 | 0.539427    0.528927    0.926118    0.770245    0.681406    0.363958    0.367491    0.926118    0.770245    0.638995    0.632622    0.615328    0.586667    1600        27.105882   0.295372    1.288985    119.823363 
[37m[36mINFO[0m[0m 03/25 10:50:53 | 0.526844    0.520021    0.949176    0.766478    0.899012    0.357774    0.353357    0.949176    0.766478    0.608911    0.606707    0.613847    0.600000    1800        30.494118   0.245928    1.258032    119.496372 
[37m[36mINFO[0m[0m 03/25 10:57:05 | 0.587623    0.565769    0.910118    0.740113    0.944985    0.525618    0.494700    0.910118    0.740113    0.610815    0.618902    0.626435    0.583704    2000        33.882353   0.235151    1.258367    120.211470 
[37m[36mINFO[0m[0m 03/25 11:03:16 | 0.608427    0.603450    0.950588    0.762712    1.140185    0.624558    0.611307    0.950588    0.762712    0.562072    0.597561    0.638652    0.601481    2200        37.270588   0.170067    1.254697    119.820664 
[37m[36mINFO[0m[0m 03/25 11:09:27 | 0.561470    0.548415    0.956235    0.757062    1.030310    0.525618    0.484099    0.956235    0.757062    0.531988    0.567073    0.626805    0.594074    2400        40.658824   0.188762    1.259266    119.534886 
[37m[36mINFO[0m[0m 03/25 11:15:41 | 0.603541    0.586404    0.925176    0.755179    0.947009    0.707597    0.667845    0.925176    0.755179    0.483625    0.509146    0.619400    0.582222    2600        44.047059   0.195179    1.272295    118.955069 
[37m[36mINFO[0m[0m 03/25 11:21:52 | 0.575362    0.560758    0.942588    0.732580    0.928137    0.547703    0.512367    0.942588    0.732580    0.530845    0.562500    0.647538    0.607407    2800        47.435294   0.155666    1.249240    121.338284 
[37m[36mINFO[0m[0m 03/25 11:28:07 | 0.617169    0.603018    0.927059    0.728814    0.984330    0.749117    0.706714    0.927059    0.728814    0.500762    0.530488    0.601629    0.571852    3000        50.823529   0.199665    1.268538    121.709310 
[37m[36mINFO[0m[0m 03/25 11:34:19 | 0.604094    0.585368    0.973176    0.717514    1.074207    0.698763    0.632509    0.973176    0.717514    0.528180    0.548780    0.585339    0.574815    3200        54.211765   0.152692    1.261271    119.223750 
[37m[36mINFO[0m[0m 03/25 11:40:29 | 0.597855    0.577523    0.960941    0.721281    1.154769    0.556537    0.533569    0.960941    0.721281    0.602818    0.596037    0.634210    0.602963    3400        57.600000   0.164848    1.262026    117.975757 
[37m[36mINFO[0m[0m 03/25 11:46:36 | 0.579824    0.567460    0.935059    0.734463    1.295205    0.651943    0.618375    0.935059    0.734463    0.485529    0.510671    0.601999    0.573333    3600        60.988235   0.159436    1.241672    118.768578 
[37m[36mINFO[0m[0m 03/25 11:52:41 | 0.515252    0.495873    0.970824    0.734463    1.258837    0.537986    0.480565    0.970824    0.734463    0.519802    0.513720    0.487967    0.493333    3800        64.376471   0.121455    1.233953    118.276772 
[37m[36mINFO[0m[0m 03/25 11:58:58 | 0.571740    0.550445    0.952000    0.709981    1.243783    0.559187    0.512367    0.952000    0.709981    0.579208    0.568598    0.576823    0.570370    4000        67.764706   0.143310    1.275004    122.087684 
[37m[36mINFO[0m[0m 03/25 12:05:12 | 0.519040    0.512274    0.912941    0.709981    1.108234    0.443463    0.399293    0.912941    0.709981    0.546458    0.570122    0.567197    0.567407    4200        71.152941   0.158695    1.266700    120.517859 
