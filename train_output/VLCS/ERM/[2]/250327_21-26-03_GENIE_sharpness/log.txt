[37m[36mINFO[0m[0m 03/27 21:26:03 | Command :: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 2 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: GENIE_sharpness
	out_dir: train_output/VLCS/ERM/[2]/250327_21-26-03_GENIE_sharpness
	out_root: train_output/VLCS/ERM/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 0
	unique_name: 250327_21-26-03_GENIE_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 21:26:03 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 21:26:03 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 21:26:03 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 21:26:03 | 
[37m[36mINFO[0m[0m 03/27 21:26:03 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 03/27 21:26:03 | Test envs = [2], name = te_S
[37m[36mINFO[0m[0m 03/27 21:26:03 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 03/27 21:26:03 | Batch sizes for each domain: [32, 32, 0, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 21:26:03 | steps-per-epoch for each domain: 35.38, 66.41, 84.41 -> min = 35.38
[37m[36mINFO[0m[0m 03/27 21:26:05 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 21:28:20 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 21:28:20 | 0.384615    0.387195    0.504871    0.521514    1.311239    0.611307    0.628975    0.459765    0.489642    0.384615    0.387195    0.443539    0.445926    0           0.000000    1.803942    1.871722    133.505288 
[37m[36mINFO[0m[0m 03/27 21:34:43 | 0.736101    0.722561    0.877620    0.865729    0.338140    0.999117    1.000000    0.765176    0.764595    0.736101    0.722561    0.868567    0.832593    200         5.653710    0.440331    1.156830    129.872633 
[37m[36mINFO[0m[0m 03/27 21:41:13 | 0.724676    0.704268    0.901139    0.867874    0.348913    1.000000    0.996466    0.814118    0.762712    0.724676    0.704268    0.889300    0.844444    400         11.307420   0.282620    1.161413    135.400205 
[37m[36mINFO[0m[0m 03/27 21:47:39 | 0.740670    0.728659    0.916296    0.884522    0.332706    0.999117    0.996466    0.835294    0.781544    0.740670    0.728659    0.914476    0.875556    600         16.961131   0.238219    1.142703    135.202189 
[37m[36mINFO[0m[0m 03/27 21:54:05 | 0.746763    0.717988    0.922703    0.873264    0.374861    0.999117    0.996466    0.833412    0.755179    0.746763    0.717988    0.935579    0.868148    800         22.614841   0.211841    1.147326    134.259897 
[37m[36mINFO[0m[0m 03/27 22:00:28 | 0.689261    0.666159    0.937830    0.880896    0.381063    1.000000    1.000000    0.882353    0.783427    0.689261    0.666159    0.931137    0.859259    1000        28.268551   0.184415    1.169021    127.589536 
[37m[36mINFO[0m[0m 03/27 22:06:46 | 0.725438    0.685976    0.944670    0.873363    0.414165    0.999117    1.000000    0.883765    0.760829    0.725438    0.685976    0.951129    0.859259    1200        33.922261   0.150127    1.139357    128.265425 
[37m[36mINFO[0m[0m 03/27 22:13:05 | 0.709825    0.666159    0.964893    0.871697    0.431937    1.000000    1.000000    0.928000    0.772128    0.709825    0.666159    0.966679    0.842963    1400        39.575972   0.120966    1.146808    126.837606 
[37m[36mINFO[0m[0m 03/27 22:19:25 | 0.733054    0.698171    0.970915    0.872417    0.394272    1.000000    1.000000    0.934588    0.768362    0.733054    0.698171    0.978156    0.848889    1600        45.229682   0.113267    1.109519    136.007228 
[37m[36mINFO[0m[0m 03/27 22:25:57 | 0.718964    0.675305    0.981180    0.857753    0.487834    1.000000    1.000000    0.960941    0.728814    0.718964    0.675305    0.982599    0.844444    1800        50.883392   0.072993    1.180505    133.717180 
[37m[36mINFO[0m[0m 03/27 22:32:06 | 0.687738    0.670732    0.981831    0.885635    0.602286    1.000000    0.996466    0.961412    0.804143    0.687738    0.670732    0.984080    0.856296    2000        56.537102   0.060845    1.086595    130.250607 
[37m[36mINFO[0m[0m 03/27 22:38:31 | 0.706017    0.675305    0.979871    0.867135    0.532385    1.000000    1.000000    0.952941    0.728814    0.706017    0.675305    0.986672    0.872593    2200        62.190813   0.065974    1.162744    129.352273 
[37m[36mINFO[0m[0m 03/27 22:44:41 | 0.685072    0.650915    0.980906    0.876085    0.626639    0.999117    0.996466    0.964706    0.774011    0.685072    0.650915    0.978897    0.857778    2400        67.844523   0.054654    1.103113    127.375296 
[37m[36mINFO[0m[0m 03/27 22:50:56 | 0.681645    0.644817    0.984253    0.862281    0.576751    1.000000    1.000000    0.964235    0.743879    0.681645    0.644817    0.988523    0.842963    2600        73.498233   0.045769    1.117424    129.494862 
[37m[36mINFO[0m[0m 03/27 22:57:13 | 0.710967    0.692073    0.987550    0.867169    0.589798    0.998233    1.000000    0.980706    0.757062    0.710967    0.692073    0.983710    0.844444    2800        79.151943   0.041430    1.111692    132.738084 
[37m[36mINFO[0m[0m 03/27 23:03:29 | 0.704494    0.676829    0.993711    0.874124    0.597830    1.000000    1.000000    0.989647    0.764595    0.704494    0.676829    0.991485    0.857778    3000        84.805654   0.039228    1.120656    129.732855 
[37m[36mINFO[0m[0m 03/27 23:10:00 | 0.677456    0.647866    0.993353    0.869638    0.646725    1.000000    1.000000    0.986353    0.757062    0.677456    0.647866    0.993706    0.851852    3200        90.459364   0.027267    1.153081    137.947942 
[37m[36mINFO[0m[0m 03/27 23:16:28 | 0.698781    0.663110    0.984470    0.868142    0.775330    1.000000    0.996466    0.981176    0.766478    0.698781    0.663110    0.972233    0.841481    3400        96.113074   0.027237    1.153158    135.527239 
[37m[36mINFO[0m[0m 03/27 23:22:49 | 0.695735    0.661585    0.993734    0.868876    0.692484    1.000000    1.000000    0.988235    0.753296    0.695735    0.661585    0.992966    0.853333    3600        101.766784  0.019602    1.135606    131.928164 
[37m[36mINFO[0m[0m 03/27 23:29:12 | 0.692688    0.652439    0.996860    0.869222    0.739618    1.000000    0.996466    0.995765    0.760829    0.692688    0.652439    0.994817    0.850370    3800        107.420495  0.033533    1.108486    139.840288 
[37m[36mINFO[0m[0m 03/27 23:35:35 | 0.679360    0.657012    0.996390    0.860080    0.694041    1.000000    1.000000    0.994353    0.747646    0.679360    0.657012    0.994817    0.832593    4000        113.074205  0.018225    1.142987    131.241831 
[37m[36mINFO[0m[0m 03/27 23:41:47 | 0.729627    0.696646    0.994865    0.870343    0.742943    1.000000    0.996466    0.992000    0.762712    0.729627    0.696646    0.992595    0.851852    4200        118.727915  0.017657    1.091650    131.459706 
[37m[36mINFO[0m[0m 03/27 23:48:04 | 0.718583    0.689024    0.996256    0.860073    0.737451    1.000000    0.996466    0.992471    0.736347    0.718583    0.689024    0.996298    0.847407    4400        124.381625  0.016930    1.117335    131.967090 
[37m[36mINFO[0m[0m 03/27 23:54:17 | 0.698020    0.649390    0.992882    0.861448    1.057071    1.000000    0.992933    0.984941    0.751412    0.698020    0.649390    0.993706    0.840000    4600        130.035336  0.009596    1.096206    131.138171 
[37m[36mINFO[0m[0m 03/28 00:00:30 | 0.731912    0.704268    0.988727    0.865830    0.895183    1.000000    1.000000    0.969882    0.738230    0.731912    0.704268    0.996298    0.859259    4800        135.689046  0.013754    1.086074    133.281859 
[37m[36mINFO[0m[0m 03/28 00:06:47 | 0.688500    0.661585    0.995953    0.866541    0.903565    1.000000    1.000000    0.993412    0.755179    0.688500    0.661585    0.994447    0.844444    5000        141.342756  0.015940    1.124844    130.386580 
[37m[36mINFO[0m[0m 03/28 00:07:09 | Cumulative gradient change saved at train_output/VLCS/ERM/[2]/250327_21-26-03_GENIE_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/28 00:07:11 | ---
[37m[36mINFO[0m[0m 03/28 00:07:11 | test-domain validation(oracle) = 74.067%
[37m[36mINFO[0m[0m 03/28 00:07:11 | training-domain validation(iid) = 68.774%
[37m[36mINFO[0m[0m 03/28 00:07:11 | last = 68.850%
[37m[36mINFO[0m[0m 03/28 00:07:11 | last (inD) = 86.654%
[37m[36mINFO[0m[0m 03/28 00:07:11 | training-domain validation (iid, inD) = 88.564%
[37m[36mINFO[0m[0m 03/28 00:07:11 | === Summary ===
[37m[36mINFO[0m[0m 03/28 00:07:11 | Command: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 2 --dataset VLCS
[37m[36mINFO[0m[0m 03/28 00:07:11 | Unique name: 250327_21-26-03_GENIE_sharpness
[37m[36mINFO[0m[0m 03/28 00:07:11 | Out path: train_output/VLCS/ERM/[2]/250327_21-26-03_GENIE_sharpness
[37m[36mINFO[0m[0m 03/28 00:07:11 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/28 00:07:11 | Dataset: VLCS
