[37m[36mINFO[0m[0m 03/18 18:07:26 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 2 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/ERM/[2]/250318_18-07-26_resnet50_GENIE
	out_root: train_output/VLCS/ERM/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 0
	unique_name: 250318_18-07-26_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/18 18:07:26 | n_steps = 5001
[37m[36mINFO[0m[0m 03/18 18:07:26 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/18 18:07:26 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/18 18:07:26 | 
[37m[36mINFO[0m[0m 03/18 18:07:26 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 03/18 18:07:26 | Test envs = [2], name = te_S
[37m[36mINFO[0m[0m 03/18 18:07:26 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 03/18 18:07:26 | Batch sizes for each domain: [32, 32, 0, 32] (total=96)
[37m[36mINFO[0m[0m 03/18 18:07:26 | steps-per-epoch for each domain: 35.38, 66.41, 84.41 -> min = 35.38
[37m[36mINFO[0m[0m 03/18 18:07:27 | # of params = 23518277
[37m[36mINFO[0m[0m 03/18 18:09:42 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/18 18:09:42 | 0.384615    0.387195    0.504871    0.521514    1.311239    0.611307    0.628975    0.459765    0.489642    0.384615    0.387195    0.443539    0.445926    0           0.000000    1.803942    1.302914    133.761827 
[37m[36mINFO[0m[0m 03/18 18:15:46 | 0.774562    0.736280    0.881029    0.869504    0.326870    0.999117    1.000000    0.768000    0.755179    0.774562    0.736280    0.875972    0.853333    200         5.653710    0.438292    1.154941    133.081702 
[37m[36mINFO[0m[0m 03/18 18:21:50 | 0.705255    0.681402    0.897967    0.867169    0.345389    0.999117    1.000000    0.801412    0.757062    0.705255    0.681402    0.893373    0.844444    400         11.307420   0.287117    1.155067    133.030461 
[37m[36mINFO[0m[0m 03/18 18:27:47 | 0.702970    0.676829    0.907271    0.872509    0.367002    0.998233    1.000000    0.813176    0.762712    0.702970    0.676829    0.910404    0.854815    600         16.961131   0.253718    1.133888    130.572401 
[37m[36mINFO[0m[0m 03/18 18:33:45 | 0.715156    0.701220    0.932194    0.869716    0.363989    0.999117    0.996466    0.859294    0.760829    0.715156    0.701220    0.938171    0.851852    800         22.614841   0.204460    1.124119    132.869467 
[37m[36mINFO[0m[0m 03/18 18:39:44 | 0.721249    0.682927    0.945803    0.879718    0.380436    1.000000    0.996466    0.890353    0.783427    0.721249    0.682927    0.947057    0.859259    1000        28.268551   0.173731    1.144653    130.182618 
[37m[36mINFO[0m[0m 03/18 18:45:45 | 0.707921    0.685976    0.954120    0.866767    0.408260    0.999117    1.000000    0.902118    0.751412    0.707921    0.685976    0.961126    0.848889    1200        33.922261   0.153409    1.127996    134.985590 
[37m[36mINFO[0m[0m 03/18 18:51:41 | 0.696497    0.661585    0.967439    0.856349    0.440107    1.000000    0.996466    0.930824    0.732580    0.696497    0.661585    0.971492    0.840000    1400        39.575972   0.129074    1.124081    131.699379 
[37m[36mINFO[0m[0m 03/18 18:57:45 | 0.680503    0.664634    0.975970    0.866809    0.443792    1.000000    1.000000    0.944941    0.758945    0.680503    0.664634    0.982969    0.841481    1600        45.229682   0.091763    1.148915    134.158318 
[37m[36mINFO[0m[0m 03/18 19:03:46 | 0.682788    0.649390    0.970820    0.866315    0.584495    1.000000    1.000000    0.945412    0.758945    0.682788    0.649390    0.967049    0.840000    1800        50.883392   0.077491    1.135167    134.069891 
[37m[36mINFO[0m[0m 03/18 19:09:46 | 0.676314    0.653963    0.979913    0.862825    0.514488    1.000000    1.000000    0.960471    0.730697    0.676314    0.653963    0.979267    0.857778    2000        56.537102   0.071294    1.138189    132.316960 
[37m[36mINFO[0m[0m 03/18 19:15:54 | 0.715918    0.684451    0.984055    0.877305    0.608883    0.998233    1.000000    0.968000    0.781544    0.715918    0.684451    0.985931    0.850370    2200        62.190813   0.054965    1.173307    133.336495 
[37m[36mINFO[0m[0m 03/18 19:22:09 | 0.691927    0.669207    0.984435    0.871683    0.641219    0.998233    0.996466    0.969882    0.781544    0.691927    0.669207    0.985191    0.837037    2400        67.844523   0.054481    1.177154    138.655803 
[37m[36mINFO[0m[0m 03/18 19:28:15 | 0.677456    0.676829    0.980436    0.849343    0.693261    0.999117    0.996466    0.963294    0.738230    0.677456    0.676829    0.978897    0.813333    2600        73.498233   0.039945    1.163028    133.855580 
[37m[36mINFO[0m[0m 03/18 19:34:19 | 0.704113    0.669207    0.992108    0.870224    0.637585    1.000000    1.000000    0.984471    0.751412    0.704113    0.669207    0.991855    0.859259    2800        79.151943   0.050258    1.149195    133.889295 
[37m[36mINFO[0m[0m 03/18 19:40:33 | 0.689642    0.667683    0.993890    0.863670    0.668576    1.000000    1.000000    0.988706    0.749529    0.689642    0.667683    0.992966    0.841481    3000        84.805654   0.032778    1.192174    136.030539 
[37m[36mINFO[0m[0m 03/18 19:46:42 | 0.691927    0.667683    0.992065    0.864637    0.807442    1.000000    0.992933    0.982118    0.747646    0.691927    0.667683    0.994076    0.853333    3200        90.459364   0.024777    1.157591    136.799492 
[37m[36mINFO[0m[0m 03/18 19:52:48 | 0.701066    0.669207    0.994451    0.867127    0.690113    1.000000    1.000000    0.989647    0.749529    0.701066    0.669207    0.993706    0.851852    3400        96.113074   0.024444    1.138021    139.011312 
[37m[36mINFO[0m[0m 03/18 19:58:53 | 0.690404    0.647866    0.995269    0.861689    0.821581    1.000000    0.996466    0.992471    0.738230    0.690404    0.647866    0.993336    0.850370    3600        101.766784  0.019254    1.147334    135.196691 
[37m[36mINFO[0m[0m 03/18 20:05:00 | 0.715537    0.667683    0.996973    0.870400    0.724066    1.000000    1.000000    0.993882    0.760829    0.715537    0.667683    0.997038    0.850370    3800        107.420495  0.019516    1.163663    134.327613 
[37m[36mINFO[0m[0m 03/18 20:11:06 | 0.712110    0.693598    0.995315    0.868834    0.781829    1.000000    1.000000    0.989647    0.745763    0.712110    0.693598    0.996298    0.860741    4000        113.074205  0.016871    1.153110    135.410196 
[37m[36mINFO[0m[0m 03/18 20:17:14 | 0.702209    0.673780    0.995895    0.866675    0.730435    0.999117    1.000000    0.991529    0.757062    0.702209    0.673780    0.997038    0.842963    4200        118.727915  0.022754    1.153312    137.705259 
[37m[36mINFO[0m[0m 03/18 20:23:19 | 0.672506    0.643293    0.994572    0.854338    0.762791    1.000000    1.000000    0.994824    0.736347    0.672506    0.643293    0.988893    0.826667    4400        124.381625  0.018431    1.176796    129.821042 
[37m[36mINFO[0m[0m 03/18 20:29:26 | 0.717060    0.650915    0.995819    0.866711    0.725305    1.000000    0.996466    0.991529    0.753296    0.717060    0.650915    0.995927    0.850370    4600        130.035336  0.017575    1.164037    134.050874 
[37m[36mINFO[0m[0m 03/18 20:35:29 | 0.691165    0.658537    0.994878    0.870076    0.883185    1.000000    0.996466    0.988706    0.758945    0.691165    0.658537    0.995927    0.854815    4800        135.689046  0.012584    1.162049    129.829850 
[37m[36mINFO[0m[0m 03/18 20:41:26 | 0.697639    0.657012    0.990098    0.865355    0.877235    1.000000    0.996466    0.989176    0.775895    0.697639    0.657012    0.981118    0.823704    5000        141.342756  0.023470    1.129397    132.031759 
[37m[36mINFO[0m[0m 03/18 20:41:27 | Cumulative gradient change saved at train_output/VLCS/ERM/[2]/250318_18-07-26_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/18 20:41:28 | ---
[37m[36mINFO[0m[0m 03/18 20:41:28 | test-domain validation(oracle) = 77.456%
[37m[36mINFO[0m[0m 03/18 20:41:28 | training-domain validation(iid) = 72.125%
[37m[36mINFO[0m[0m 03/18 20:41:28 | last = 69.764%
[37m[36mINFO[0m[0m 03/18 20:41:28 | last (inD) = 86.535%
[37m[36mINFO[0m[0m 03/18 20:41:28 | training-domain validation (iid, inD) = 87.972%
[37m[36mINFO[0m[0m 03/18 20:41:28 | === Summary ===
[37m[36mINFO[0m[0m 03/18 20:41:28 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 2 --dataset VLCS
[37m[36mINFO[0m[0m 03/18 20:41:28 | Unique name: 250318_18-07-26_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 20:41:28 | Out path: train_output/VLCS/ERM/[2]/250318_18-07-26_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 20:41:28 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/18 20:41:28 | Dataset: VLCS
