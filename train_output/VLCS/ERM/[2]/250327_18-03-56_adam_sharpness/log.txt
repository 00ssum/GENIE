[37m[36mINFO[0m[0m 03/27 18:03:56 | Command :: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 2 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: adam_sharpness
	out_dir: train_output/VLCS/ERM/[2]/250327_18-03-56_adam_sharpness
	out_root: train_output/VLCS/ERM/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 0
	unique_name: 250327_18-03-56_adam_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 18:03:56 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 18:03:56 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 18:03:56 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 18:03:56 | 
[37m[36mINFO[0m[0m 03/27 18:03:56 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 03/27 18:03:56 | Test envs = [2], name = te_S
[37m[36mINFO[0m[0m 03/27 18:03:56 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 03/27 18:03:56 | Batch sizes for each domain: [32, 32, 0, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 18:03:56 | steps-per-epoch for each domain: 35.38, 66.41, 84.41 -> min = 35.38
[37m[36mINFO[0m[0m 03/27 18:03:57 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 18:06:06 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 18:06:06 | 0.408606    0.416159    0.559529    0.548285    1.404800    0.662544    0.611307    0.521412    0.546139    0.408606    0.416159    0.494632    0.487407    0           0.000000    1.803942    1.309840    127.808271 
[37m[36mINFO[0m[0m 03/27 18:12:21 | 0.672125    0.640244    0.877573    0.859571    0.408744    1.000000    0.996466    0.776000    0.757062    0.672125    0.640244    0.856720    0.825185    200         5.653710    0.401815    1.095190    133.625415 
[37m[36mINFO[0m[0m 03/27 18:18:33 | 0.779893    0.748476    0.914342    0.875874    0.367148    1.000000    1.000000    0.819294    0.768362    0.779893    0.748476    0.923732    0.859259    400         11.307420   0.270848    1.098103    130.157170 
[37m[36mINFO[0m[0m 03/27 18:24:40 | 0.717060    0.693598    0.947549    0.874378    0.368878    1.000000    0.996466    0.899294    0.777778    0.717060    0.693598    0.943354    0.848889    600         16.961131   0.187213    1.086741    127.825301 
[37m[36mINFO[0m[0m 03/27 18:30:46 | 0.674029    0.669207    0.899095    0.813495    0.556428    0.998233    0.996466    0.807529    0.685499    0.674029    0.669207    0.891522    0.758519    800         22.614841   0.147045    1.082869    127.146143 
[37m[36mINFO[0m[0m 03/27 18:36:55 | 0.677075    0.692073    0.943678    0.860607    0.528908    0.999117    1.000000    0.894118    0.775895    0.677075    0.692073    0.937801    0.805926    1000        28.268551   0.113449    1.081705    130.629393 
[37m[36mINFO[0m[0m 03/27 18:43:00 | 0.739147    0.682927    0.974395    0.872980    0.651486    1.000000    0.996466    0.955765    0.792844    0.739147    0.682927    0.967419    0.829630    1200        33.922261   0.090694    1.072484    128.517062 
[37m[36mINFO[0m[0m 03/27 18:49:10 | 0.675171    0.650915    0.968815    0.861722    0.645230    1.000000    0.996466    0.939765    0.766478    0.675171    0.650915    0.966679    0.822222    1400        39.575972   0.080175    1.093817    129.005795 
[37m[36mINFO[0m[0m 03/27 18:55:20 | 0.701828    0.692073    0.987498    0.864248    0.586938    1.000000    1.000000    0.982118    0.764595    0.701828    0.692073    0.980378    0.828148    1600        45.229682   0.064268    1.094081    129.449916 
[37m[36mINFO[0m[0m 03/27 19:01:36 | 0.717441    0.682927    0.977129    0.855698    0.620558    0.999117    0.992933    0.956706    0.760829    0.717441    0.682927    0.975565    0.813333    1800        50.883392   0.056271    1.109653    131.832375 
[37m[36mINFO[0m[0m 03/27 19:07:52 | 0.677837    0.664634    0.984968    0.864933    0.793046    1.000000    0.989399    0.970824    0.768362    0.677837    0.664634    0.984080    0.837037    2000        56.537102   0.051678    1.118095    130.597874 
[37m[36mINFO[0m[0m 03/27 19:13:58 | 0.708302    0.663110    0.977878    0.842965    0.666895    0.998233    0.996466    0.955765    0.745763    0.708302    0.663110    0.979637    0.786667    2200        62.190813   0.045317    1.077730    128.172207 
[37m[36mINFO[0m[0m 03/27 19:20:07 | 0.700305    0.693598    0.975821    0.855325    0.723477    1.000000    1.000000    0.951529    0.736347    0.700305    0.693598    0.975935    0.829630    2400        67.844523   0.037115    1.075484    131.685858 
[37m[36mINFO[0m[0m 03/27 19:26:20 | 0.716299    0.695122    0.990124    0.874786    0.643774    1.000000    1.000000    0.982588    0.794727    0.716299    0.695122    0.987782    0.829630    2600        73.498233   0.033057    1.109779    128.987134 
[37m[36mINFO[0m[0m 03/27 19:32:26 | 0.678218    0.661585    0.985359    0.849935    0.593341    1.000000    1.000000    0.974588    0.743879    0.678218    0.661585    0.981488    0.805926    2800        79.151943   0.027931    1.077672    127.806472 
[37m[36mINFO[0m[0m 03/27 19:38:35 | 0.715537    0.716463    0.991322    0.870335    0.809151    1.000000    0.996466    0.987294    0.783427    0.715537    0.716463    0.986672    0.831111    3000        84.805654   0.029829    1.095981    128.113166 
[37m[36mINFO[0m[0m 03/27 19:44:40 | 0.674791    0.675305    0.994541    0.845231    0.688737    1.000000    1.000000    0.989176    0.719397    0.674791    0.675305    0.994447    0.816296    3200        90.459364   0.031589    1.079238    127.376488 
[37m[36mINFO[0m[0m 03/27 19:50:58 | 0.701828    0.672256    0.997354    0.855995    0.789287    1.000000    1.000000    0.995765    0.745763    0.701828    0.672256    0.996298    0.822222    3400        96.113074   0.029050    1.098289    136.411711 
[37m[36mINFO[0m[0m 03/27 19:57:23 | 0.737624    0.681402    0.993326    0.862746    0.796719    0.999117    0.992933    0.990118    0.762712    0.737624    0.681402    0.990744    0.832593    3600        101.766784  0.022279    1.125224    136.635142 
[37m[36mINFO[0m[0m 03/27 20:03:46 | 0.699543    0.658537    0.997848    0.862816    0.851189    1.000000    1.000000    0.995765    0.751412    0.699543    0.658537    0.997779    0.837037    3800        107.420495  0.019642    1.145504    132.752965 
[37m[36mINFO[0m[0m 03/27 20:10:06 | 0.605103    0.583841    0.981487    0.826424    0.921014    1.000000    1.000000    0.977412    0.711864    0.605103    0.583841    0.967049    0.767407    4000        113.074205  0.022067    1.113370    135.105177 
[37m[36mINFO[0m[0m 03/27 20:16:35 | 0.738385    0.722561    0.996256    0.861865    0.821548    1.000000    0.996466    0.992471    0.747646    0.738385    0.722561    0.996298    0.841481    4200        118.727915  0.025905    1.151316    137.040690 
[37m[36mINFO[0m[0m 03/27 20:22:59 | 0.690023    0.675305    0.995829    0.863415    0.753645    1.000000    0.992933    0.993412    0.772128    0.690023    0.675305    0.994076    0.825185    4400        124.381625  0.024515    1.148396    131.539111 
[37m[36mINFO[0m[0m 03/27 20:29:19 | 0.707159    0.698171    0.992713    0.855945    0.956189    1.000000    1.000000    0.987765    0.758945    0.707159    0.698171    0.990374    0.808889    4600        130.035336  0.020251    1.118014    134.287893 
[37m[36mINFO[0m[0m 03/27 20:35:40 | 0.697258    0.670732    0.994694    0.864255    0.654327    0.999117    0.989399    0.992000    0.779661    0.697258    0.670732    0.992966    0.823704    4800        135.689046  0.022886    1.118638    134.718590 
[37m[36mINFO[0m[0m 03/27 20:42:09 | 0.698781    0.693598    0.996379    0.859761    0.808925    1.000000    1.000000    0.992471    0.757062    0.698781    0.693598    0.996668    0.822222    5000        141.342756  0.017447    1.163774    133.595846 
[37m[36mINFO[0m[0m 03/27 20:42:31 | Cumulative gradient change saved at train_output/VLCS/ERM/[2]/250327_18-03-56_adam_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 20:42:32 | ---
[37m[36mINFO[0m[0m 03/27 20:42:32 | test-domain validation(oracle) = 77.989%
[37m[36mINFO[0m[0m 03/27 20:42:32 | training-domain validation(iid) = 77.989%
[37m[36mINFO[0m[0m 03/27 20:42:32 | last = 69.878%
[37m[36mINFO[0m[0m 03/27 20:42:32 | last (inD) = 85.976%
[37m[36mINFO[0m[0m 03/27 20:42:32 | training-domain validation (iid, inD) = 87.587%
[37m[36mINFO[0m[0m 03/27 20:42:33 | === Summary ===
[37m[36mINFO[0m[0m 03/27 20:42:33 | Command: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 2 --dataset VLCS
[37m[36mINFO[0m[0m 03/27 20:42:33 | Unique name: 250327_18-03-56_adam_sharpness
[37m[36mINFO[0m[0m 03/27 20:42:33 | Out path: train_output/VLCS/ERM/[2]/250327_18-03-56_adam_sharpness
[37m[36mINFO[0m[0m 03/27 20:42:33 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 20:42:33 | Dataset: VLCS
