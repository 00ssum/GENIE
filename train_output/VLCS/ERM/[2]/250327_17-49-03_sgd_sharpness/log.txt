[37m[36mINFO[0m[0m 03/27 17:49:03 | Command :: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 2 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: sgd_sharpness
	out_dir: train_output/VLCS/ERM/[2]/250327_17-49-03_sgd_sharpness
	out_root: train_output/VLCS/ERM/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 0
	unique_name: 250327_17-49-03_sgd_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 17:49:03 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 17:49:03 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 17:49:03 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 17:49:03 | 
[37m[36mINFO[0m[0m 03/27 17:49:03 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 03/27 17:49:03 | Test envs = [2], name = te_S
[37m[36mINFO[0m[0m 03/27 17:49:03 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 03/27 17:49:03 | Batch sizes for each domain: [32, 32, 0, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 17:49:03 | steps-per-epoch for each domain: 35.38, 66.41, 84.41 -> min = 35.38
[37m[36mINFO[0m[0m 03/27 17:49:04 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 17:51:12 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 17:51:12 | 0.269231    0.242378    0.127751    0.122954    1.857654    0.142226    0.141343    0.072941    0.073446    0.269231    0.242378    0.168086    0.154074    0           0.000000    1.803942    1.278312    127.498289 
[37m[36mINFO[0m[0m 03/27 17:57:23 | 0.391089    0.379573    0.530683    0.523236    1.214200    0.631625    0.632509    0.482824    0.476460    0.391089    0.379573    0.477601    0.460741    200         5.653710    1.448177    1.101413    127.805314 
[37m[36mINFO[0m[0m 03/27 18:03:37 | 0.459634    0.455793    0.608000    0.611470    1.025921    0.695230    0.699647    0.593412    0.596987    0.459634    0.455793    0.535357    0.537778    400         11.307420   1.118774    1.120936    127.920797 
[37m[36mINFO[0m[0m 03/27 18:09:49 | 0.532749    0.525915    0.645139    0.659146    0.906127    0.708481    0.724382    0.652706    0.672316    0.532749    0.525915    0.574232    0.580741    600         16.961131   0.964037    1.115494    127.694331 
[37m[36mINFO[0m[0m 03/27 18:16:00 | 0.620335    0.594512    0.682192    0.700248    0.809586    0.783569    0.780919    0.662118    0.700565    0.620335    0.594512    0.600889    0.619259    800         22.614841   0.868601    1.104705    127.523564 
[37m[36mINFO[0m[0m 03/27 18:22:11 | 0.672506    0.638720    0.720829    0.745220    0.727858    0.859541    0.869258    0.670588    0.717514    0.672506    0.638720    0.632358    0.648889    1000        28.268551   0.774668    1.119455    125.570662 
[37m[36mINFO[0m[0m 03/27 18:28:17 | 0.709825    0.669207    0.762921    0.780306    0.659193    0.909011    0.922261    0.697412    0.719397    0.709825    0.669207    0.682340    0.699259    1200        33.922261   0.698949    1.094163    125.080945 
[37m[36mINFO[0m[0m 03/27 18:34:31 | 0.718964    0.676829    0.789275    0.801489    0.598314    0.951413    0.950530    0.692235    0.725047    0.718964    0.676829    0.724176    0.728889    1400        39.575972   0.637724    1.115457    128.165140 
[37m[36mINFO[0m[0m 03/27 18:40:40 | 0.730388    0.689024    0.809650    0.822157    0.550900    0.970848    0.971731    0.700235    0.728814    0.730388    0.689024    0.757867    0.765926    1600        45.229682   0.592018    1.105136    126.071844 
[37m[36mINFO[0m[0m 03/27 18:46:53 | 0.734958    0.692073    0.825117    0.834600    0.511386    0.984982    0.989399    0.709176    0.730697    0.734958    0.692073    0.781192    0.783704    1800        50.883392   0.546958    1.112637    128.480057 
[37m[36mINFO[0m[0m 03/27 18:53:03 | 0.716299    0.692073    0.833021    0.838169    0.483576    0.990283    0.996466    0.708706    0.726930    0.716299    0.692073    0.800074    0.791111    2000        56.537102   0.516566    1.094715    129.209175 
[37m[36mINFO[0m[0m 03/27 18:59:08 | 0.720487    0.701220    0.841514    0.838889    0.461482    0.992049    0.996466    0.720941    0.723164    0.720487    0.701220    0.811551    0.797037    2200        62.190813   0.480493    1.084295    126.407576 
[37m[36mINFO[0m[0m 03/27 19:05:16 | 0.721630    0.701220    0.838200    0.844285    0.445407    0.993816    1.000000    0.718118    0.726930    0.721630    0.701220    0.802666    0.805926    2400        67.844523   0.460895    1.092742    127.348514 
[37m[36mINFO[0m[0m 03/27 19:11:27 | 0.725819    0.705793    0.843736    0.847022    0.432615    0.995583    1.000000    0.720000    0.730697    0.725819    0.705793    0.815624    0.810370    2600        73.498233   0.446373    1.118246    125.677360 
[37m[36mINFO[0m[0m 03/27 19:17:26 | 0.728865    0.705793    0.842122    0.849625    0.422272    0.996466    1.000000    0.720941    0.732580    0.728865    0.705793    0.808960    0.816296    2800        79.151943   0.433968    1.057062    125.518864 
[37m[36mINFO[0m[0m 03/27 19:23:25 | 0.727342    0.707317    0.847094    0.850119    0.413584    0.997350    1.000000    0.724235    0.732580    0.727342    0.707317    0.819696    0.817778    3000        84.805654   0.432957    1.065608    123.783977 
[37m[36mINFO[0m[0m 03/27 19:29:22 | 0.725057    0.701220    0.853540    0.854246    0.406812    0.997350    1.000000    0.735059    0.741996    0.725057    0.701220    0.828212    0.820741    3200        90.459364   0.420704    1.047992    125.151369 
[37m[36mINFO[0m[0m 03/27 19:35:20 | 0.725057    0.698171    0.852562    0.852496    0.400508    0.996466    1.000000    0.734118    0.738230    0.725057    0.698171    0.827101    0.819259    3400        96.113074   0.416521    1.056014    124.750647 
[37m[36mINFO[0m[0m 03/27 19:41:22 | 0.725438    0.702744    0.849135    0.856355    0.395056    0.997350    1.000000    0.725176    0.743879    0.725438    0.702744    0.824880    0.825185    3600        101.766784  0.403996    1.049269    129.951244 
[37m[36mINFO[0m[0m 03/27 19:47:17 | 0.722391    0.696646    0.853143    0.857970    0.390281    0.998233    1.000000    0.727059    0.745763    0.722391    0.696646    0.834136    0.828148    3800        107.420495  0.390193    1.045784    124.244499 
[37m[36mINFO[0m[0m 03/27 19:53:16 | 0.728865    0.705793    0.855886    0.860439    0.385707    0.994700    1.000000    0.738824    0.745763    0.728865    0.705793    0.834136    0.835556    4000        113.074205  0.398085    1.059083    124.804793 
[37m[36mINFO[0m[0m 03/27 19:59:20 | 0.732673    0.704268    0.858859    0.859946    0.381446    0.998233    1.000000    0.739765    0.745763    0.732673    0.704268    0.838578    0.834074    4200        118.727915  0.389908    1.063136    128.544684 
[37m[36mINFO[0m[0m 03/27 20:05:18 | 0.729627    0.701220    0.858337    0.862415    0.379012    0.996466    1.000000    0.740706    0.745763    0.729627    0.701220    0.837838    0.841481    4400        124.381625  0.390534    1.056815    125.306233 
[37m[36mINFO[0m[0m 03/27 20:11:18 | 0.731150    0.707317    0.859437    0.864298    0.375173    0.996466    1.000000    0.738824    0.751412    0.731150    0.707317    0.843021    0.841481    4600        130.035336  0.381824    1.056923    125.706948 
[37m[36mINFO[0m[0m 03/27 20:17:18 | 0.731912    0.704268    0.859575    0.864030    0.371797    0.997350    1.000000    0.738353    0.747646    0.731912    0.704268    0.843021    0.844444    4800        135.689046  0.364301    1.054610    126.501972 
[37m[36mINFO[0m[0m 03/27 20:23:17 | 0.730008    0.708841    0.864275    0.860665    0.369402    0.999117    1.000000    0.751059    0.741996    0.730008    0.708841    0.842651    0.840000    5000        141.342756  0.363961    1.059911    124.658972 
[37m[36mINFO[0m[0m 03/27 20:23:39 | Cumulative gradient change saved at train_output/VLCS/ERM/[2]/250327_17-49-03_sgd_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 20:23:41 | ---
[37m[36mINFO[0m[0m 03/27 20:23:41 | test-domain validation(oracle) = 73.001%
[37m[36mINFO[0m[0m 03/27 20:23:41 | training-domain validation(iid) = 73.115%
[37m[36mINFO[0m[0m 03/27 20:23:41 | last = 73.001%
[37m[36mINFO[0m[0m 03/27 20:23:41 | last (inD) = 86.067%
[37m[36mINFO[0m[0m 03/27 20:23:41 | training-domain validation (iid, inD) = 86.430%
[37m[36mINFO[0m[0m 03/27 20:23:41 | === Summary ===
[37m[36mINFO[0m[0m 03/27 20:23:41 | Command: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 2 --dataset VLCS
[37m[36mINFO[0m[0m 03/27 20:23:41 | Unique name: 250327_17-49-03_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 20:23:41 | Out path: train_output/VLCS/ERM/[2]/250327_17-49-03_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 20:23:41 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 20:23:41 | Dataset: VLCS
