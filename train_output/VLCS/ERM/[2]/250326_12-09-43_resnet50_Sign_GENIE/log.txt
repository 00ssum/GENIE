[37m[36mINFO[0m[0m 03/26 12:09:43 | Command :: /jsm0707/GENIE/train_all.py resnet50_Sign_GENIE config/resnet50_Sign_GENIE.yaml --algorithm ERM --test_envs 2 --dataset VLCS --trial_seed 1 --hparams_seed 14
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_Sign_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 14
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_Sign_GENIE
	out_dir: train_output/VLCS/ERM/[2]/250326_12-09-43_resnet50_Sign_GENIE
	out_root: train_output/VLCS/ERM/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 1
	unique_name: 250326_12-09-43_resnet50_Sign_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: sign_genie
	freeze_bn: False
	pretrained: True
	lr: 0.00011127987942826837
	batch_size: 12
	weight_decay: 3.236767350601091e-05
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/26 12:09:43 | n_steps = 5001
[37m[36mINFO[0m[0m 03/26 12:09:43 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/26 12:09:43 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/26 12:09:43 | 
[37m[36mINFO[0m[0m 03/26 12:09:43 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 03/26 12:09:43 | Test envs = [2], name = te_S
[37m[36mINFO[0m[0m 03/26 12:09:43 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 03/26 12:09:43 | Batch sizes for each domain: [12, 12, 0, 12] (total=36)
[37m[36mINFO[0m[0m 03/26 12:09:43 | steps-per-epoch for each domain: 94.33, 177.08, 225.08 -> min = 94.33
[37m[36mINFO[0m[0m 03/26 12:09:44 | # of params = 23518277
[37m[36mINFO[0m[0m 03/26 12:11:53 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/26 12:11:53 | 0.376618    0.419207    0.511439    0.495231    1.487109    0.613958    0.618375    0.466824    0.461394    0.376618    0.419207    0.453536    0.405926    0           0.000000    1.977285    0.973936    127.706567 
[37m[36mINFO[0m[0m 03/26 12:15:27 | 0.728104    0.737805    0.844134    0.845055    0.423753    1.000000    1.000000    0.687529    0.709981    0.728104    0.737805    0.844872    0.825185    200         2.120141    0.494073    0.424617    129.779739 
[37m[36mINFO[0m[0m 03/26 12:19:03 | 0.744478    0.788110    0.872904    0.860172    0.391382    0.999117    1.000000    0.749176    0.741996    0.744478    0.788110    0.870418    0.838519    400         4.240283    0.360397    0.424353    131.046399 
[37m[36mINFO[0m[0m 03/26 12:22:42 | 0.768850    0.797256    0.874110    0.859473    0.406401    0.999117    0.992933    0.760941    0.751412    0.768850    0.797256    0.862273    0.834074    600         6.360424    0.318931    0.436928    130.954612 
[37m[36mINFO[0m[0m 03/26 12:26:15 | 0.712110    0.717988    0.892205    0.867782    0.400903    0.997350    0.996466    0.803294    0.768362    0.712110    0.717988    0.875972    0.838519    800         8.480565    0.291996    0.431927    127.540176 
[37m[36mINFO[0m[0m 03/26 12:29:54 | 0.761234    0.759146    0.897176    0.863522    0.379011    1.000000    0.996466    0.794824    0.757062    0.761234    0.759146    0.896705    0.837037    1000        10.600707   0.303049    0.432586    132.246770 
[37m[36mINFO[0m[0m 03/26 12:33:30 | 0.717060    0.765244    0.899753    0.869517    0.401710    1.000000    0.992933    0.803294    0.781544    0.717060    0.765244    0.895964    0.834074    1200        12.720848   0.279836    0.433227    129.079813 
[37m[36mINFO[0m[0m 03/26 12:37:09 | 0.720487    0.731707    0.913683    0.869567    0.391532    0.999117    0.992933    0.831529    0.768362    0.720487    0.731707    0.910404    0.847407    1400        14.840989   0.244060    0.438553    131.134006 
[37m[36mINFO[0m[0m 03/26 12:40:49 | 0.703732    0.702744    0.919031    0.860586    0.392351    1.000000    0.992933    0.847059    0.774011    0.703732    0.702744    0.910033    0.814815    1600        16.961131   0.240076    0.440795    132.078566 
[37m[36mINFO[0m[0m 03/26 12:44:26 | 0.706017    0.737805    0.923016    0.861038    0.437061    1.000000    0.992933    0.842353    0.766478    0.706017    0.737805    0.926694    0.823704    1800        19.081272   0.213718    0.429652    130.710621 
[37m[36mINFO[0m[0m 03/26 12:48:05 | 0.719726    0.743902    0.932388    0.856340    0.426657    1.000000    0.996466    0.857882    0.753296    0.719726    0.743902    0.939282    0.819259    2000        21.201413   0.205365    0.450441    129.338443 
[37m[36mINFO[0m[0m 03/26 12:51:45 | 0.742193    0.757622    0.938299    0.867612    0.418253    1.000000    1.000000    0.861176    0.770245    0.742193    0.757622    0.953721    0.832593    2200        23.321555   0.187923    0.439129    132.353644 
[37m[36mINFO[0m[0m 03/26 12:55:21 | 0.742955    0.743902    0.950572    0.865525    0.453304    1.000000    0.992933    0.888000    0.774011    0.742955    0.743902    0.963717    0.829630    2400        25.441696   0.164783    0.429739    129.472990 
[37m[36mINFO[0m[0m 03/26 12:58:58 | 0.691165    0.728659    0.934044    0.854845    0.520042    1.000000    0.992933    0.867294    0.762712    0.691165    0.728659    0.934839    0.808889    2600        27.561837   0.162169    0.428058    132.089139 
[37m[36mINFO[0m[0m 03/26 13:02:38 | 0.681264    0.670732    0.952151    0.849795    0.436006    1.000000    0.996466    0.894588    0.730697    0.681264    0.670732    0.961866    0.822222    2800        29.681979   0.167170    0.430959    133.074529 
[37m[36mINFO[0m[0m 03/26 13:06:15 | 0.675171    0.657012    0.946828    0.862949    0.424072    0.999117    0.989399    0.900235    0.789077    0.675171    0.657012    0.941133    0.810370    3000        31.802120   0.138674    0.442318    129.071105 
[37m[36mINFO[0m[0m 03/26 13:09:54 | 0.704874    0.713415    0.961260    0.868833    0.506529    1.000000    0.989399    0.923765    0.781544    0.704874    0.713415    0.960015    0.835556    3200        33.922261   0.143130    0.435188    132.054753 
[37m[36mINFO[0m[0m 03/26 13:13:34 | 0.657654    0.670732    0.954002    0.858161    0.512610    0.999117    0.989399    0.914353    0.749529    0.657654    0.670732    0.948538    0.835556    3400        36.042403   0.117610    0.440993    131.131547 
[37m[36mINFO[0m[0m 03/26 13:17:09 | 0.710206    0.728659    0.967638    0.861053    0.490699    0.999117    0.996466    0.930824    0.757062    0.710206    0.728659    0.972973    0.829630    3600        38.162544   0.129178    0.423507    130.917504 
[37m[36mINFO[0m[0m 03/26 13:20:48 | 0.706778    0.728659    0.968074    0.858251    0.510265    1.000000    0.992933    0.939765    0.775895    0.706778    0.728659    0.964458    0.805926    3800        40.282686   0.116108    0.436725    131.099239 
[37m[36mINFO[0m[0m 03/26 13:24:25 | 0.705255    0.705793    0.975229    0.848327    0.526748    0.997350    0.989399    0.953882    0.757062    0.705255    0.705793    0.974454    0.798519    4000        42.402827   0.100645    0.442808    129.018101 
[37m[36mINFO[0m[0m 03/26 13:28:07 | 0.728865    0.734756    0.956404    0.862858    0.606592    1.000000    1.000000    0.893647    0.758945    0.728865    0.734756    0.975565    0.829630    4200        44.522968   0.107728    0.447095    132.579476 
