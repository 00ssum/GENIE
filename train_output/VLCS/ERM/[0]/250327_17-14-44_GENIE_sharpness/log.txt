[37m[36mINFO[0m[0m 03/27 17:14:44 | Command :: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 0 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: GENIE_sharpness
	out_dir: train_output/VLCS/ERM/[0]/250327_17-14-44_GENIE_sharpness
	out_root: train_output/VLCS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250327_17-14-44_GENIE_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 17:14:44 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 17:14:44 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 17:14:44 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 17:14:44 | 
[37m[36mINFO[0m[0m 03/27 17:14:44 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/27 17:14:44 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 03/27 17:14:44 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/27 17:14:44 | Batch sizes for each domain: [0, 32, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 17:14:44 | steps-per-epoch for each domain: 66.41, 82.06, 84.41 -> min = 66.41
[37m[36mINFO[0m[0m 03/27 17:14:46 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 17:17:04 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 17:17:04 | 0.611307    0.628975    0.429307    0.440921    1.358290    0.611307    0.628975    0.459765    0.489642    0.384615    0.387195    0.443539    0.445926    0           0.000000    1.786248    1.554731    136.487127 
[37m[36mINFO[0m[0m 03/27 17:23:24 | 0.988516    0.989399    0.806970    0.791603    0.555798    0.988516    0.989399    0.748235    0.781544    0.811881    0.760671    0.860792    0.832593    200         3.011765    0.626843    1.114936    135.898572 
[37m[36mINFO[0m[0m 03/27 17:29:44 | 0.980565    0.971731    0.851262    0.804571    0.527423    0.980565    0.971731    0.801882    0.781544    0.851866    0.774390    0.900037    0.857778    400         6.023529    0.445391    1.124151    134.121595 
[37m[36mINFO[0m[0m 03/27 17:36:01 | 0.956714    0.964664    0.865141    0.808138    0.522786    0.956714    0.964664    0.815529    0.772128    0.873191    0.804878    0.906701    0.847407    600         9.035294    0.385866    1.099194    135.695934 
[37m[36mINFO[0m[0m 03/27 17:42:15 | 0.989399    0.989399    0.887347    0.809655    0.573398    0.989399    0.989399    0.838588    0.758945    0.895278    0.803354    0.928175    0.866667    800         12.047059   0.323129    1.090942    133.938237 
[37m[36mINFO[0m[0m 03/27 17:48:26 | 0.977915    0.968198    0.890695    0.795760    0.579040    0.977915    0.968198    0.838118    0.775895    0.900609    0.772866    0.933358    0.838519    1000        15.058824   0.299518    1.082911    132.820766 
[37m[36mINFO[0m[0m 03/27 17:54:38 | 0.984982    0.985866    0.922262    0.800505    0.656221    0.984982    0.985866    0.886118    0.749529    0.929170    0.794207    0.951499    0.857778    1200        18.070588   0.244068    1.091985    132.223823 
[37m[36mINFO[0m[0m 03/27 18:00:48 | 0.976148    0.978799    0.916719    0.790452    0.792013    0.976148    0.978799    0.891765    0.779661    0.913557    0.757622    0.944835    0.834074    1400        21.082353   0.206429    1.092427    130.729907 
[37m[36mINFO[0m[0m 03/27 18:06:58 | 0.972615    0.978799    0.942214    0.794088    0.764923    0.972615    0.978799    0.925176    0.766478    0.931455    0.771341    0.970011    0.844444    1600        24.094118   0.170830    1.076246    133.200507 
[37m[36mINFO[0m[0m 03/27 18:13:08 | 0.983216    0.978799    0.925715    0.766647    0.914958    0.983216    0.978799    0.877647    0.702448    0.940594    0.753049    0.958904    0.844444    1800        27.105882   0.159296    1.082694    131.794604 
[37m[36mINFO[0m[0m 03/27 18:19:20 | 0.976148    0.968198    0.964961    0.787299    0.827724    0.976148    0.968198    0.936941    0.730697    0.978675    0.792683    0.979267    0.838519    2000        30.117647   0.146134    1.081780    134.679244 
[37m[36mINFO[0m[0m 03/27 18:25:32 | 0.962898    0.961131    0.973776    0.798510    0.928239    0.962898    0.961131    0.960941    0.747646    0.973343    0.806402    0.987042    0.841481    2200        33.129412   0.106618    1.097718    130.888525 
[37m[36mINFO[0m[0m 03/27 18:31:44 | 0.972615    0.957597    0.905035    0.747568    1.308718    0.972615    0.957597    0.807059    0.655367    0.955065    0.760671    0.952980    0.826667    2400        36.141176   0.094538    1.083208    134.233547 
[37m[36mINFO[0m[0m 03/27 18:37:58 | 0.984982    0.985866    0.961208    0.775926    1.081210    0.984982    0.985866    0.950118    0.711864    0.952018    0.775915    0.981488    0.840000    2600        39.152941   0.098616    1.087785    135.014318 
[37m[36mINFO[0m[0m 03/27 18:44:07 | 0.981449    0.971731    0.977401    0.791071    0.988121    0.981449    0.971731    0.975529    0.743879    0.972963    0.778963    0.983710    0.850370    2800        42.164706   0.073699    1.081698    131.694116 
[37m[36mINFO[0m[0m 03/27 18:50:19 | 0.978799    0.971731    0.987033    0.791202    1.197038    0.978799    0.971731    0.984000    0.728814    0.988576    0.801829    0.988523    0.842963    3000        45.176471   0.070143    1.088766    133.198654 
[37m[36mINFO[0m[0m 03/27 18:56:38 | 0.970848    0.964664    0.968632    0.773059    0.987113    0.970848    0.964664    0.936000    0.683616    0.982483    0.789634    0.987412    0.845926    3200        48.188235   0.062802    1.114385    134.099192 
[37m[36mINFO[0m[0m 03/27 19:02:53 | 0.977915    0.978799    0.976172    0.794717    1.229329    0.977915    0.978799    0.973176    0.760829    0.978294    0.775915    0.977046    0.847407    3400        51.200000   0.045954    1.095173    134.942951 
[37m[36mINFO[0m[0m 03/27 19:09:17 | 0.972615    0.985866    0.985159    0.793354    1.060308    0.972615    0.985866    0.978824    0.770245    0.985910    0.769817    0.990744    0.840000    3600        54.211765   0.057425    1.133168    135.656166 
[37m[36mINFO[0m[0m 03/27 19:15:29 | 0.982332    0.975265    0.981469    0.792809    1.164666    0.982332    0.975265    0.959059    0.725047    0.992384    0.791159    0.992966    0.862222    3800        57.223529   0.035960    1.084876    133.840180 
[37m[36mINFO[0m[0m 03/27 19:21:49 | 0.972615    0.975265    0.987760    0.791068    1.299954    0.972615    0.975265    0.992000    0.766478    0.979056    0.765244    0.992225    0.841481    4000        60.235294   0.027899    1.094774    139.213451 
[37m[36mINFO[0m[0m 03/27 19:28:04 | 0.964664    0.975265    0.988040    0.781472    1.226520    0.964664    0.975265    0.980706    0.719397    0.989337    0.783537    0.994076    0.841481    4200        63.247059   0.034842    1.105149    133.521653 
[37m[36mINFO[0m[0m 03/27 19:34:21 | 0.990283    0.982332    0.986075    0.780073    1.298970    0.990283    0.982332    0.973647    0.715631    0.991241    0.768293    0.993336    0.856296    4400        66.258824   0.034569    1.091995    136.318080 
[37m[36mINFO[0m[0m 03/27 19:40:39 | 0.970848    0.964664    0.987460    0.781365    1.451096    0.970848    0.964664    0.975529    0.725047    0.993145    0.782012    0.993706    0.837037    4600        69.270588   0.028525    1.103620    135.776421 
[37m[36mINFO[0m[0m 03/27 19:46:54 | 0.976148    0.968198    0.996762    0.808595    1.256591    0.976148    0.968198    0.996706    0.766478    0.995430    0.791159    0.998149    0.868148    4800        72.282353   0.031515    1.094628    134.998446 
[37m[36mINFO[0m[0m 03/27 19:53:18 | 0.965548    0.964664    0.991074    0.788241    1.280240    0.965548    0.964664    0.993882    0.753296    0.989337    0.774390    0.990004    0.837037    5000        75.294118   0.022057    1.130745    136.688373 
[37m[36mINFO[0m[0m 03/27 19:53:40 | Cumulative gradient change saved at train_output/VLCS/ERM/[0]/250327_17-14-44_GENIE_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 19:53:41 | ---
[37m[36mINFO[0m[0m 03/27 19:53:41 | test-domain validation(oracle) = 98.852%
[37m[36mINFO[0m[0m 03/27 19:53:41 | training-domain validation(iid) = 98.940%
[37m[36mINFO[0m[0m 03/27 19:53:41 | last = 96.555%
[37m[36mINFO[0m[0m 03/27 19:53:41 | last (inD) = 78.824%
[37m[36mINFO[0m[0m 03/27 19:53:41 | training-domain validation (iid, inD) = 80.966%
[37m[36mINFO[0m[0m 03/27 19:53:41 | === Summary ===
[37m[36mINFO[0m[0m 03/27 19:53:41 | Command: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 0 --dataset VLCS
[37m[36mINFO[0m[0m 03/27 19:53:41 | Unique name: 250327_17-14-44_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 19:53:41 | Out path: train_output/VLCS/ERM/[0]/250327_17-14-44_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 19:53:41 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 19:53:41 | Dataset: VLCS
