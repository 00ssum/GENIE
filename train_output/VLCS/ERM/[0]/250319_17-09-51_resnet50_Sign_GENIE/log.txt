[37m[36mINFO[0m[0m 03/19 17:09:51 | Command :: /jsm0707/GENIE/train_all.py resnet50_Sign_GENIE config/resnet50_Sign_GENIE.yaml --algorithm ERM --test_envs 0 --dataset VLCS --trial_seed 0 --hparams_seed 2
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_Sign_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 2
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_Sign_GENIE
	out_dir: train_output/VLCS/ERM/[0]/250319_17-09-51_resnet50_Sign_GENIE
	out_root: train_output/VLCS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250319_17-09-51_resnet50_Sign_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sign_genie
	freeze_bn: False
	pretrained: True
	lr: 1.9041073434446342e-05
	batch_size: 9
	weight_decay: 0.0006566989842279891
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/19 17:09:51 | n_steps = 5001
[37m[36mINFO[0m[0m 03/19 17:09:51 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/19 17:09:51 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/19 17:09:51 | 
[37m[36mINFO[0m[0m 03/19 17:09:51 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/19 17:09:51 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 03/19 17:09:51 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/19 17:09:51 | Batch sizes for each domain: [0, 9, 9, 9] (total=27)
[37m[36mINFO[0m[0m 03/19 17:09:51 | steps-per-epoch for each domain: 236.11, 291.78, 300.11 -> min = 236.11
[37m[36mINFO[0m[0m 03/19 17:09:52 | # of params = 23518277
[37m[36mINFO[0m[0m 03/19 17:12:10 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/19 17:12:10 | 0.502650    0.522968    0.506059    0.500776    1.218229    0.502650    0.522968    0.552941    0.572505    0.454684    0.454268    0.510552    0.475556    0           0.000000    1.724798    1.048642    137.663732 
[37m[36mINFO[0m[0m 03/19 17:15:32 | 0.943463    0.946996    0.745990    0.738557    0.669624    0.943463    0.946996    0.734588    0.747646    0.701828    0.679878    0.801555    0.788148    200         0.847059    0.705722    0.333957    134.443551 
[37m[36mINFO[0m[0m 03/19 17:18:56 | 0.992933    0.975265    0.807323    0.804226    0.551842    0.992933    0.975265    0.759059    0.775895    0.793602    0.780488    0.869308    0.856296    400         1.694118    0.586711    0.328803    139.159220 
[37m[36mINFO[0m[0m 03/19 17:22:17 | 0.943463    0.939929    0.788016    0.762273    0.638878    0.943463    0.939929    0.767059    0.749529    0.752856    0.719512    0.844132    0.817778    600         2.541176    0.530091    0.322323    135.599404 
[37m[36mINFO[0m[0m 03/19 17:25:42 | 0.975265    0.961131    0.825282    0.803645    0.549292    0.975265    0.961131    0.784471    0.770245    0.832064    0.814024    0.859311    0.826667    800         3.388235    0.501202    0.336136    138.298948 
[37m[36mINFO[0m[0m 03/19 17:29:04 | 0.986749    0.996466    0.831956    0.785450    0.552249    0.986749    0.996466    0.774588    0.753296    0.842346    0.792683    0.878934    0.810370    1000        4.235294    0.448163    0.331636    135.602370 
[37m[36mINFO[0m[0m 03/19 17:32:27 | 0.992933    0.996466    0.836037    0.802414    0.552883    0.992933    0.996466    0.777882    0.770245    0.844631    0.788110    0.885598    0.848889    1200        5.082353    0.436921    0.338431    135.377217 
[37m[36mINFO[0m[0m 03/19 17:35:53 | 0.988516    0.985866    0.846331    0.792141    0.579078    0.988516    0.985866    0.802353    0.766478    0.848819    0.774390    0.887819    0.835556    1400        5.929412    0.404517    0.328357    140.042478 
[37m[36mINFO[0m[0m 03/19 17:39:16 | 0.977032    0.971731    0.829965    0.764098    0.619855    0.977032    0.971731    0.723765    0.683616    0.857578    0.782012    0.908552    0.826667    1600        6.776471    0.392807    0.329355    137.426013 
[37m[36mINFO[0m[0m 03/19 17:42:37 | 0.973498    0.975265    0.846535    0.784354    0.651756    0.973498    0.975265    0.807059    0.774011    0.848058    0.782012    0.884487    0.797037    1800        7.623529    0.385655    0.326707    135.707250 
[37m[36mINFO[0m[0m 03/19 17:46:04 | 0.984982    0.968198    0.860401    0.798445    0.603842    0.984982    0.968198    0.790118    0.760829    0.876238    0.804878    0.914846    0.829630    2000        8.470588    0.407547    0.332531    140.026635 
[37m[36mINFO[0m[0m 03/19 17:49:29 | 0.973498    0.964664    0.846877    0.755942    0.682083    0.973498    0.964664    0.780235    0.726930    0.851104    0.742378    0.909293    0.798519    2200        9.317647    0.378484    0.328072    139.531377 
[37m[36mINFO[0m[0m 03/19 17:52:51 | 0.961131    0.957597    0.849838    0.768464    0.643330    0.961131    0.957597    0.830118    0.755179    0.841203    0.757622    0.878193    0.792593    2400        10.164706   0.362289    0.324149    137.047160 
[37m[36mINFO[0m[0m 03/19 17:56:18 | 0.985866    0.975265    0.891980    0.786938    0.638353    0.985866    0.975265    0.840471    0.740113    0.901371    0.788110    0.934098    0.832593    2600        11.011765   0.343375    0.330235    141.095623 
[37m[36mINFO[0m[0m 03/19 17:59:41 | 0.957597    0.964664    0.861492    0.770145    0.693651    0.957597    0.964664    0.825412    0.736347    0.869764    0.763720    0.889300    0.810370    2800        11.858824   0.303230    0.332332    136.770548 
[37m[36mINFO[0m[0m 03/19 18:03:04 | 0.971731    0.964664    0.872906    0.767709    0.698561    0.971731    0.964664    0.836235    0.745763    0.873191    0.748476    0.909293    0.808889    3000        12.705882   0.320120    0.331190    136.245973 
[37m[36mINFO[0m[0m 03/19 18:06:32 | 0.958481    0.964664    0.886971    0.781712    0.646159    0.958481    0.964664    0.844706    0.741996    0.900990    0.795732    0.915217    0.807407    3200        13.552941   0.308989    0.333352    142.058595 
[37m[36mINFO[0m[0m 03/19 18:09:57 | 0.972615    0.961131    0.888764    0.778939    0.670102    0.972615    0.961131    0.859765    0.764595    0.885758    0.750000    0.920770    0.822222    3400        14.400000   0.285775    0.325757    139.536252 
[37m[36mINFO[0m[0m 03/19 18:13:20 | 0.990283    0.992933    0.919221    0.790029    0.641012    0.990283    0.992933    0.865412    0.747646    0.932978    0.797256    0.959274    0.825185    3600        15.247059   0.256112    0.338201    135.223642 
[37m[36mINFO[0m[0m 03/19 18:16:47 | 0.974382    0.957597    0.915141    0.792815    0.707788    0.974382    0.957597    0.869176    0.745763    0.929931    0.792683    0.946316    0.840000    3800        16.094118   0.243417    0.333020    140.783090 
[37m[36mINFO[0m[0m 03/19 18:20:08 | 0.963781    0.968198    0.909318    0.788056    0.722606    0.963781    0.968198    0.870588    0.772128    0.916603    0.769817    0.940763    0.822222    4000        16.941176   0.243511    0.316986    136.828930 
[37m[36mINFO[0m[0m 03/19 18:23:33 | 0.973498    0.957597    0.930842    0.796968    0.654471    0.973498    0.957597    0.899765    0.770245    0.937928    0.786585    0.954832    0.834074    4200        17.788235   0.235138    0.330665    138.877910 
