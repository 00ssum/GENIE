[37m[36mINFO[0m[0m 03/27 13:44:02 | Command :: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 0 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: sgd_sharpness
	out_dir: train_output/VLCS/ERM/[0]/250327_13-44-02_sgd_sharpness
	out_root: train_output/VLCS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250327_13-44-02_sgd_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 13:44:02 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 13:44:02 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 13:44:02 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 13:44:02 | 
[37m[36mINFO[0m[0m 03/27 13:44:03 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/27 13:44:03 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 03/27 13:44:03 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/27 13:44:03 | Batch sizes for each domain: [0, 32, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 13:44:03 | steps-per-epoch for each domain: 66.41, 82.06, 84.41 -> min = 66.41
[37m[36mINFO[0m[0m 03/27 13:44:03 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 13:46:18 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 13:46:18 | 0.152827    0.141343    0.155257    0.157141    1.767361    0.152827    0.141343    0.073412    0.073446    0.223534    0.243902    0.168826    0.154074    0           0.000000    1.786248    1.605934    133.229490 
[37m[36mINFO[0m[0m 03/27 13:52:33 | 0.533569    0.526502    0.484539    0.496656    1.243379    0.533569    0.526502    0.506353    0.517891    0.460777    0.481707    0.486486    0.490370    200         3.011765    1.432178    1.111589    131.286134 
[37m[36mINFO[0m[0m 03/27 13:58:51 | 0.688163    0.653710    0.581042    0.606992    1.107644    0.688163    0.653710    0.596235    0.636535    0.583397    0.605183    0.563495    0.579259    400         6.023529    1.177040    1.117926    132.478293 
[37m[36mINFO[0m[0m 03/27 14:05:06 | 0.715548    0.674912    0.645507    0.657944    1.009185    0.715548    0.674912    0.647059    0.661017    0.677837    0.692073    0.611625    0.620741    600         9.035294    1.075387    1.112463    131.342268 
[37m[36mINFO[0m[0m 03/27 14:11:18 | 0.727032    0.692580    0.680684    0.682598    0.928455    0.727032    0.692580    0.664941    0.668550    0.741051    0.736280    0.636061    0.642963    800         12.047059   0.984242    1.104279    129.843461 
[37m[36mINFO[0m[0m 03/27 14:17:30 | 0.744700    0.717314    0.692182    0.699135    0.861572    0.744700    0.717314    0.671529    0.683616    0.759330    0.753049    0.645687    0.660741    1000        15.058824   0.908812    1.097746    131.100635 
[37m[36mINFO[0m[0m 03/27 14:23:42 | 0.757951    0.727915    0.700264    0.705011    0.809895    0.757951    0.727915    0.681412    0.696798    0.769992    0.753049    0.649389    0.665185    1200        18.070588   0.843574    1.101587    130.701278 
[37m[36mINFO[0m[0m 03/27 14:29:55 | 0.765901    0.752650    0.704820    0.713682    0.769505    0.765901    0.752650    0.681412    0.706215    0.782178    0.763720    0.650870    0.671111    1400        21.082353   0.800768    1.107128    130.686076 
[37m[36mINFO[0m[0m 03/27 14:36:07 | 0.787986    0.780919    0.714716    0.720186    0.739112    0.787986    0.780919    0.680941    0.713748    0.788271    0.768293    0.674935    0.678519    1600        24.094118   0.751775    1.097244    131.278365 
[37m[36mINFO[0m[0m 03/27 14:42:20 | 0.825972    0.809187    0.726904    0.732856    0.714327    0.825972    0.809187    0.694118    0.725047    0.790556    0.769817    0.696039    0.703704    1800        27.105882   0.727992    1.097858    131.475967 
[37m[36mINFO[0m[0m 03/27 14:48:36 | 0.870141    0.844523    0.738069    0.738782    0.690337    0.870141    0.844523    0.700235    0.725047    0.789794    0.769817    0.724176    0.721481    2000        30.117647   0.705405    1.110540    132.549160 
[37m[36mINFO[0m[0m 03/27 14:54:51 | 0.903710    0.883392    0.750936    0.749477    0.673372    0.903710    0.883392    0.711059    0.728814    0.790175    0.775915    0.751573    0.743704    2200        33.129412   0.676730    1.118819    130.215537 
[37m[36mINFO[0m[0m 03/27 15:01:04 | 0.916078    0.897527    0.760017    0.756017    0.658697    0.916078    0.897527    0.707765    0.730697    0.796649    0.774390    0.775639    0.762963    2400        36.141176   0.667744    1.102766    131.249273 
[37m[36mINFO[0m[0m 03/27 15:07:20 | 0.936396    0.915194    0.770818    0.762661    0.644955    0.936396    0.915194    0.718588    0.734463    0.800457    0.769817    0.793410    0.783704    2600        39.152941   0.652669    1.105432    133.781826 
[37m[36mINFO[0m[0m 03/27 15:13:36 | 0.937279    0.918728    0.772231    0.769434    0.632697    0.937279    0.918728    0.727059    0.745763    0.794745    0.774390    0.794891    0.788148    2800        42.164706   0.634160    1.113641    131.949214 
[37m[36mINFO[0m[0m 03/27 15:19:50 | 0.944346    0.929329    0.775477    0.772354    0.621221    0.944346    0.929329    0.731294    0.745763    0.792841    0.769817    0.802295    0.801481    3000        45.176471   0.626669    1.094629    132.929389 
[37m[36mINFO[0m[0m 03/27 15:26:08 | 0.948763    0.932862    0.780697    0.775211    0.611945    0.948763    0.932862    0.727059    0.743879    0.802742    0.772866    0.812292    0.808889    3200        48.188235   0.607675    1.109984    135.207292 
[37m[36mINFO[0m[0m 03/27 15:32:20 | 0.953180    0.939929    0.785549    0.778979    0.603241    0.953180    0.939929    0.732706    0.747646    0.803503    0.777439    0.820437    0.811852    3400        51.200000   0.598325    1.093277    132.374598 
[37m[36mINFO[0m[0m 03/27 15:38:34 | 0.963781    0.950530    0.781907    0.782479    0.596763    0.963781    0.950530    0.728941    0.747646    0.798934    0.782012    0.817845    0.817778    3600        54.211765   0.600705    1.116358    128.820683 
[37m[36mINFO[0m[0m 03/27 15:44:41 | 0.964664    0.950530    0.786939    0.782211    0.589496    0.964664    0.950530    0.726118    0.743879    0.804265    0.782012    0.830433    0.820741    3800        57.223529   0.585995    1.077425    130.767898 
[37m[36mINFO[0m[0m 03/27 15:50:47 | 0.967314    0.957597    0.789461    0.782946    0.583358    0.967314    0.957597    0.731294    0.732580    0.810358    0.788110    0.826731    0.828148    4000        60.235294   0.585219    1.071424    129.731420 
[37m[36mINFO[0m[0m 03/27 15:56:53 | 0.969081    0.961131    0.787044    0.787297    0.577614    0.969081    0.961131    0.729882    0.745763    0.800076    0.783537    0.831174    0.832593    4200        63.247059   0.570755    1.083879    128.749727 
[37m[36mINFO[0m[0m 03/27 16:03:00 | 0.969965    0.961131    0.795955    0.788074    0.570452    0.969965    0.961131    0.742588    0.741996    0.811881    0.789634    0.833395    0.832593    4400        66.258824   0.558760    1.081402    129.268737 
[37m[36mINFO[0m[0m 03/27 16:09:05 | 0.971731    0.961131    0.792119    0.790466    0.567182    0.971731    0.961131    0.734118    0.747646    0.809216    0.791159    0.833025    0.832593    4600        69.270588   0.556825    1.075930    128.704457 
[37m[36mINFO[0m[0m 03/27 16:15:14 | 0.969965    0.957597    0.792089    0.792455    0.561985    0.969965    0.957597    0.737412    0.747646    0.807312    0.792683    0.831544    0.837037    4800        72.282353   0.552867    1.081185    130.799063 
[37m[36mINFO[0m[0m 03/27 16:21:21 | 0.973498    0.961131    0.798947    0.789167    0.558356    0.973498    0.961131    0.743059    0.743879    0.811500    0.786585    0.842281    0.837037    5000        75.294118   0.539017    1.082789    129.862502 
[37m[36mINFO[0m[0m 03/27 16:21:43 | Cumulative gradient change saved at train_output/VLCS/ERM/[0]/250327_13-44-02_sgd_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 16:21:44 | ---
[37m[36mINFO[0m[0m 03/27 16:21:44 | test-domain validation(oracle) = 96.908%
[37m[36mINFO[0m[0m 03/27 16:21:44 | training-domain validation(iid) = 96.996%
[37m[36mINFO[0m[0m 03/27 16:21:44 | last = 97.350%
[37m[36mINFO[0m[0m 03/27 16:21:44 | last (inD) = 78.917%
[37m[36mINFO[0m[0m 03/27 16:21:44 | training-domain validation (iid, inD) = 79.246%
[37m[36mINFO[0m[0m 03/27 16:21:44 | === Summary ===
[37m[36mINFO[0m[0m 03/27 16:21:44 | Command: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 0 --dataset VLCS
[37m[36mINFO[0m[0m 03/27 16:21:44 | Unique name: 250327_13-44-02_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 16:21:44 | Out path: train_output/VLCS/ERM/[0]/250327_13-44-02_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 16:21:44 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 16:21:44 | Dataset: VLCS
