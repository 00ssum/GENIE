[37m[36mINFO[0m[0m 03/18 14:14:53 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 0 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/ERM/[0]/250318_14-14-53_resnet50_GENIE
	out_root: train_output/VLCS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250318_14-14-53_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/18 14:14:53 | n_steps = 5001
[37m[36mINFO[0m[0m 03/18 14:14:53 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/18 14:14:53 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/18 14:14:53 | 
[37m[36mINFO[0m[0m 03/18 14:14:53 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/18 14:14:53 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 03/18 14:14:53 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/18 14:14:53 | Batch sizes for each domain: [0, 32, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/18 14:14:53 | steps-per-epoch for each domain: 66.41, 82.06, 84.41 -> min = 66.41
[37m[36mINFO[0m[0m 03/18 14:14:54 | # of params = 23518277
[37m[36mINFO[0m[0m 03/18 14:17:10 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/18 14:17:10 | 0.611307    0.628975    0.429307    0.440921    1.358290    0.611307    0.628975    0.459765    0.489642    0.384615    0.387195    0.443539    0.445926    0           0.000000    1.786248    1.344415    134.613237 
[37m[36mINFO[0m[0m 03/18 14:23:19 | 0.983216    0.975265    0.819157    0.810909    0.517640    0.983216    0.975265    0.753412    0.777778    0.835491    0.794207    0.868567    0.860741    200         3.011765    0.617012    1.131619    142.935198 
[37m[36mINFO[0m[0m 03/18 14:29:15 | 0.986749    0.996466    0.852603    0.811682    0.507815    0.986749    0.996466    0.787765    0.764595    0.865194    0.818598    0.904850    0.851852    400         6.023529    0.435939    1.109112    133.288436 
[37m[36mINFO[0m[0m 03/18 14:35:11 | 0.986749    0.989399    0.872735    0.813211    0.525843    0.986749    0.989399    0.818353    0.766478    0.885377    0.809451    0.914476    0.863704    600         9.035294    0.379659    1.108818    135.068645 
[37m[36mINFO[0m[0m 03/18 14:41:08 | 0.984099    0.985866    0.885020    0.808002    0.603345    0.984099    0.985866    0.840471    0.785311    0.896040    0.795732    0.918549    0.842963    800         12.047059   0.332505    1.097236    136.635480 
[37m[36mINFO[0m[0m 03/18 14:47:05 | 0.972615    0.961131    0.893270    0.799512    0.602745    0.972615    0.961131    0.850824    0.760829    0.881188    0.760671    0.947797    0.877037    1000        15.058824   0.295330    1.103364    137.180243 
[37m[36mINFO[0m[0m 03/18 14:52:59 | 0.971731    0.971731    0.920649    0.804899    0.624086    0.971731    0.971731    0.887529    0.762712    0.930693    0.794207    0.943725    0.857778    1200        18.070588   0.249440    1.097107    134.418576 
[37m[36mINFO[0m[0m 03/18 14:58:55 | 0.975265    0.978799    0.901287    0.769640    0.749596    0.975265    0.978799    0.824471    0.694915    0.923077    0.760671    0.956312    0.853333    1400        21.082353   0.216723    1.111943    133.817602 
[37m[36mINFO[0m[0m 03/18 15:04:54 | 0.983216    0.985866    0.950095    0.794259    0.773620    0.983216    0.985866    0.925176    0.747646    0.956207    0.774390    0.968900    0.860741    1600        24.094118   0.171265    1.115962    135.388253 
[37m[36mINFO[0m[0m 03/18 15:10:54 | 0.979682    0.964664    0.941619    0.786495    0.749776    0.979682    0.964664    0.892706    0.726930    0.956588    0.788110    0.975565    0.844444    1800        27.105882   0.168055    1.132450    133.630559 
[37m[36mINFO[0m[0m 03/18 15:16:49 | 0.946996    0.964664    0.965591    0.795811    0.844670    0.946996    0.964664    0.952000    0.755179    0.971059    0.777439    0.973713    0.854815    2000        30.117647   0.118944    1.111800    132.453696 
[37m[36mINFO[0m[0m 03/18 15:22:46 | 0.982332    0.985866    0.946610    0.795219    0.866526    0.982332    0.985866    0.913412    0.740113    0.950114    0.775915    0.976305    0.869630    2200        33.129412   0.136021    1.106093    135.754980 
[37m[36mINFO[0m[0m 03/18 15:28:44 | 0.973498    0.964664    0.963199    0.786724    0.970038    0.973498    0.964664    0.944941    0.740113    0.966870    0.765244    0.977786    0.854815    2400        36.141176   0.090496    1.112622    135.832094 
[37m[36mINFO[0m[0m 03/18 15:34:46 | 0.971731    0.975265    0.954841    0.766500    0.998179    0.971731    0.975265    0.941647    0.719397    0.955826    0.766768    0.967049    0.813333    2600        39.152941   0.081124    1.107388    139.720647 
[37m[36mINFO[0m[0m 03/18 15:40:39 | 0.984982    0.982332    0.977590    0.806334    0.975236    0.984982    0.982332    0.952000    0.745763    0.987433    0.812500    0.993336    0.860741    2800        42.164706   0.085475    1.096050    133.994717 
[37m[36mINFO[0m[0m 03/18 15:46:33 | 0.970848    0.968198    0.981445    0.791324    1.034981    0.970848    0.968198    0.975529    0.755179    0.983244    0.772866    0.985561    0.845926    3000        45.176471   0.060773    1.095549    135.206557 
[37m[36mINFO[0m[0m 03/18 15:52:18 | 0.983216    0.989399    0.975927    0.767848    1.289989    0.983216    0.989399    0.959059    0.709981    0.980198    0.771341    0.988523    0.822222    3200        48.188235   0.067856    1.062724    132.323845 
[37m[36mINFO[0m[0m 03/18 15:58:12 | 0.968198    0.964664    0.978164    0.804757    1.142576    0.968198    0.964664    0.973176    0.781544    0.980198    0.794207    0.981118    0.838519    3400        51.200000   0.055967    1.102326    133.441833 
[37m[36mINFO[0m[0m 03/18 16:04:15 | 0.970848    0.975265    0.976939    0.795327    1.407681    0.970848    0.975265    0.977412    0.785311    0.975248    0.760671    0.978156    0.840000    3600        54.211765   0.041293    1.135592    136.394964 
[37m[36mINFO[0m[0m 03/18 16:10:17 | 0.979682    0.968198    0.987093    0.796468    1.104124    0.979682    0.968198    0.983059    0.749529    0.988957    0.785061    0.989263    0.854815    3800        57.223529   0.049819    1.114626    138.750010 
[37m[36mINFO[0m[0m 03/18 16:16:11 | 0.966431    0.954064    0.986430    0.787760    1.108921    0.966431    0.954064    0.983059    0.760829    0.984006    0.771341    0.992225    0.831111    4000        60.235294   0.047204    1.097736    133.943327 
[37m[36mINFO[0m[0m 03/18 16:22:07 | 0.976148    0.961131    0.989885    0.779557    1.265287    0.976148    0.961131    0.987765    0.736347    0.987814    0.766768    0.994076    0.835556    4200        63.247059   0.032436    1.108119    134.929858 
[37m[36mINFO[0m[0m 03/18 16:28:07 | 0.966431    0.971731    0.993187    0.794791    1.159521    0.966431    0.971731    0.990118    0.745763    0.993145    0.792683    0.996298    0.845926    4400        66.258824   0.033674    1.102307    139.528817 
[37m[36mINFO[0m[0m 03/18 16:34:05 | 0.977915    0.964664    0.994983    0.795121    1.316833    0.977915    0.964664    0.994353    0.751412    0.994669    0.785061    0.995927    0.848889    4600        69.270588   0.026311    1.091920    139.616744 
[37m[36mINFO[0m[0m 03/18 16:40:00 | 0.969965    0.964664    0.994058    0.793259    1.199478    0.969965    0.964664    0.992000    0.758945    0.992765    0.792683    0.997408    0.828148    4800        72.282353   0.034290    1.092332    136.852121 
[37m[36mINFO[0m[0m 03/18 16:45:56 | 0.969081    0.964664    0.994281    0.800144    1.242707    0.969081    0.964664    0.995294    0.758945    0.991622    0.789634    0.995927    0.851852    5000        75.294118   0.016010    1.092752    137.383860 
[37m[36mINFO[0m[0m 03/18 16:45:56 | Cumulative gradient change saved at train_output/VLCS/ERM/[0]/250318_14-14-53_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/18 16:45:58 | ---
[37m[36mINFO[0m[0m 03/18 16:45:58 | test-domain validation(oracle) = 98.675%
[37m[36mINFO[0m[0m 03/18 16:45:58 | training-domain validation(iid) = 98.675%
[37m[36mINFO[0m[0m 03/18 16:45:58 | last = 96.908%
[37m[36mINFO[0m[0m 03/18 16:45:58 | last (inD) = 80.014%
[37m[36mINFO[0m[0m 03/18 16:45:58 | training-domain validation (iid, inD) = 81.321%
[37m[36mINFO[0m[0m 03/18 16:45:58 | === Summary ===
[37m[36mINFO[0m[0m 03/18 16:45:58 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 0 --dataset VLCS
[37m[36mINFO[0m[0m 03/18 16:45:58 | Unique name: 250318_14-14-53_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 16:45:58 | Out path: train_output/VLCS/ERM/[0]/250318_14-14-53_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 16:45:58 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/18 16:45:58 | Dataset: VLCS
