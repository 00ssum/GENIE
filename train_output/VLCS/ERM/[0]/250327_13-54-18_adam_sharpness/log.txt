[37m[36mINFO[0m[0m 03/27 13:54:18 | Command :: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 0 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: adam_sharpness
	out_dir: train_output/VLCS/ERM/[0]/250327_13-54-18_adam_sharpness
	out_root: train_output/VLCS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 0
	unique_name: 250327_13-54-18_adam_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 13:54:18 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 13:54:18 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 13:54:18 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 13:54:18 | 
[37m[36mINFO[0m[0m 03/27 13:54:18 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/27 13:54:18 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 03/27 13:54:18 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/27 13:54:18 | Batch sizes for each domain: [0, 32, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 13:54:18 | steps-per-epoch for each domain: 66.41, 82.06, 84.41 -> min = 66.41
[37m[36mINFO[0m[0m 03/27 13:54:20 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 13:56:35 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 13:56:35 | 0.300353    0.275618    0.467250    0.479788    1.358681    0.300353    0.275618    0.513882    0.536723    0.415080    0.435976    0.472788    0.466667    0           0.000000    1.786248    1.387765    133.670389 
[37m[36mINFO[0m[0m 03/27 14:02:51 | 0.973498    0.982332    0.804956    0.778040    0.616423    0.973498    0.982332    0.773176    0.781544    0.789414    0.736280    0.852277    0.816296    200         3.011765    0.578576    1.112728    132.793969 
[37m[36mINFO[0m[0m 03/27 14:09:08 | 0.979682    0.985866    0.891162    0.800587    0.558370    0.979682    0.985866    0.844235    0.772128    0.904037    0.789634    0.925213    0.840000    400         6.023529    0.396180    1.100163    134.894439 
[37m[36mINFO[0m[0m 03/27 14:15:27 | 0.950530    0.943463    0.881770    0.763399    0.699918    0.950530    0.943463    0.785882    0.681733    0.910891    0.774390    0.948538    0.834074    600         9.035294    0.309146    1.118483    134.578125 
[37m[36mINFO[0m[0m 03/27 14:21:40 | 0.977032    0.989399    0.932809    0.770794    0.703172    0.977032    0.989399    0.901647    0.732580    0.936024    0.756098    0.960755    0.823704    800         12.047059   0.232375    1.102547    130.980062 
[37m[36mINFO[0m[0m 03/27 14:27:53 | 0.957597    0.961131    0.935826    0.781764    0.826609    0.957597    0.961131    0.909176    0.753296    0.937548    0.768293    0.960755    0.823704    1000        15.058824   0.192998    1.099085    131.689362 
[37m[36mINFO[0m[0m 03/27 14:34:06 | 0.979682    0.968198    0.952610    0.800275    1.056385    0.979682    0.968198    0.947765    0.783427    0.947829    0.775915    0.962236    0.841481    1200        18.070588   0.143177    1.102126    130.971522 
[37m[36mINFO[0m[0m 03/27 14:40:22 | 0.980565    0.975265    0.957004    0.797700    1.057016    0.980565    0.975265    0.941176    0.781544    0.953161    0.778963    0.976675    0.832593    1400        21.082353   0.106772    1.110931    132.489827 
[37m[36mINFO[0m[0m 03/27 14:46:32 | 0.931979    0.929329    0.963904    0.781922    0.880252    0.931979    0.929329    0.966118    0.745763    0.973724    0.789634    0.951870    0.810370    1600        24.094118   0.096466    1.091199    130.943059 
[37m[36mINFO[0m[0m 03/27 14:52:49 | 0.945230    0.950530    0.963454    0.783259    0.841916    0.945230    0.950530    0.952000    0.760829    0.966870    0.765244    0.971492    0.823704    1800        27.105882   0.082357    1.107948    133.137972 
[37m[36mINFO[0m[0m 03/27 14:59:10 | 0.980565    0.982332    0.984913    0.788284    0.907441    0.980565    0.982332    0.975059    0.753296    0.988195    0.778963    0.991485    0.832593    2000        30.117647   0.069062    1.100390    140.238274 
[37m[36mINFO[0m[0m 03/27 15:05:26 | 0.981449    0.978799    0.977388    0.788400    1.057761    0.981449    0.978799    0.973176    0.777778    0.976390    0.763720    0.982599    0.823704    2200        33.129412   0.063364    1.109556    132.483142 
[37m[36mINFO[0m[0m 03/27 15:11:43 | 0.924912    0.936396    0.972996    0.793123    1.089058    0.924912    0.936396    0.980235    0.772128    0.967631    0.783537    0.971122    0.823704    2400        36.141176   0.049514    1.090683    137.630039 
[37m[36mINFO[0m[0m 03/27 15:18:02 | 0.954947    0.957597    0.986659    0.796775    1.258364    0.954947    0.957597    0.985882    0.783427    0.987053    0.771341    0.987042    0.835556    2600        39.152941   0.052553    1.110958    135.141670 
[37m[36mINFO[0m[0m 03/27 15:24:17 | 0.959364    0.968198    0.992259    0.801438    1.109573    0.959364    0.968198    0.991059    0.792844    0.992384    0.775915    0.993336    0.835556    2800        42.164706   0.040047    1.097231    133.773414 
[37m[36mINFO[0m[0m 03/27 15:30:33 | 0.967314    0.971731    0.981603    0.788543    1.057789    0.967314    0.971731    0.980235    0.758945    0.977532    0.763720    0.987042    0.842963    3000        45.176471   0.046645    1.099822    134.691597 
[37m[36mINFO[0m[0m 03/27 15:36:58 | 0.954947    0.943463    0.962319    0.763341    1.067989    0.954947    0.943463    0.981176    0.747646    0.940213    0.742378    0.965568    0.800000    3200        48.188235   0.040475    1.129328    137.528866 
[37m[36mINFO[0m[0m 03/27 15:43:14 | 0.978799    0.975265    0.994657    0.793737    1.099318    0.978799    0.975265    0.994824    0.766478    0.995811    0.786585    0.993336    0.828148    3400        51.200000   0.039813    1.108904    132.990584 
[37m[36mINFO[0m[0m 03/27 15:49:36 | 0.941696    0.957597    0.979621    0.776827    1.236074    0.941696    0.957597    0.969412    0.745763    0.979817    0.772866    0.989633    0.811852    3600        54.211765   0.039676    1.113446    138.173957 
[37m[36mINFO[0m[0m 03/27 15:55:56 | 0.961131    0.964664    0.992573    0.793454    0.998097    0.961131    0.964664    0.993882    0.770245    0.991241    0.780488    0.992595    0.829630    3800        57.223529   0.044911    1.105297    137.448563 
[37m[36mINFO[0m[0m 03/27 16:02:12 | 0.940813    0.954064    0.983464    0.772587    1.238701    0.940813    0.954064    0.989647    0.755179    0.986291    0.775915    0.974454    0.786667    4000        60.235294   0.032807    1.090528    136.269991 
[37m[36mINFO[0m[0m 03/27 16:08:30 | 0.926678    0.946996    0.988367    0.771808    1.303694    0.926678    0.946996    0.985412    0.747646    0.988576    0.750000    0.991114    0.817778    4200        63.247059   0.036377    1.107586    135.499493 
[37m[36mINFO[0m[0m 03/27 16:14:49 | 0.924028    0.936396    0.984965    0.777097    1.259855    0.924028    0.936396    0.977882    0.734463    0.985529    0.782012    0.991485    0.814815    4400        66.258824   0.034808    1.108162    135.553679 
[37m[36mINFO[0m[0m 03/27 16:21:10 | 0.954947    0.964664    0.989197    0.774517    1.445256    0.954947    0.964664    0.992471    0.743879    0.984006    0.751524    0.991114    0.828148    4600        69.270588   0.028545    1.118002    136.347616 
[37m[36mINFO[0m[0m 03/27 16:27:34 | 0.977032    0.975265    0.995618    0.781388    1.062425    0.977032    0.975265    0.996235    0.770245    0.995430    0.757622    0.995187    0.816296    4800        72.282353   0.034253    1.124038    136.876197 
[37m[36mINFO[0m[0m 03/27 16:33:51 | 0.950530    0.950530    0.981564    0.772827    1.375278    0.950530    0.950530    0.983059    0.764595    0.978294    0.730183    0.983340    0.823704    5000        75.294118   0.028161    1.107623    134.408502 
[37m[36mINFO[0m[0m 03/27 16:34:12 | Cumulative gradient change saved at train_output/VLCS/ERM/[0]/250327_13-54-18_adam_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 16:34:14 | ---
[37m[36mINFO[0m[0m 03/27 16:34:14 | test-domain validation(oracle) = 97.703%
[37m[36mINFO[0m[0m 03/27 16:34:14 | training-domain validation(iid) = 95.936%
[37m[36mINFO[0m[0m 03/27 16:34:14 | last = 95.053%
[37m[36mINFO[0m[0m 03/27 16:34:14 | last (inD) = 77.283%
[37m[36mINFO[0m[0m 03/27 16:34:14 | training-domain validation (iid, inD) = 80.144%
[37m[36mINFO[0m[0m 03/27 16:34:14 | === Summary ===
[37m[36mINFO[0m[0m 03/27 16:34:14 | Command: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 0 --dataset VLCS
[37m[36mINFO[0m[0m 03/27 16:34:14 | Unique name: 250327_13-54-18_adam_sharpness
[37m[36mINFO[0m[0m 03/27 16:34:14 | Out path: train_output/VLCS/ERM/[0]/250327_13-54-18_adam_sharpness
[37m[36mINFO[0m[0m 03/27 16:34:14 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 16:34:14 | Dataset: VLCS
