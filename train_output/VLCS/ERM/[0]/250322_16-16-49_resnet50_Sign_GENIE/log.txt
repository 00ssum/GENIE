[37m[36mINFO[0m[0m 03/22 16:16:49 | Command :: /jsm0707/GENIE/train_all.py resnet50_Sign_GENIE config/resnet50_Sign_GENIE.yaml --algorithm ERM --test_envs 0 --dataset VLCS --trial_seed 1 --hparams_seed 19
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_Sign_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 19
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_Sign_GENIE
	out_dir: train_output/VLCS/ERM/[0]/250322_16-16-49_resnet50_Sign_GENIE
	out_root: train_output/VLCS/ERM/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 1
	unique_name: 250322_16-16-49_resnet50_Sign_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: sign_genie
	freeze_bn: False
	pretrained: True
	lr: 3.7692486045276154e-05
	batch_size: 10
	weight_decay: 0.0003178970604685295
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/22 16:16:49 | n_steps = 5001
[37m[36mINFO[0m[0m 03/22 16:16:49 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/22 16:16:49 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/22 16:16:49 | 
[37m[36mINFO[0m[0m 03/22 16:16:49 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/22 16:16:49 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 03/22 16:16:49 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/22 16:16:49 | Batch sizes for each domain: [0, 10, 10, 10] (total=30)
[37m[36mINFO[0m[0m 03/22 16:16:49 | steps-per-epoch for each domain: 212.50, 262.60, 270.10 -> min = 212.50
[37m[36mINFO[0m[0m 03/22 16:16:50 | # of params = 23518277
[37m[36mINFO[0m[0m 03/22 16:19:06 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/22 16:19:06 | 0.613958    0.618375    0.431964    0.428828    1.263228    0.613958    0.618375    0.467294    0.461394    0.373953    0.417683    0.454646    0.407407    0           0.000000    1.769563    1.290922    135.038505 
[37m[36mINFO[0m[0m 03/22 16:22:37 | 0.920495    0.911661    0.772163    0.775264    0.630352    0.920495    0.911661    0.741176    0.734463    0.772277    0.797256    0.803036    0.794074    200         0.941176    0.742972    0.360904    138.341604 
[37m[36mINFO[0m[0m 03/22 16:26:05 | 0.973498    0.950530    0.786662    0.779786    0.589861    0.973498    0.950530    0.744000    0.736347    0.797030    0.791159    0.818956    0.811852    400         1.882353    0.590172    0.364186    134.697114 
[37m[36mINFO[0m[0m 03/22 16:29:32 | 0.977915    0.968198    0.801360    0.787868    0.564688    0.977915    0.968198    0.766588    0.747646    0.771516    0.777439    0.865976    0.838519    600         2.823529    0.526322    0.350142    137.386421 
[37m[36mINFO[0m[0m 03/22 16:33:01 | 0.975265    0.975265    0.826899    0.791031    0.561685    0.975265    0.975265    0.780235    0.747646    0.825971    0.798780    0.874491    0.826667    800         3.764706    0.512512    0.355216    138.403481 
[37m[36mINFO[0m[0m 03/22 16:36:29 | 0.981449    0.978799    0.800526    0.779614    0.592495    0.981449    0.978799    0.712000    0.696798    0.820640    0.809451    0.868937    0.832593    1000        4.705882    0.489326    0.356927    135.953906 
[37m[36mINFO[0m[0m 03/22 16:39:58 | 0.984982    0.964664    0.852033    0.812774    0.520675    0.984982    0.964664    0.794824    0.766478    0.856055    0.815549    0.905220    0.856296    1200        5.647059    0.468808    0.363393    136.599906 
[37m[36mINFO[0m[0m 03/22 16:43:25 | 0.986749    0.978799    0.843208    0.795020    0.576434    0.986749    0.978799    0.768941    0.738230    0.861386    0.821646    0.899297    0.825185    1400        6.588235    0.446428    0.360058    135.224421 
[37m[36mINFO[0m[0m 03/22 16:46:55 | 0.938163    0.922261    0.850796    0.796886    0.549852    0.938163    0.922261    0.807529    0.766478    0.864813    0.806402    0.880044    0.817778    1600        7.529412    0.420757    0.363353    136.943612 
[37m[36mINFO[0m[0m 03/22 16:50:23 | 0.975265    0.985866    0.855040    0.794682    0.549065    0.975265    0.985866    0.822588    0.766478    0.847677    0.782012    0.894854    0.835556    1800        8.470588    0.400465    0.358678    136.743782 
[37m[36mINFO[0m[0m 03/22 16:53:50 | 0.984099    0.964664    0.857970    0.796103    0.602303    0.984099    0.964664    0.820706    0.749529    0.845392    0.798780    0.907812    0.840000    2000        9.411765    0.369787    0.358137    135.378000 
[37m[36mINFO[0m[0m 03/22 16:57:21 | 0.965548    0.968198    0.869861    0.791451    0.583583    0.965548    0.968198    0.820235    0.749529    0.880427    0.829268    0.908923    0.795556    2200        10.352941   0.380618    0.365362    137.229773 
[37m[36mINFO[0m[0m 03/22 17:00:49 | 0.972615    0.968198    0.869127    0.794561    0.591082    0.972615    0.968198    0.791059    0.732580    0.891851    0.815549    0.924472    0.835556    2400        11.294118   0.344473    0.361998    136.212904 
[37m[36mINFO[0m[0m 03/22 17:04:29 | 0.982332    0.968198    0.884319    0.792090    0.585022    0.982332    0.968198    0.816471    0.747646    0.897944    0.806402    0.938541    0.822222    2600        12.235294   0.335595    0.369006    145.425598 
[37m[36mINFO[0m[0m 03/22 17:07:58 | 0.981449    0.968198    0.880180    0.797156    0.625765    0.981449    0.968198    0.838118    0.755179    0.884615    0.815549    0.917808    0.820741    2800        13.176471   0.324590    0.360609    137.482321 
[37m[36mINFO[0m[0m 03/22 17:11:29 | 0.967314    0.964664    0.879432    0.782327    0.685862    0.967314    0.964664    0.811765    0.728814    0.899467    0.803354    0.927064    0.814815    3000        14.117647   0.305220    0.369279    136.944149 
[37m[36mINFO[0m[0m 03/22 17:15:00 | 0.971731    0.957597    0.890498    0.788073    0.621430    0.971731    0.957597    0.855529    0.749529    0.878903    0.785061    0.937060    0.829630    3200        15.058824   0.305786    0.369618    137.224845 
[37m[36mINFO[0m[0m 03/22 17:18:31 | 0.962898    0.961131    0.834965    0.722246    0.799842    0.962898    0.961131    0.832941    0.728814    0.799315    0.663110    0.872640    0.774815    3400        16.000000   0.285358    0.362868    138.704174 
[37m[36mINFO[0m[0m 03/22 17:22:01 | 0.980565    0.954064    0.895860    0.794066    0.657426    0.980565    0.954064    0.842824    0.747646    0.902513    0.806402    0.942244    0.828148    3600        16.941176   0.271423    0.362172    137.602447 
[37m[36mINFO[0m[0m 03/22 17:25:31 | 0.989399    0.982332    0.909880    0.776590    0.703827    0.989399    0.982332    0.857412    0.726930    0.918507    0.785061    0.953721    0.817778    3800        17.882353   0.293467    0.361243    136.976255 
