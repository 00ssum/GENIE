[37m[36mINFO[0m[0m 03/27 16:34:18 | Command :: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 1 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: adam_sharpness
	out_dir: train_output/VLCS/ERM/[1]/250327_16-34-18_adam_sharpness
	out_root: train_output/VLCS/ERM/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250327_16-34-18_adam_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 16:34:18 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 16:34:18 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 16:34:18 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 16:34:18 | 
[37m[36mINFO[0m[0m 03/27 16:34:18 | Testenv name escaping te_L -> te_L
[37m[36mINFO[0m[0m 03/27 16:34:18 | Test envs = [1], name = te_L
[37m[36mINFO[0m[0m 03/27 16:34:18 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 03/27 16:34:18 | Batch sizes for each domain: [32, 0, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 16:34:18 | steps-per-epoch for each domain: 35.38, 82.06, 84.41 -> min = 35.38
[37m[36mINFO[0m[0m 03/27 16:34:20 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 16:36:33 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 16:36:33 | 0.448471    0.461394    0.528018    0.529603    1.387600    0.621908    0.597173    0.448471    0.461394    0.463442    0.492378    0.498704    0.499259    0           0.000000    1.800293    0.991011    132.177912 
[37m[36mINFO[0m[0m 03/27 16:39:58 | 0.642353    0.630885    0.908782    0.881436    0.358270    1.000000    0.996466    0.642353    0.630885    0.851485    0.804878    0.874861    0.842963    200         5.653710    0.339127    0.239118    129.501801 
[37m[36mINFO[0m[0m 03/27 16:43:22 | 0.648471    0.649718    0.921030    0.871083    0.377364    0.999117    0.992933    0.648471    0.649718    0.859863    0.774390    0.904110    0.845926    400         11.307420   0.211921    0.239472    128.093566 
[37m[36mINFO[0m[0m 03/27 16:46:50 | 0.651294    0.651601    0.953264    0.881436    0.396828    0.997350    0.996466    0.651294    0.651601    0.926123    0.804878    0.936320    0.842963    600         16.961131   0.140230    0.239164    131.173707 
[37m[36mINFO[0m[0m 03/27 16:50:24 | 0.601882    0.600753    0.971570    0.878867    0.402392    1.000000    0.996466    0.601882    0.600753    0.955065    0.794207    0.959645    0.845926    800         22.614841   0.112684    0.245147    136.670856 
[37m[36mINFO[0m[0m 03/27 16:54:00 | 0.598118    0.591337    0.981280    0.885477    0.370716    1.000000    1.000000    0.598118    0.591337    0.964204    0.794207    0.979637    0.862222    1000        28.268551   0.094602    0.248480    137.614032 
[37m[36mINFO[0m[0m 03/27 16:57:32 | 0.619294    0.627119    0.979091    0.881725    0.500043    1.000000    0.992933    0.619294    0.627119    0.967631    0.803354    0.969641    0.848889    1200        33.922261   0.058706    0.241105    134.804330 
[37m[36mINFO[0m[0m 03/27 17:01:03 | 0.627294    0.621469    0.984203    0.879157    0.521934    1.000000    1.000000    0.627294    0.621469    0.973343    0.804878    0.979267    0.832593    1400        39.575972   0.060591    0.242613    133.750550 
[37m[36mINFO[0m[0m 03/27 17:04:35 | 0.612706    0.615819    0.993967    0.879883    0.517338    1.000000    0.996466    0.612706    0.615819    0.988195    0.797256    0.993706    0.845926    1600        45.229682   0.041682    0.237232    136.631379 
[37m[36mINFO[0m[0m 03/27 17:08:03 | 0.606118    0.600753    0.988653    0.881436    0.562963    0.998233    0.996466    0.606118    0.600753    0.984387    0.804878    0.983340    0.842963    1800        50.883392   0.040711    0.237846    131.923639 
[37m[36mINFO[0m[0m 03/27 17:11:36 | 0.664000    0.674200    0.982380    0.870822    0.597384    0.998233    0.996466    0.664000    0.674200    0.973343    0.778963    0.975565    0.837037    2000        56.537102   0.043518    0.249635    134.286798 
[37m[36mINFO[0m[0m 03/27 17:15:09 | 0.639059    0.634652    0.989468    0.883458    0.636780    1.000000    1.000000    0.639059    0.634652    0.982102    0.789634    0.986301    0.860741    2200        62.190813   0.027802    0.249870    134.195517 
[37m[36mINFO[0m[0m 03/27 17:18:38 | 0.592471    0.593220    0.984168    0.871316    0.614674    1.000000    0.996466    0.592471    0.593220    0.969535    0.778963    0.982969    0.838519    2400        67.844523   0.037820    0.235553    133.424295 
[37m[36mINFO[0m[0m 03/27 17:22:12 | 0.592471    0.595104    0.994485    0.881963    0.673355    1.000000    1.000000    0.592471    0.595104    0.990861    0.788110    0.992595    0.857778    2600        73.498233   0.026866    0.267120    131.909062 
[37m[36mINFO[0m[0m 03/27 17:25:43 | 0.593412    0.581921    0.982410    0.863923    0.807933    0.999117    0.996466    0.593412    0.581921    0.971439    0.780488    0.976675    0.814815    2800        79.151943   0.024368    0.256581    131.036417 
[37m[36mINFO[0m[0m 03/27 17:29:11 | 0.595765    0.593220    0.991724    0.875424    0.622885    0.994700    0.996466    0.595765    0.593220    0.990099    0.795732    0.990374    0.834074    3000        84.805654   0.025520    0.243075    131.197098 
[37m[36mINFO[0m[0m 03/27 17:32:39 | 0.672941    0.691149    0.988923    0.865897    0.586659    0.999117    0.989399    0.672941    0.691149    0.981721    0.768293    0.985931    0.840000    3200        90.459364   0.029688    0.240268    131.529054 
[37m[36mINFO[0m[0m 03/27 17:36:02 | 0.653176    0.632768    0.991947    0.873900    0.673828    1.000000    0.996466    0.653176    0.632768    0.983244    0.791159    0.992595    0.834074    3400        96.113074   0.023164    0.232451    129.038530 
[37m[36mINFO[0m[0m 03/27 17:39:25 | 0.606588    0.600753    0.993576    0.874683    0.785371    1.000000    0.992933    0.606588    0.600753    0.985910    0.789634    0.994817    0.841481    3600        101.766784  0.024195    0.225642    129.619456 
[37m[36mINFO[0m[0m 03/27 17:42:51 | 0.622588    0.623352    0.990663    0.881597    0.590857    1.000000    1.000000    0.622588    0.623352    0.991241    0.801829    0.980748    0.842963    3800        107.420495  0.027089    0.238151    129.973066 
[37m[36mINFO[0m[0m 03/27 17:46:16 | 0.638118    0.630885    0.994722    0.883616    0.568548    1.000000    1.000000    0.638118    0.630885    0.989718    0.806402    0.994447    0.844444    4000        113.074205  0.026145    0.232024    130.350245 
[37m[36mINFO[0m[0m 03/27 17:49:41 | 0.677647    0.687382    0.987375    0.864596    0.904540    0.999117    0.996466    0.677647    0.687382    0.974486    0.746951    0.988523    0.850370    4200        118.727915  0.016558    0.226444    131.323287 
[37m[36mINFO[0m[0m 03/27 17:53:06 | 0.618353    0.629002    0.997486    0.879522    0.612774    1.000000    1.000000    0.618353    0.629002    0.995050    0.791159    0.997408    0.847407    4400        124.381625  0.017923    0.233830    130.123787 
[37m[36mINFO[0m[0m 03/27 17:56:31 | 0.636235    0.630885    0.983669    0.862270    0.669726    0.999117    0.996466    0.636235    0.630885    0.974105    0.762195    0.977786    0.828148    4600        130.035336  0.021567    0.230430    130.855552 
[37m[36mINFO[0m[0m 03/27 17:59:58 | 0.640941    0.640301    0.998258    0.873767    0.627103    1.000000    0.992933    0.640941    0.640301    0.998477    0.797256    0.996298    0.831111    4800        135.689046  0.026834    0.233137    131.476955 
[37m[36mINFO[0m[0m 03/27 18:03:23 | 0.637647    0.615819    0.995250    0.869749    0.756799    1.000000    0.996466    0.637647    0.615819    0.993526    0.769817    0.992225    0.842963    5000        141.342756  0.014913    0.234493    129.257717 
[37m[36mINFO[0m[0m 03/27 18:03:51 | Cumulative gradient change saved at train_output/VLCS/ERM/[1]/250327_16-34-18_adam_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 18:03:52 | ---
[37m[36mINFO[0m[0m 03/27 18:03:52 | test-domain validation(oracle) = 67.294%
[37m[36mINFO[0m[0m 03/27 18:03:52 | training-domain validation(iid) = 59.812%
[37m[36mINFO[0m[0m 03/27 18:03:52 | last = 63.765%
[37m[36mINFO[0m[0m 03/27 18:03:52 | last (inD) = 86.975%
[37m[36mINFO[0m[0m 03/27 18:03:52 | training-domain validation (iid, inD) = 88.548%
[37m[36mINFO[0m[0m 03/27 18:03:52 | === Summary ===
[37m[36mINFO[0m[0m 03/27 18:03:52 | Command: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 1 --dataset VLCS
[37m[36mINFO[0m[0m 03/27 18:03:52 | Unique name: 250327_16-34-18_adam_sharpness
[37m[36mINFO[0m[0m 03/27 18:03:52 | Out path: train_output/VLCS/ERM/[1]/250327_16-34-18_adam_sharpness
[37m[36mINFO[0m[0m 03/27 18:03:52 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 18:03:52 | Dataset: VLCS
