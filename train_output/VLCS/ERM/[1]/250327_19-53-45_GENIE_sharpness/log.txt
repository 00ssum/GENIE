[37m[36mINFO[0m[0m 03/27 19:53:45 | Command :: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 1 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: GENIE_sharpness
	out_dir: train_output/VLCS/ERM/[1]/250327_19-53-45_GENIE_sharpness
	out_root: train_output/VLCS/ERM/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250327_19-53-45_GENIE_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 19:53:45 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 19:53:45 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 19:53:45 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 19:53:45 | 
[37m[36mINFO[0m[0m 03/27 19:53:45 | Testenv name escaping te_L -> te_L
[37m[36mINFO[0m[0m 03/27 19:53:45 | Test envs = [1], name = te_L
[37m[36mINFO[0m[0m 03/27 19:53:45 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 03/27 19:53:45 | Batch sizes for each domain: [32, 0, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 19:53:45 | steps-per-epoch for each domain: 35.38, 82.06, 84.41 -> min = 35.38
[37m[36mINFO[0m[0m 03/27 19:53:46 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 19:56:07 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 19:56:07 | 0.459765    0.489642    0.479821    0.487365    1.478232    0.611307    0.628975    0.459765    0.489642    0.384615    0.387195    0.443539    0.445926    0           0.000000    1.800293    1.030501    139.890913 
[37m[36mINFO[0m[0m 03/27 19:59:45 | 0.582118    0.574388    0.905159    0.882903    0.321708    0.999117    0.996466    0.582118    0.574388    0.838538    0.803354    0.877823    0.848889    200         5.653710    0.386408    0.254202    138.854256 
[37m[36mINFO[0m[0m 03/27 20:03:16 | 0.666353    0.661017    0.926500    0.883487    0.305637    1.000000    1.000000    0.666353    0.661017    0.872430    0.792683    0.907071    0.857778    400         11.307420   0.225376    0.241332    134.067553 
[37m[36mINFO[0m[0m 03/27 20:06:47 | 0.626353    0.612053    0.933085    0.890876    0.310117    0.997350    0.996466    0.626353    0.612053    0.892612    0.810976    0.909293    0.865185    600         16.961131   0.185946    0.246010    133.376704 
[37m[36mINFO[0m[0m 03/27 20:10:26 | 0.592000    0.568738    0.942545    0.884907    0.329643    0.997350    0.996466    0.592000    0.568738    0.901371    0.806402    0.928915    0.851852    800         22.614841   0.149220    0.257566    137.867454 
[37m[36mINFO[0m[0m 03/27 20:14:06 | 0.649882    0.662900    0.966369    0.887727    0.348728    1.000000    0.996466    0.649882    0.662900    0.939832    0.791159    0.959274    0.875556    1000        28.268551   0.127503    0.268001    138.454238 
[37m[36mINFO[0m[0m 03/27 20:17:49 | 0.683765    0.679849    0.962457    0.895742    0.339708    0.999117    0.996466    0.683765    0.679849    0.935644    0.803354    0.952610    0.887407    1200        33.922261   0.112307    0.242721    145.872246 
[37m[36mINFO[0m[0m 03/27 20:21:27 | 0.649412    0.632768    0.979930    0.896033    0.392466    1.000000    1.000000    0.649412    0.632768    0.964966    0.814024    0.974824    0.874074    1400        39.575972   0.078889    0.261376    136.370384 
[37m[36mINFO[0m[0m 03/27 20:24:58 | 0.661176    0.666667    0.973012    0.889266    0.447397    1.000000    0.996466    0.661176    0.666667    0.950876    0.797256    0.968160    0.874074    1600        45.229682   0.080561    0.252954    132.718991 
[37m[36mINFO[0m[0m 03/27 20:28:30 | 0.580235    0.561205    0.981436    0.882993    0.447647    1.000000    1.000000    0.580235    0.561205    0.967631    0.792683    0.976675    0.856296    1800        50.883392   0.055617    0.239049    136.031822 
[37m[36mINFO[0m[0m 03/27 20:32:00 | 0.612235    0.612053    0.976949    0.869677    0.487017    0.995583    0.996466    0.612235    0.612053    0.961919    0.762195    0.973343    0.850370    2000        56.537102   0.048793    0.243768    131.984471 
[37m[36mINFO[0m[0m 03/27 20:35:36 | 0.615059    0.613936    0.994242    0.891503    0.457264    1.000000    1.000000    0.615059    0.613936    0.991241    0.804878    0.991485    0.869630    2200        62.190813   0.039840    0.250362    137.511406 
[37m[36mINFO[0m[0m 03/27 20:39:09 | 0.654588    0.649718    0.983798    0.879143    0.568933    1.000000    1.000000    0.654588    0.649718    0.982864    0.803354    0.968530    0.834074    2400        67.844523   0.035029    0.244228    135.035533 
[37m[36mINFO[0m[0m 03/27 20:42:44 | 0.622118    0.623352    0.992726    0.886915    0.508702    1.000000    1.000000    0.622118    0.623352    0.987433    0.789634    0.990744    0.871111    2600        73.498233   0.041320    0.251647    137.070147 
[37m[36mINFO[0m[0m 03/27 20:46:23 | 0.657882    0.668550    0.991063    0.890980    0.618415    0.999117    1.000000    0.657882    0.668550    0.986291    0.801829    0.987782    0.871111    2800        79.151943   0.020536    0.265438    136.806519 
[37m[36mINFO[0m[0m 03/27 20:49:57 | 0.644706    0.634652    0.996118    0.886925    0.586514    1.000000    0.996466    0.644706    0.634652    0.993907    0.810976    0.994447    0.853333    3000        84.805654   0.023779    0.246300    135.875806 
[37m[36mINFO[0m[0m 03/27 20:53:37 | 0.651294    0.651601    0.994427    0.886606    0.670298    0.999117    0.992933    0.651294    0.651601    0.989718    0.797256    0.994447    0.869630    3200        90.459364   0.018026    0.264523    138.516973 
[37m[36mINFO[0m[0m 03/27 20:57:18 | 0.640000    0.636535    0.994841    0.890035    0.587889    1.000000    1.000000    0.640000    0.636535    0.989337    0.806402    0.995187    0.863704    3400        96.113074   0.023136    0.245556    143.285927 
[37m[36mINFO[0m[0m 03/27 21:00:55 | 0.660706    0.651601    0.994982    0.890177    0.580835    1.000000    0.992933    0.660706    0.651601    0.991241    0.809451    0.993706    0.868148    3600        101.766784  0.026165    0.244674    139.517859 
[37m[36mINFO[0m[0m 03/27 21:04:27 | 0.656941    0.645951    0.994482    0.888349    0.682982    1.000000    0.996466    0.656941    0.645951    0.990480    0.804878    0.992966    0.863704    3800        107.420495  0.013626    0.247226    133.927456 
[37m[36mINFO[0m[0m 03/27 21:08:02 | 0.608471    0.612053    0.992599    0.883819    0.631657    1.000000    0.996466    0.608471    0.612053    0.987053    0.795732    0.990744    0.859259    4000        113.074205  0.015613    0.238462    138.995286 
[37m[36mINFO[0m[0m 03/27 21:11:33 | 0.631059    0.627119    0.994859    0.888003    0.696566    1.000000    1.000000    0.631059    0.627119    0.991241    0.800305    0.993336    0.863704    4200        118.727915  0.017198    0.243477    133.119538 
[37m[36mINFO[0m[0m 03/27 21:15:00 | 0.603765    0.615819    0.996238    0.888349    0.717188    1.000000    0.996466    0.603765    0.615819    0.993526    0.804878    0.995187    0.863704    4400        124.381625  0.017765    0.236653    131.184052 
[37m[36mINFO[0m[0m 03/27 21:18:29 | 0.632000    0.630885    0.993614    0.887903    0.633353    1.000000    1.000000    0.632000    0.630885    0.990099    0.789634    0.990744    0.874074    4600        130.035336  0.017693    0.243481    131.092798 
[37m[36mINFO[0m[0m 03/27 21:22:00 | 0.648941    0.649718    0.994623    0.889033    0.732698    1.000000    1.000000    0.648941    0.649718    0.992384    0.804878    0.991485    0.862222    4800        135.689046  0.010655    0.237325    133.158082 
[37m[36mINFO[0m[0m 03/27 21:25:29 | 0.581176    0.581921    0.994852    0.878692    0.693360    1.000000    1.000000    0.581176    0.581921    0.990480    0.807927    0.994076    0.828148    5000        141.342756  0.009065    0.229535    133.191082 
[37m[36mINFO[0m[0m 03/27 21:25:57 | Cumulative gradient change saved at train_output/VLCS/ERM/[1]/250327_19-53-45_GENIE_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 21:25:59 | ---
[37m[36mINFO[0m[0m 03/27 21:25:59 | test-domain validation(oracle) = 68.376%
[37m[36mINFO[0m[0m 03/27 21:25:59 | training-domain validation(iid) = 64.941%
[37m[36mINFO[0m[0m 03/27 21:25:59 | last = 58.118%
[37m[36mINFO[0m[0m 03/27 21:25:59 | last (inD) = 87.869%
[37m[36mINFO[0m[0m 03/27 21:25:59 | training-domain validation (iid, inD) = 89.603%
[37m[36mINFO[0m[0m 03/27 21:25:59 | === Summary ===
[37m[36mINFO[0m[0m 03/27 21:25:59 | Command: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 1 --dataset VLCS
[37m[36mINFO[0m[0m 03/27 21:25:59 | Unique name: 250327_19-53-45_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 21:25:59 | Out path: train_output/VLCS/ERM/[1]/250327_19-53-45_GENIE_sharpness
[37m[36mINFO[0m[0m 03/27 21:25:59 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 21:25:59 | Dataset: VLCS
