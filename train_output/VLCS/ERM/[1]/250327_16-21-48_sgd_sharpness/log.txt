[37m[36mINFO[0m[0m 03/27 16:21:48 | Command :: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 1 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: sgd_sharpness
	out_dir: train_output/VLCS/ERM/[1]/250327_16-21-48_sgd_sharpness
	out_root: train_output/VLCS/ERM/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250327_16-21-48_sgd_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 16:21:48 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 16:21:48 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 16:21:48 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 16:21:48 | 
[37m[36mINFO[0m[0m 03/27 16:21:48 | Testenv name escaping te_L -> te_L
[37m[36mINFO[0m[0m 03/27 16:21:48 | Test envs = [1], name = te_L
[37m[36mINFO[0m[0m 03/27 16:21:48 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 03/27 16:21:48 | Batch sizes for each domain: [32, 0, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/27 16:21:48 | steps-per-epoch for each domain: 35.38, 82.06, 84.41 -> min = 35.38
[37m[36mINFO[0m[0m 03/27 16:21:50 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 16:24:00 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 16:24:00 | 0.070118    0.071563    0.177781    0.180296    1.817287    0.141343    0.141343    0.070118    0.071563    0.223915    0.246951    0.168086    0.152593    0           0.000000    1.800293    0.958414    129.078607 
[37m[36mINFO[0m[0m 03/27 16:27:22 | 0.456000    0.465160    0.524233    0.535688    1.248292    0.615724    0.628975    0.456000    0.465160    0.477152    0.484756    0.479822    0.493333    200         5.653710    1.453372    0.232661    127.790506 
[37m[36mINFO[0m[0m 03/27 16:30:43 | 0.506824    0.527307    0.597888    0.607192    1.059297    0.671378    0.671378    0.506824    0.527307    0.590632    0.599085    0.531655    0.551111    400         11.307420   1.150163    0.229297    126.860060 
[37m[36mINFO[0m[0m 03/27 16:34:03 | 0.601882    0.595104    0.692648    0.701352    0.918105    0.742933    0.749117    0.601882    0.595104    0.718203    0.714939    0.616809    0.640000    600         16.961131   0.982915    0.229507    126.709611 
[37m[36mINFO[0m[0m 03/27 16:37:25 | 0.598588    0.602637    0.731644    0.738423    0.798817    0.785336    0.809187    0.598588    0.602637    0.764280    0.742378    0.645317    0.663704    800         22.614841   0.842730    0.233764    127.526831 
[37m[36mINFO[0m[0m 03/27 16:40:47 | 0.584000    0.585687    0.762620    0.765123    0.701110    0.857774    0.862191    0.584000    0.585687    0.768850    0.757622    0.661237    0.675556    1000        28.268551   0.733688    0.230153    126.457486 
[37m[36mINFO[0m[0m 03/27 16:44:10 | 0.582588    0.587571    0.796097    0.798262    0.619099    0.915194    0.925795    0.582588    0.587571    0.784463    0.766768    0.688634    0.702222    1200        33.922261   0.645566    0.227313    130.071258 
[37m[36mINFO[0m[0m 03/27 16:47:34 | 0.587765    0.587571    0.824480    0.819423    0.552951    0.954064    0.961131    0.587765    0.587571    0.784463    0.766768    0.734913    0.730370    1400        39.575972   0.565784    0.231645    127.852196 
[37m[36mINFO[0m[0m 03/27 16:50:58 | 0.599059    0.593220    0.846668    0.839691    0.502784    0.974382    0.978799    0.599059    0.593220    0.796649    0.772866    0.768974    0.767407    1600        45.229682   0.505672    0.230863    128.646183 
[37m[36mINFO[0m[0m 03/27 16:54:20 | 0.599529    0.596987    0.851411    0.848480    0.465301    0.983216    0.985866    0.599529    0.596987    0.790937    0.774390    0.780081    0.785185    1800        50.883392   0.459187    0.227870    127.259023 
[37m[36mINFO[0m[0m 03/27 16:57:40 | 0.596235    0.591337    0.860769    0.857460    0.441043    0.986749    0.996466    0.596235    0.591337    0.794745    0.775915    0.800815    0.800000    2000        56.537102   0.439445    0.231874    126.142534 
[37m[36mINFO[0m[0m 03/27 17:01:02 | 0.611294    0.612053    0.868777    0.865129    0.417056    0.992049    1.000000    0.611294    0.612053    0.802361    0.783537    0.811922    0.811852    2200        62.190813   0.405393    0.230811    127.884718 
[37m[36mINFO[0m[0m 03/27 17:04:22 | 0.614588    0.612053    0.872307    0.867162    0.400352    0.990283    1.000000    0.614588    0.612053    0.807312    0.789634    0.819326    0.811852    2400        67.844523   0.383690    0.224127    127.226012 
[37m[36mINFO[0m[0m 03/27 17:07:46 | 0.608471    0.606403    0.877233    0.868643    0.389187    0.993816    1.000000    0.608471    0.606403    0.812262    0.789634    0.825620    0.816296    2600        73.498233   0.366834    0.239092    128.367099 
[37m[36mINFO[0m[0m 03/27 17:11:09 | 0.606588    0.606403    0.878693    0.872157    0.380400    0.995583    1.000000    0.606588    0.606403    0.813024    0.795732    0.827471    0.820741    2800        79.151943   0.364704    0.239835    127.437278 
[37m[36mINFO[0m[0m 03/27 17:14:29 | 0.612235    0.612053    0.878853    0.874641    0.371737    0.994700    1.000000    0.612235    0.612053    0.808835    0.797256    0.833025    0.826667    3000        84.805654   0.346725    0.231678    124.983148 
[37m[36mINFO[0m[0m 03/27 17:17:52 | 0.604706    0.610169    0.883000    0.875629    0.364599    0.996466    1.000000    0.604706    0.610169    0.819878    0.797256    0.832655    0.829630    3200        90.459364   0.350313    0.231483    129.220606 
[37m[36mINFO[0m[0m 03/27 17:21:14 | 0.607529    0.612053    0.885077    0.878592    0.358440    0.996466    1.000000    0.607529    0.612053    0.817593    0.797256    0.841170    0.838519    3400        96.113074   0.334422    0.232674    127.063647 
[37m[36mINFO[0m[0m 03/27 17:24:36 | 0.608471    0.613936    0.886090    0.878592    0.353945    0.999117    1.000000    0.608471    0.613936    0.818355    0.797256    0.840800    0.838519    3600        101.766784  0.332088    0.226539    128.711159 
[37m[36mINFO[0m[0m 03/27 17:28:04 | 0.609882    0.615819    0.888072    0.879593    0.349572    0.995583    1.000000    0.609882    0.615819    0.826352    0.798780    0.842281    0.840000    3800        107.420495  0.324213    0.238056    131.770440 
[37m[36mINFO[0m[0m 03/27 17:31:30 | 0.610353    0.613936    0.887850    0.881583    0.345676    0.997350    1.000000    0.610353    0.613936    0.818736    0.800305    0.847464    0.844444    4000        113.074205  0.316576    0.234641    130.417752 
[37m[36mINFO[0m[0m 03/27 17:34:51 | 0.616000    0.615819    0.893532    0.883558    0.340955    0.994700    1.000000    0.616000    0.615819    0.834730    0.800305    0.851166    0.850370    4200        118.727915  0.308396    0.233001    126.835064 
[37m[36mINFO[0m[0m 03/27 17:38:16 | 0.608471    0.615819    0.895951    0.882556    0.338741    0.996466    1.000000    0.608471    0.615819    0.832445    0.798780    0.858941    0.848889    4400        124.381625  0.315353    0.235321    129.455896 
[37m[36mINFO[0m[0m 03/27 17:41:41 | 0.611294    0.613936    0.895158    0.884546    0.335421    1.000000    1.000000    0.611294    0.613936    0.832826    0.800305    0.852647    0.853333    4600        130.035336  0.302847    0.231842    130.130750 
[37m[36mINFO[0m[0m 03/27 17:45:06 | 0.611765    0.612053    0.894489    0.884052    0.333103    0.999117    1.000000    0.611765    0.612053    0.832445    0.800305    0.851907    0.851852    4800        135.689046  0.294013    0.234889    129.527433 
[37m[36mINFO[0m[0m 03/27 17:48:29 | 0.608941    0.613936    0.890501    0.883022    0.331841    0.999117    1.000000    0.608941    0.613936    0.828256    0.795732    0.844132    0.853333    5000        141.342756  0.292071    0.227086    130.189610 
[37m[36mINFO[0m[0m 03/27 17:48:57 | Cumulative gradient change saved at train_output/VLCS/ERM/[1]/250327_16-21-48_sgd_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 17:48:59 | ---
[37m[36mINFO[0m[0m 03/27 17:48:59 | test-domain validation(oracle) = 60.988%
[37m[36mINFO[0m[0m 03/27 17:48:59 | training-domain validation(iid) = 61.129%
[37m[36mINFO[0m[0m 03/27 17:48:59 | last = 60.894%
[37m[36mINFO[0m[0m 03/27 17:48:59 | last (inD) = 88.302%
[37m[36mINFO[0m[0m 03/27 17:48:59 | training-domain validation (iid, inD) = 88.455%
[37m[36mINFO[0m[0m 03/27 17:48:59 | === Summary ===
[37m[36mINFO[0m[0m 03/27 17:48:59 | Command: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 1 --dataset VLCS
[37m[36mINFO[0m[0m 03/27 17:48:59 | Unique name: 250327_16-21-48_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 17:48:59 | Out path: train_output/VLCS/ERM/[1]/250327_16-21-48_sgd_sharpness
[37m[36mINFO[0m[0m 03/27 17:48:59 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 17:48:59 | Dataset: VLCS
