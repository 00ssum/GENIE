[37m[36mINFO[0m[0m 03/18 16:46:02 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 1 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/ERM/[1]/250318_16-46-02_resnet50_GENIE
	out_root: train_output/VLCS/ERM/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250318_16-46-02_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/18 16:46:02 | n_steps = 5001
[37m[36mINFO[0m[0m 03/18 16:46:02 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/18 16:46:02 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/18 16:46:02 | 
[37m[36mINFO[0m[0m 03/18 16:46:02 | Testenv name escaping te_L -> te_L
[37m[36mINFO[0m[0m 03/18 16:46:02 | Test envs = [1], name = te_L
[37m[36mINFO[0m[0m 03/18 16:46:02 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 03/18 16:46:02 | Batch sizes for each domain: [32, 0, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/18 16:46:02 | steps-per-epoch for each domain: 35.38, 82.06, 84.41 -> min = 35.38
[37m[36mINFO[0m[0m 03/18 16:46:03 | # of params = 23518277
[37m[36mINFO[0m[0m 03/18 16:48:19 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/18 16:48:19 | 0.459765    0.489642    0.479821    0.487365    1.478232    0.611307    0.628975    0.459765    0.489642    0.384615    0.387195    0.443539    0.445926    0           0.000000    1.800293    1.021794    134.854563 
[37m[36mINFO[0m[0m 03/18 16:51:25 | 0.613647    0.608286    0.911226    0.891109    0.304014    0.998233    1.000000    0.613647    0.608286    0.845773    0.815549    0.889670    0.857778    200         5.653710    0.385370    0.249401    136.636761 
[37m[36mINFO[0m[0m 03/18 16:54:30 | 0.663529    0.666667    0.926275    0.891996    0.308988    0.998233    1.000000    0.663529    0.666667    0.871668    0.804878    0.908923    0.871111    400         11.307420   0.227045    0.264880    131.692221 
[37m[36mINFO[0m[0m 03/18 16:57:35 | 0.618353    0.625235    0.939802    0.891820    0.330451    0.999117    0.996466    0.618353    0.625235    0.888043    0.806402    0.932247    0.872593    600         16.961131   0.183876    0.261697    132.235665 
[37m[36mINFO[0m[0m 03/18 17:00:44 | 0.656471    0.653484    0.951900    0.887466    0.320419    0.999117    1.000000    0.656471    0.653484    0.915080    0.795732    0.941503    0.866667    800         22.614841   0.144974    0.268283    135.817358 
[37m[36mINFO[0m[0m 03/18 17:04:04 | 0.645647    0.632768    0.964955    0.893273    0.323456    1.000000    0.996466    0.645647    0.632768    0.933740    0.803354    0.961126    0.880000    1000        28.268551   0.120096    0.333127    132.809313 
[37m[36mINFO[0m[0m 03/18 17:07:22 | 0.646118    0.644068    0.976143    0.889351    0.401740    1.000000    0.996466    0.646118    0.644068    0.955826    0.806402    0.972603    0.865185    1200        33.922261   0.086115    0.321066    134.441159 
[37m[36mINFO[0m[0m 03/18 17:10:25 | 0.640471    0.634652    0.981547    0.889964    0.395348    0.999117    1.000000    0.640471    0.634652    0.971439    0.798780    0.974084    0.871111    1400        39.575972   0.078235    0.252558    132.052125 
[37m[36mINFO[0m[0m 03/18 17:13:38 | 0.649412    0.647834    0.981658    0.894508    0.404668    1.000000    1.000000    0.649412    0.647834    0.964966    0.809451    0.980007    0.874074    1600        45.229682   0.064283    0.261394    140.893823 
[37m[36mINFO[0m[0m 03/18 17:16:40 | 0.609412    0.589454    0.974024    0.878549    0.506360    0.998233    1.000000    0.609412    0.589454    0.963823    0.792683    0.960015    0.842963    1800        50.883392   0.059211    0.252262    131.701366 
[37m[36mINFO[0m[0m 03/18 17:19:42 | 0.595765    0.612053    0.983646    0.890486    0.435459    1.000000    1.000000    0.595765    0.612053    0.966489    0.801829    0.984450    0.869630    2000        56.537102   0.055721    0.251408    131.668369 
[37m[36mINFO[0m[0m 03/18 17:22:49 | 0.586353    0.566855    0.984588    0.873045    0.514822    1.000000    1.000000    0.586353    0.566855    0.974867    0.785061    0.978897    0.834074    2200        62.190813   0.033420    0.264532    133.992929 
[37m[36mINFO[0m[0m 03/18 17:25:56 | 0.643765    0.645951    0.992729    0.890866    0.567943    1.000000    1.000000    0.643765    0.645951    0.987814    0.789634    0.990374    0.882963    2400        67.844523   0.040289    0.249396    137.357783 
[37m[36mINFO[0m[0m 03/18 17:29:15 | 0.592941    0.591337    0.989193    0.880989    0.580141    1.000000    1.000000    0.592941    0.591337    0.979056    0.789634    0.988523    0.853333    2600        73.498233   0.027255    0.310840    136.758020 
[37m[36mINFO[0m[0m 03/18 17:32:45 | 0.592941    0.589454    0.985215    0.867845    0.666709    1.000000    0.996466    0.592941    0.589454    0.976009    0.777439    0.979637    0.829630    2800        79.151943   0.026066    0.347635    140.125598 
[37m[36mINFO[0m[0m 03/18 17:36:01 | 0.624000    0.608286    0.992204    0.891996    0.611612    1.000000    1.000000    0.624000    0.608286    0.984387    0.804878    0.992225    0.871111    3000        84.805654   0.023047    0.286201    139.296610 
[37m[36mINFO[0m[0m 03/18 17:39:08 | 0.638118    0.632768    0.993382    0.884992    0.605979    1.000000    0.989399    0.638118    0.632768    0.991622    0.803354    0.988523    0.862222    3200        90.459364   0.020189    0.247422    136.763485 
[37m[36mINFO[0m[0m 03/18 17:42:17 | 0.656471    0.653484    0.994982    0.883981    0.583694    1.000000    1.000000    0.656471    0.653484    0.991241    0.792683    0.993706    0.859259    3400        96.113074   0.023172    0.246436    139.907879 
[37m[36mINFO[0m[0m 03/18 17:45:17 | 0.626353    0.604520    0.991605    0.878434    0.582387    1.000000    1.000000    0.626353    0.604520    0.986291    0.780488    0.988523    0.854815    3600        101.766784  0.017038    0.241566    132.232308 
[37m[36mINFO[0m[0m 03/18 17:48:37 | 0.648941    0.644068    0.988082    0.887552    0.667080    1.000000    1.000000    0.648941    0.644068    0.979056    0.804878    0.985191    0.857778    3800        107.420495  0.016357    0.330195    133.614924 
[37m[36mINFO[0m[0m 03/18 17:51:58 | 0.637176    0.621469    0.996107    0.896555    0.623355    1.000000    1.000000    0.637176    0.621469    0.992765    0.817073    0.995557    0.872593    4000        113.074205  0.022255    0.320174    137.318774 
[37m[36mINFO[0m[0m 03/18 17:55:00 | 0.649412    0.625235    0.994129    0.879000    0.642818    1.000000    1.000000    0.649412    0.625235    0.992384    0.788110    0.990004    0.848889    4200        118.727915  0.018414    0.249380    131.499979 
[37m[36mINFO[0m[0m 03/18 17:58:02 | 0.637176    0.638418    0.994475    0.884009    0.641251    1.000000    1.000000    0.637176    0.638418    0.989718    0.795732    0.993706    0.856296    4400        124.381625  0.013569    0.240214    134.353624 
[37m[36mINFO[0m[0m 03/18 18:01:04 | 0.641412    0.632768    0.998241    0.892462    0.660180    1.000000    1.000000    0.641412    0.632768    0.996573    0.801829    0.998149    0.875556    4600        130.035336  0.011250    0.247612    132.470696 
[37m[36mINFO[0m[0m 03/18 18:04:10 | 0.614118    0.602637    0.997950    0.888031    0.701313    0.999117    1.000000    0.614118    0.602637    0.996954    0.803354    0.997779    0.860741    4800        135.689046  0.007113    0.244361    136.729957 
[37m[36mINFO[0m[0m 03/18 18:07:20 | 0.624471    0.593220    0.995994    0.883966    0.630236    1.000000    1.000000    0.624471    0.593220    0.993907    0.791159    0.994076    0.860741    5000        141.342756  0.010323    0.281728    134.332724 
[37m[36mINFO[0m[0m 03/18 18:07:21 | Cumulative gradient change saved at train_output/VLCS/ERM/[1]/250318_16-46-02_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/18 18:07:22 | ---
[37m[36mINFO[0m[0m 03/18 18:07:22 | test-domain validation(oracle) = 66.353%
[37m[36mINFO[0m[0m 03/18 18:07:22 | training-domain validation(iid) = 63.718%
[37m[36mINFO[0m[0m 03/18 18:07:22 | last = 62.447%
[37m[36mINFO[0m[0m 03/18 18:07:22 | last (inD) = 88.397%
[37m[36mINFO[0m[0m 03/18 18:07:22 | training-domain validation (iid, inD) = 89.656%
[37m[36mINFO[0m[0m 03/18 18:07:22 | === Summary ===
[37m[36mINFO[0m[0m 03/18 18:07:22 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 1 --dataset VLCS
[37m[36mINFO[0m[0m 03/18 18:07:22 | Unique name: 250318_16-46-02_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 18:07:22 | Out path: train_output/VLCS/ERM/[1]/250318_16-46-02_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 18:07:22 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/18 18:07:22 | Dataset: VLCS
