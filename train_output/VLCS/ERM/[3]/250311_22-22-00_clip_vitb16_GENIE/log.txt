[37m[36mINFO[0m[0m 03/11 22:22:00 | Command :: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 0 --hparams_seed 17
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 17
	in_domain: False
	model_save: None
	mpa: False
	name: clip_vitb16_GENIE
	out_dir: train_output/VLCS/ERM/[3]/250311_22-22-00_clip_vitb16_GENIE
	out_root: train_output/VLCS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250311_22-22-00_clip_vitb16_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.00025159202447589334
	batch_size: 8
	weight_decay: 0.007781932002143384
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: openclip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/11 22:22:00 | n_steps = 5001
[37m[36mINFO[0m[0m 03/11 22:22:00 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/11 22:22:00 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/11 22:22:00 | 
[37m[36mINFO[0m[0m 03/11 22:22:00 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/11 22:22:00 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/11 22:22:00 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/11 22:22:00 | Batch sizes for each domain: [8, 8, 8, 0] (total=24)
[37m[36mINFO[0m[0m 03/11 22:22:00 | steps-per-epoch for each domain: 141.50, 265.62, 328.25 -> min = 141.50
[37m[36mINFO[0m[0m 03/11 22:22:03 | # of params = 86195205
[37m[36mINFO[0m[0m 03/11 22:24:14 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/11 22:24:14 | 0.246946    0.241481    0.335092    0.314932    1.451353    0.211131    0.173145    0.481882    0.457627    0.312262    0.314024    0.246946    0.241481    0           0.000000    1.675795    1.363788    130.239259 
[37m[36mINFO[0m[0m 03/11 22:27:24 | 0.437986    0.435556    0.500005    0.519513    1.187192    0.618375    0.628975    0.481412    0.510358    0.400228    0.419207    0.437986    0.435556    200         1.413428    1.254321    0.281941    133.467509 
[37m[36mINFO[0m[0m 03/11 22:30:31 | 0.437616    0.440000    0.541109    0.552808    1.155652    0.648410    0.671378    0.541176    0.548023    0.433740    0.439024    0.437616    0.440000    400         2.826855    1.186564    0.284960    130.156342 
[37m[36mINFO[0m[0m 03/11 22:33:40 | 0.437986    0.435556    0.550226    0.544839    1.065348    0.674912    0.692580    0.541647    0.495292    0.434120    0.446646    0.437986    0.435556    600         4.240283    1.171311    0.285893    131.348913 
[37m[36mINFO[0m[0m 03/11 22:36:45 | 0.449463    0.454815    0.607882    0.603406    0.955435    0.778269    0.773852    0.573176    0.566855    0.472201    0.469512    0.449463    0.454815    800         5.653710    1.067172    0.283971    128.860823 
[37m[36mINFO[0m[0m 03/11 22:39:54 | 0.460200    0.481481    0.650768    0.652497    0.850387    0.869258    0.890459    0.551059    0.551789    0.531988    0.515244    0.460200    0.481481    1000        7.067138    0.888648    0.277509    133.326792 
[37m[36mINFO[0m[0m 03/11 22:43:06 | 0.438356    0.441481    0.700143    0.690838    0.755019    0.932862    0.922261    0.618824    0.604520    0.548743    0.545732    0.438356    0.441481    1200        8.480565    0.824803    0.285558    135.034952 
[37m[36mINFO[0m[0m 03/11 22:46:18 | 0.482414    0.502222    0.731805    0.721459    0.677897    0.957597    0.943463    0.627765    0.623352    0.610053    0.597561    0.482414    0.502222    1400        9.893993    0.742253    0.293733    132.708830 
[37m[36mINFO[0m[0m 03/11 22:49:28 | 0.466864    0.485926    0.717464    0.691936    0.697382    0.911661    0.893993    0.631059    0.610169    0.609673    0.571646    0.466864    0.485926    1600        11.307420   0.694563    0.292975    131.253460 
[37m[36mINFO[0m[0m 03/11 22:52:34 | 0.536838    0.546667    0.751067    0.761447    0.611551    0.954064    0.932862    0.651765    0.700565    0.647372    0.650915    0.536838    0.546667    1800        12.720848   0.658722    0.285587    129.368628 
[37m[36mINFO[0m[0m 03/11 22:55:41 | 0.539060    0.577778    0.745987    0.744924    0.614355    0.955830    0.961131    0.640471    0.644068    0.641660    0.629573    0.539060    0.577778    2000        14.134276   0.636385    0.288996    129.113973 
[37m[36mINFO[0m[0m 03/11 22:58:51 | 0.533876    0.555556    0.750274    0.742717    0.645095    0.939046    0.918728    0.662118    0.679849    0.649657    0.629573    0.533876    0.555556    2200        15.547703   0.613962    0.295852    130.828221 
[37m[36mINFO[0m[0m 03/11 23:02:02 | 0.451314    0.475556    0.725662    0.718045    0.664401    0.939929    0.932862    0.627765    0.625235    0.609292    0.596037    0.451314    0.475556    2400        16.961131   0.609093    0.299439    131.250164 
[37m[36mINFO[0m[0m 03/11 23:05:09 | 0.569419    0.574815    0.776655    0.762094    0.599747    0.978799    0.946996    0.688941    0.700565    0.662224    0.638720    0.569419    0.574815    2600        18.374558   0.599067    0.286469    129.306913 
[37m[36mINFO[0m[0m 03/11 23:08:16 | 0.532025    0.537778    0.755710    0.747673    0.617787    0.972615    0.943463    0.631529    0.644068    0.662986    0.655488    0.532025    0.537778    2800        19.787986   0.631133    0.287675    129.859581 
[37m[36mINFO[0m[0m 03/11 23:11:26 | 0.524991    0.528889    0.772306    0.755033    0.608858    0.969965    0.936396    0.690824    0.693032    0.656131    0.635671    0.524991    0.528889    3000        21.201413   0.625194    0.286609    132.459748 
[37m[36mINFO[0m[0m 03/11 23:14:33 | 0.544613    0.560000    0.753881    0.734998    0.643277    0.950530    0.925795    0.628706    0.625235    0.682407    0.653963    0.544613    0.560000    3200        22.614841   0.602147    0.282413    130.419624 
[37m[36mINFO[0m[0m 03/11 23:17:44 | 0.516475    0.536296    0.776751    0.753639    0.608414    0.968198    0.939929    0.694118    0.668550    0.667936    0.652439    0.516475    0.536296    3400        24.028269   0.612843    0.287305    133.828707 
[37m[36mINFO[0m[0m 03/11 23:20:57 | 0.499815    0.530370    0.719803    0.721739    0.723532    0.956714    0.929329    0.569412    0.613936    0.633283    0.621951    0.499815    0.530370    3600        25.441696   0.598686    0.289969    135.195671 
[37m[36mINFO[0m[0m 03/11 23:24:09 | 0.522769    0.542222    0.753847    0.734657    0.675998    0.916961    0.904594    0.690353    0.659134    0.654227    0.640244    0.522769    0.542222    3800        26.855124   0.626695    0.287524    134.531808 
[37m[36mINFO[0m[0m 03/11 23:27:19 | 0.470196    0.496296    0.736966    0.709609    0.659968    0.968198    0.939929    0.625412    0.591337    0.617289    0.597561    0.470196    0.496296    4000        28.268551   0.623232    0.297396    130.483615 
[37m[36mINFO[0m[0m 03/11 23:30:25 | 0.486116    0.494815    0.758699    0.728800    0.667169    0.953180    0.922261    0.686588    0.674200    0.636329    0.589939    0.486116    0.494815    4200        29.681979   0.629997    0.274325    130.671855 
[37m[36mINFO[0m[0m 03/11 23:33:32 | 0.491670    0.503704    0.730757    0.723174    0.668763    0.948763    0.929329    0.629647    0.636535    0.613861    0.603659    0.491670    0.503704    4400        31.095406   0.616004    0.284012    130.725503 
[37m[36mINFO[0m[0m 03/11 23:36:41 | 0.532766    0.528889    0.761416    0.731658    0.654266    0.967314    0.918728    0.677176    0.681733    0.639756    0.594512    0.532766    0.528889    4600        32.508834   0.646518    0.280333    132.187105 
[37m[36mINFO[0m[0m 03/11 23:39:50 | 0.523140    0.537778    0.738551    0.728620    0.687261    0.949647    0.904594    0.651765    0.676083    0.614242    0.605183    0.523140    0.537778    4800        33.922261   0.625511    0.286482    132.121707 
[37m[36mINFO[0m[0m 03/11 23:43:02 | 0.510552    0.515556    0.730997    0.721494    0.689563    0.927562    0.901060    0.660706    0.670433    0.604722    0.592988    0.510552    0.515556    5000        35.335689   0.666625    0.288792    133.726947 
[37m[36mINFO[0m[0m 03/11 23:43:02 | Cumulative gradient change saved at train_output/VLCS/ERM/[3]/250311_22-22-00_clip_vitb16_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/11 23:43:05 | ---
[37m[36mINFO[0m[0m 03/11 23:43:05 | test-domain validation(oracle) = 53.906%
[37m[36mINFO[0m[0m 03/11 23:43:05 | training-domain validation(iid) = 56.942%
[37m[36mINFO[0m[0m 03/11 23:43:05 | last = 51.055%
[37m[36mINFO[0m[0m 03/11 23:43:05 | last (inD) = 72.149%
[37m[36mINFO[0m[0m 03/11 23:43:05 | training-domain validation (iid, inD) = 76.209%
[37m[36mINFO[0m[0m 03/11 23:43:06 | === Summary ===
[37m[36mINFO[0m[0m 03/11 23:43:06 | Command: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 0 --hparams_seed 17
[37m[36mINFO[0m[0m 03/11 23:43:06 | Unique name: 250311_22-22-00_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/11 23:43:06 | Out path: train_output/VLCS/ERM/[3]/250311_22-22-00_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/11 23:43:06 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/11 23:43:06 | Dataset: VLCS
