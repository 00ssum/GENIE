[37m[36mINFO[0m[0m 03/12 05:52:04 | Command :: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 1 --hparams_seed 5
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 5
	in_domain: False
	model_save: None
	mpa: False
	name: clip_vitb16_GENIE
	out_dir: train_output/VLCS/ERM/[3]/250312_05-52-04_clip_vitb16_GENIE
	out_root: train_output/VLCS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 1
	unique_name: 250312_05-52-04_clip_vitb16_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.0002591339485391279
	batch_size: 10
	weight_decay: 0.00020150290684242954
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: openclip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/12 05:52:04 | n_steps = 5001
[37m[36mINFO[0m[0m 03/12 05:52:04 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/12 05:52:04 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/12 05:52:04 | 
[37m[36mINFO[0m[0m 03/12 05:52:04 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/12 05:52:04 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/12 05:52:04 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/12 05:52:04 | Batch sizes for each domain: [10, 10, 10, 0] (total=30)
[37m[36mINFO[0m[0m 03/12 05:52:04 | steps-per-epoch for each domain: 113.20, 212.50, 262.60 -> min = 113.20
[37m[36mINFO[0m[0m 03/12 05:52:06 | # of params = 86195205
[37m[36mINFO[0m[0m 03/12 05:54:18 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/12 05:54:18 | 0.428360    0.388148    0.482335    0.497781    1.391496    0.609541    0.611307    0.473412    0.468927    0.364052    0.413110    0.428360    0.388148    0           0.000000    1.690544    1.203723    131.220945 
[37m[36mINFO[0m[0m 03/12 05:57:38 | 0.574602    0.560000    0.738145    0.719287    0.635336    0.908127    0.893993    0.715294    0.664783    0.591013    0.599085    0.574602    0.560000    200         1.766784    0.986648    0.354672    128.564917 
[37m[36mINFO[0m[0m 03/12 06:01:02 | 0.554609    0.543704    0.763555    0.750956    0.601062    0.970848    0.950530    0.666353    0.666667    0.653465    0.635671    0.554609    0.543704    400         3.533569    0.636895    0.369830    129.734571 
[37m[36mINFO[0m[0m 03/12 06:04:25 | 0.540170    0.573333    0.795584    0.785452    0.514529    0.969965    0.985866    0.719529    0.704331    0.697258    0.666159    0.540170    0.573333    600         5.300353    0.559813    0.361197    130.961894 
[37m[36mINFO[0m[0m 03/12 06:07:47 | 0.528323    0.565926    0.786031    0.768986    0.542475    0.956714    0.961131    0.668706    0.662900    0.732673    0.682927    0.528323    0.565926    800         7.067138    0.506145    0.360208    129.781097 
[37m[36mINFO[0m[0m 03/12 06:11:08 | 0.667901    0.662222    0.837381    0.822383    0.463077    0.984099    0.964664    0.763765    0.725047    0.764280    0.777439    0.667901    0.662222    1000        8.833922    0.470184    0.368093    127.975647 
[37m[36mINFO[0m[0m 03/12 06:14:26 | 0.651981    0.635556    0.831302    0.813017    0.464054    0.986749    0.978799    0.734118    0.719397    0.773039    0.740854    0.651981    0.635556    1200        10.600707   0.454714    0.341970    129.353632 
[37m[36mINFO[0m[0m 03/12 06:17:46 | 0.664569    0.635556    0.847175    0.813417    0.477717    0.995583    0.982332    0.763765    0.723164    0.782178    0.734756    0.664569    0.635556    1400        12.367491   0.431220    0.350952    129.408677 
[37m[36mINFO[0m[0m 03/12 06:21:08 | 0.655683    0.654815    0.871399    0.813853    0.461315    0.988516    0.978799    0.800471    0.700565    0.825209    0.762195    0.655683    0.654815    1600        14.134276   0.379876    0.353100    131.549927 
[37m[36mINFO[0m[0m 03/12 06:24:31 | 0.620881    0.614815    0.847835    0.813023    0.461548    0.994700    0.989399    0.754824    0.711864    0.793983    0.737805    0.620881    0.614815    1800        15.901060   0.382324    0.360464    131.434259 
[37m[36mINFO[0m[0m 03/12 06:27:52 | 0.600518    0.602963    0.853051    0.808274    0.504211    0.983216    0.946996    0.763294    0.715631    0.812643    0.762195    0.600518    0.602963    2000        17.667845   0.386618    0.355508    129.637459 
[37m[36mINFO[0m[0m 03/12 06:31:10 | 0.635690    0.626667    0.877911    0.814942    0.462953    0.994700    0.982332    0.806588    0.723164    0.832445    0.739329    0.635690    0.626667    2200        19.434629   0.349205    0.350991    128.129449 
[37m[36mINFO[0m[0m 03/12 06:34:34 | 0.601999    0.641481    0.878111    0.818750    0.472111    0.993816    0.985866    0.818353    0.732580    0.822163    0.737805    0.601999    0.641481    2400        21.201413   0.330250    0.363090    131.060953 
[37m[36mINFO[0m[0m 03/12 06:37:56 | 0.587930    0.600000    0.866806    0.791669    0.526563    0.984099    0.968198    0.803294    0.726930    0.813024    0.679878    0.587930    0.600000    2600        22.968198   0.323439    0.365033    128.859102 
[37m[36mINFO[0m[0m 03/12 06:41:18 | 0.601259    0.591111    0.877581    0.808462    0.520358    0.992049    0.975265    0.811294    0.738230    0.829398    0.711890    0.601259    0.591111    2800        24.734982   0.324406    0.358782    130.278927 
[37m[36mINFO[0m[0m 03/12 06:44:40 | 0.623103    0.617778    0.876367    0.806852    0.504516    0.984982    0.968198    0.811294    0.709981    0.832826    0.742378    0.623103    0.617778    3000        26.501767   0.294550    0.356159    130.920841 
[37m[36mINFO[0m[0m 03/12 06:48:05 | 0.631988    0.613333    0.891148    0.806237    0.549896    0.989399    0.971731    0.832941    0.713748    0.851104    0.733232    0.631988    0.613333    3200        28.268551   0.276677    0.371630    130.941669 
[37m[36mINFO[0m[0m 03/12 06:51:25 | 0.619770    0.608889    0.892019    0.819145    0.522384    0.996466    0.978799    0.839529    0.743879    0.840061    0.734756    0.619770    0.608889    3400        30.035336   0.280797    0.349812    129.278199 
[37m[36mINFO[0m[0m 03/12 06:54:47 | 0.617179    0.605926    0.908399    0.825721    0.498165    0.995583    0.978799    0.852235    0.751412    0.877380    0.746951    0.617179    0.605926    3600        31.802120   0.271746    0.360647    130.468126 
[37m[36mINFO[0m[0m 03/12 06:58:08 | 0.596446    0.583704    0.899764    0.821726    0.547180    0.995583    0.982332    0.820235    0.709981    0.883473    0.772866    0.596446    0.583704    3800        33.568905   0.243209    0.350364    130.743127 
[37m[36mINFO[0m[0m 03/12 07:01:28 | 0.633839    0.610370    0.914265    0.816371    0.564840    0.998233    0.989399    0.868706    0.732580    0.875857    0.727134    0.633839    0.610370    4000        35.335689   0.238126    0.356623    128.696366 
[37m[36mINFO[0m[0m 03/12 07:04:48 | 0.587560    0.588148    0.908942    0.806428    0.550081    0.992933    0.957597    0.852706    0.726930    0.881188    0.734756    0.587560    0.588148    4200        37.102473   0.219485    0.357674    127.933947 
[37m[36mINFO[0m[0m 03/12 07:08:08 | 0.640504    0.611852    0.918389    0.808526    0.632545    0.994700    0.968198    0.868235    0.704331    0.892232    0.753049    0.640504    0.611852    4400        38.869258   0.209290    0.356999    128.558777 
[37m[36mINFO[0m[0m 03/12 07:11:29 | 0.578675    0.592593    0.936029    0.822104    0.567238    0.996466    0.978799    0.906824    0.734463    0.904798    0.753049    0.578675    0.592593    4600        40.636042   0.203143    0.351943    131.287276 
[37m[36mINFO[0m[0m 03/12 07:14:55 | 0.554239    0.561481    0.938121    0.803733    0.617199    0.994700    0.982332    0.903059    0.730697    0.916603    0.698171    0.554239    0.561481    4800        42.402827   0.186879    0.367489    132.228607 
[37m[36mINFO[0m[0m 03/12 07:18:21 | 0.602369    0.597037    0.948532    0.807283    0.576878    0.999117    0.971731    0.913882    0.706215    0.932597    0.743902    0.602369    0.597037    5000        44.169611   0.186409    0.350212    135.853818 
[37m[36mINFO[0m[0m 03/12 07:18:21 | Cumulative gradient change saved at train_output/VLCS/ERM/[3]/250312_05-52-04_clip_vitb16_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/12 07:18:24 | ---
[37m[36mINFO[0m[0m 03/12 07:18:24 | test-domain validation(oracle) = 66.790%
[37m[36mINFO[0m[0m 03/12 07:18:24 | training-domain validation(iid) = 61.718%
[37m[36mINFO[0m[0m 03/12 07:18:24 | last = 60.237%
[37m[36mINFO[0m[0m 03/12 07:18:24 | last (inD) = 80.728%
[37m[36mINFO[0m[0m 03/12 07:18:24 | training-domain validation (iid, inD) = 82.572%
[37m[36mINFO[0m[0m 03/12 07:18:24 | === Summary ===
[37m[36mINFO[0m[0m 03/12 07:18:24 | Command: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 1 --hparams_seed 5
[37m[36mINFO[0m[0m 03/12 07:18:24 | Unique name: 250312_05-52-04_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/12 07:18:24 | Out path: train_output/VLCS/ERM/[3]/250312_05-52-04_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/12 07:18:24 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/12 07:18:24 | Dataset: VLCS
