[37m[36mINFO[0m[0m 03/28 00:07:15 | Command :: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: GENIE_sharpness
	out_dir: train_output/VLCS/ERM/[3]/250328_00-07-15_GENIE_sharpness
	out_root: train_output/VLCS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250328_00-07-15_GENIE_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/28 00:07:15 | n_steps = 5001
[37m[36mINFO[0m[0m 03/28 00:07:15 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/28 00:07:15 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/28 00:07:15 | 
[37m[36mINFO[0m[0m 03/28 00:07:15 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/28 00:07:15 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/28 00:07:15 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/28 00:07:15 | Batch sizes for each domain: [32, 32, 32, 0] (total=96)
[37m[36mINFO[0m[0m 03/28 00:07:15 | steps-per-epoch for each domain: 35.38, 66.41, 82.06 -> min = 35.38
[37m[36mINFO[0m[0m 03/28 00:07:16 | # of params = 23518277
[37m[36mINFO[0m[0m 03/28 00:09:27 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/28 00:09:27 | 0.445761    0.445926    0.484889    0.500921    1.210749    0.612191    0.628975    0.459765    0.489642    0.382711    0.384146    0.445761    0.445926    0           0.000000    1.809193    1.511571    129.403085 
[37m[36mINFO[0m[0m 03/28 00:15:34 | 0.785265    0.792593    0.865185    0.852874    0.383587    1.000000    1.000000    0.754353    0.747646    0.841203    0.810976    0.785265    0.792593    200         5.653710    0.448231    1.076991    129.566397 
[37m[36mINFO[0m[0m 03/28 00:21:36 | 0.743799    0.746667    0.860949    0.833685    0.435319    0.998233    1.000000    0.786824    0.749529    0.797791    0.751524    0.743799    0.746667    400         11.307420   0.327415    1.067167    126.753167 
[37m[36mINFO[0m[0m 03/28 00:27:48 | 0.779711    0.791111    0.906129    0.856432    0.379956    1.000000    1.000000    0.835294    0.779661    0.883092    0.789634    0.779711    0.791111    600         16.961131   0.295148    1.097392    130.933706 
[37m[36mINFO[0m[0m 03/28 00:34:05 | 0.771936    0.767407    0.922749    0.858284    0.412564    0.999117    1.000000    0.851765    0.760829    0.917365    0.814024    0.771936    0.767407    800         22.614841   0.237763    1.113656    132.701336 
[37m[36mINFO[0m[0m 03/28 00:40:23 | 0.781933    0.776296    0.932195    0.852127    0.462369    0.998233    1.000000    0.885176    0.775895    0.913176    0.780488    0.781933    0.776296    1000        28.268551   0.206854    1.115285    133.375620 
[37m[36mINFO[0m[0m 03/28 00:46:33 | 0.773787    0.768889    0.950985    0.853801    0.481382    0.998233    1.000000    0.909176    0.770245    0.945545    0.791159    0.773787    0.768889    1200        33.922261   0.167094    1.104936    127.103841 
[37m[36mINFO[0m[0m 03/28 00:52:52 | 0.756757    0.761481    0.954995    0.860049    0.503228    1.000000    1.000000    0.919059    0.796610    0.945925    0.783537    0.756757    0.761481    1400        39.575972   0.152058    1.133542    131.008633 
[37m[36mINFO[0m[0m 03/28 00:59:09 | 0.742688    0.734815    0.953875    0.851380    0.570819    1.000000    1.000000    0.921412    0.772128    0.940213    0.782012    0.742688    0.734815    1600        45.229682   0.122812    1.115396    131.461044 
[37m[36mINFO[0m[0m 03/28 01:05:33 | 0.760089    0.742222    0.966180    0.850334    0.531225    0.999117    1.000000    0.945882    0.779661    0.953542    0.771341    0.760089    0.742222    1800        50.883392   0.102822    1.135436    135.008710 
[37m[36mINFO[0m[0m 03/28 01:11:50 | 0.734543    0.724444    0.939303    0.812102    0.834200    0.998233    1.000000    0.877176    0.681733    0.942498    0.754573    0.734543    0.724444    2000        56.537102   0.081858    1.120795    131.328207 
[37m[36mINFO[0m[0m 03/28 01:18:09 | 0.764161    0.770370    0.968782    0.856731    0.706176    1.000000    1.000000    0.948235    0.800377    0.958111    0.769817    0.764161    0.770370    2200        62.190813   0.082019    1.112995    135.507159 
[37m[36mINFO[0m[0m 03/28 01:24:24 | 0.716031    0.725926    0.978161    0.845670    0.699371    1.000000    1.000000    0.952000    0.738230    0.982483    0.798780    0.716031    0.725926    2400        67.844523   0.062457    1.109422    130.897753 
[37m[36mINFO[0m[0m 03/28 01:30:33 | 0.740466    0.736296    0.982403    0.823221    0.817714    1.000000    1.000000    0.962824    0.696798    0.984387    0.772866    0.740466    0.736296    2600        73.498233   0.053762    1.109098    126.141868 
[37m[36mINFO[0m[0m 03/28 01:36:50 | 0.736394    0.733333    0.963201    0.830844    0.879398    1.000000    1.000000    0.922353    0.728814    0.967251    0.763720    0.736394    0.733333    2800        79.151943   0.061510    1.117164    131.348156 
[37m[36mINFO[0m[0m 03/28 01:43:06 | 0.754165    0.762963    0.977213    0.818905    0.792116    1.000000    0.996466    0.951059    0.687382    0.980579    0.772866    0.754165    0.762963    3000        84.805654   0.044590    1.118964    130.335311 
[37m[36mINFO[0m[0m 03/28 01:49:17 | 0.752314    0.760000    0.985590    0.839423    0.845539    0.999117    1.000000    0.973647    0.743879    0.984006    0.774390    0.752314    0.760000    3200        90.459364   0.037915    1.099535    128.894440 
[37m[36mINFO[0m[0m 03/28 01:55:36 | 0.761200    0.776296    0.985757    0.836213    0.841853    1.000000    0.996466    0.973647    0.743879    0.983625    0.768293    0.761200    0.776296    3400        96.113074   0.037789    1.116654    134.506950 
[37m[36mINFO[0m[0m 03/28 02:01:49 | 0.753795    0.774815    0.983023    0.849616    0.946598    1.000000    1.000000    0.958588    0.736347    0.990480    0.812500    0.753795    0.774815    3600        101.766784  0.033761    1.102883    131.278676 
[37m[36mINFO[0m[0m 03/28 02:08:02 | 0.722325    0.737778    0.983606    0.843482    0.889735    1.000000    0.989399    0.963765    0.751412    0.987053    0.789634    0.722325    0.737778    3800        107.420495  0.032973    1.098704    130.653887 
[37m[36mINFO[0m[0m 03/28 02:14:19 | 0.750093    0.743704    0.979107    0.831113    0.907275    0.999117    1.000000    0.959529    0.725047    0.978675    0.768293    0.750093    0.743704    4000        113.074205  0.030653    1.131094    128.945138 
[37m[36mINFO[0m[0m 03/28 02:20:38 | 0.749722    0.730370    0.985989    0.834641    0.934369    1.000000    1.000000    0.971294    0.732580    0.986672    0.771341    0.749722    0.730370    4200        118.727915  0.024711    1.116076    133.917895 
[37m[36mINFO[0m[0m 03/28 02:26:50 | 0.728619    0.709630    0.996594    0.836572    0.798020    1.000000    0.996466    0.994353    0.749529    0.995430    0.763720    0.728619    0.709630    4400        124.381625  0.029230    1.106322    129.440720 
[37m[36mINFO[0m[0m 03/28 02:33:03 | 0.748982    0.746667    0.978850    0.843699    1.177418    1.000000    1.000000    0.970824    0.787194    0.965727    0.743902    0.748982    0.746667    4600        130.035336  0.020160    1.112805    128.821248 
[37m[36mINFO[0m[0m 03/28 02:39:14 | 0.734543    0.722963    0.995019    0.849557    0.882777    1.000000    1.000000    0.991529    0.783427    0.993526    0.765244    0.734543    0.722963    4800        135.689046  0.031386    1.094477    129.951864 
[37m[36mINFO[0m[0m 03/28 02:45:26 | 0.734543    0.740741    0.996878    0.855475    0.836990    1.000000    1.000000    0.994824    0.764595    0.995811    0.801829    0.734543    0.740741    5000        141.342756  0.026042    1.103682    130.378343 
[37m[36mINFO[0m[0m 03/28 02:45:48 | Cumulative gradient change saved at train_output/VLCS/ERM/[3]/250328_00-07-15_GENIE_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/28 02:45:50 | ---
[37m[36mINFO[0m[0m 03/28 02:45:50 | test-domain validation(oracle) = 78.526%
[37m[36mINFO[0m[0m 03/28 02:45:50 | training-domain validation(iid) = 75.676%
[37m[36mINFO[0m[0m 03/28 02:45:50 | last = 73.454%
[37m[36mINFO[0m[0m 03/28 02:45:50 | last (inD) = 85.547%
[37m[36mINFO[0m[0m 03/28 02:45:50 | training-domain validation (iid, inD) = 86.005%
[37m[36mINFO[0m[0m 03/28 02:45:50 | === Summary ===
[37m[36mINFO[0m[0m 03/28 02:45:50 | Command: /jsm0707/GENIE/train_all.py GENIE_sharpness config/resnet50_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS
[37m[36mINFO[0m[0m 03/28 02:45:50 | Unique name: 250328_00-07-15_GENIE_sharpness
[37m[36mINFO[0m[0m 03/28 02:45:50 | Out path: train_output/VLCS/ERM/[3]/250328_00-07-15_GENIE_sharpness
[37m[36mINFO[0m[0m 03/28 02:45:50 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/28 02:45:50 | Dataset: VLCS
