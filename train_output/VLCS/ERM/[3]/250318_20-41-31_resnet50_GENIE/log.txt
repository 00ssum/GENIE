[37m[36mINFO[0m[0m 03/18 20:41:31 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/ERM/[3]/250318_20-41-31_resnet50_GENIE
	out_root: train_output/VLCS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250318_20-41-31_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/18 20:41:31 | n_steps = 5001
[37m[36mINFO[0m[0m 03/18 20:41:31 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/18 20:41:31 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/18 20:41:31 | 
[37m[36mINFO[0m[0m 03/18 20:41:31 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/18 20:41:31 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/18 20:41:31 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/18 20:41:31 | Batch sizes for each domain: [32, 32, 32, 0] (total=96)
[37m[36mINFO[0m[0m 03/18 20:41:31 | steps-per-epoch for each domain: 35.38, 66.41, 82.06 -> min = 35.38
[37m[36mINFO[0m[0m 03/18 20:41:32 | # of params = 23518277
[37m[36mINFO[0m[0m 03/18 20:43:44 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/18 20:43:44 | 0.445761    0.445926    0.484889    0.500921    1.210749    0.612191    0.628975    0.459765    0.489642    0.382711    0.384146    0.445761    0.445926    0           0.000000    1.809193    1.409879    130.506957 
[37m[36mINFO[0m[0m 03/18 20:49:36 | 0.794150    0.798519    0.869068    0.853323    0.378630    0.999117    1.000000    0.782118    0.762712    0.825971    0.797256    0.794150    0.798519    200         5.653710    0.449444    1.105646    130.073306 
[37m[36mINFO[0m[0m 03/18 20:55:33 | 0.790078    0.773333    0.879481    0.856162    0.375910    1.000000    1.000000    0.783529    0.751412    0.854912    0.817073    0.790078    0.773333    400         11.307420   0.323044    1.121723    132.930094 
[37m[36mINFO[0m[0m 03/18 21:01:19 | 0.768234    0.754074    0.901667    0.856043    0.409198    0.997350    1.000000    0.824941    0.781544    0.882711    0.786585    0.768234    0.754074    600         16.961131   0.278608    1.086505    128.736181 
[37m[36mINFO[0m[0m 03/18 21:07:05 | 0.767494    0.773333    0.919273    0.851649    0.445875    1.000000    1.000000    0.858353    0.768362    0.899467    0.786585    0.767494    0.773333    800         22.614841   0.238184    1.087631    128.178890 
[37m[36mINFO[0m[0m 03/18 21:12:56 | 0.713439    0.721481    0.921385    0.834629    0.490904    0.999117    0.996466    0.876235    0.758945    0.888804    0.748476    0.713439    0.721481    1000        28.268551   0.206726    1.106562    129.960615 
[37m[36mINFO[0m[0m 03/18 21:18:49 | 0.728989    0.740741    0.952859    0.852815    0.449689    0.998233    1.000000    0.908706    0.762712    0.951637    0.795732    0.728989    0.740741    1200        33.922261   0.174839    1.092783    134.684381 
[37m[36mINFO[0m[0m 03/18 21:24:39 | 0.714180    0.725926    0.949069    0.839561    0.533388    0.999117    0.996466    0.906353    0.764595    0.941736    0.757622    0.714180    0.725926    1400        39.575972   0.153913    1.103382    129.615952 
[37m[36mINFO[0m[0m 03/18 21:30:37 | 0.741947    0.745185    0.962559    0.848062    0.633829    1.000000    1.000000    0.924235    0.775895    0.963442    0.768293    0.741947    0.745185    1600        45.229682   0.118721    1.111759    135.536952 
[37m[36mINFO[0m[0m 03/18 21:36:30 | 0.739356    0.746667    0.932602    0.822660    0.679219    1.000000    0.992933    0.858353    0.725047    0.939452    0.750000    0.739356    0.746667    1800        50.883392   0.104642    1.095126    133.468075 
[37m[36mINFO[0m[0m 03/18 21:42:16 | 0.786005    0.791111    0.977862    0.850979    0.646014    0.997350    0.996466    0.965176    0.768362    0.971059    0.788110    0.786005    0.791111    2000        56.537102   0.081244    1.067239    132.483330 
[37m[36mINFO[0m[0m 03/18 21:48:09 | 0.750833    0.739259    0.913677    0.828861    0.639001    0.996466    0.996466    0.876706    0.772128    0.867860    0.717988    0.750833    0.739259    2200        62.190813   0.083024    1.107064    131.586915 
[37m[36mINFO[0m[0m 03/18 21:53:57 | 0.745650    0.737778    0.984130    0.842579    0.770619    1.000000    0.996466    0.976000    0.740113    0.976390    0.791159    0.745650    0.737778    2400        67.844523   0.077591    1.093227    129.972676 
[37m[36mINFO[0m[0m 03/18 21:59:48 | 0.743058    0.751111    0.966337    0.841068    0.930298    0.999117    1.000000    0.946353    0.777778    0.953542    0.745427    0.743058    0.751111    2600        73.498233   0.059333    1.099220    130.522568 
[37m[36mINFO[0m[0m 03/18 22:05:40 | 0.754165    0.760000    0.971881    0.847631    0.815239    0.996466    0.996466    0.933647    0.747646    0.985529    0.798780    0.754165    0.760000    2800        79.151943   0.049696    1.095996    132.964833 
[37m[36mINFO[0m[0m 03/18 22:11:29 | 0.721585    0.737778    0.976839    0.839812    0.794199    1.000000    1.000000    0.949176    0.741996    0.981340    0.777439    0.721585    0.737778    3000        84.805654   0.045158    1.098962    128.929298 
[37m[36mINFO[0m[0m 03/18 22:17:13 | 0.666420    0.674074    0.958854    0.816024    0.900802    0.999117    0.992933    0.936471    0.700565    0.940975    0.754573    0.666420    0.674074    3200        90.459364   0.036321    1.078830    128.607518 
[37m[36mINFO[0m[0m 03/18 22:23:00 | 0.730840    0.736296    0.993226    0.852067    0.801633    1.000000    1.000000    0.987294    0.758945    0.992384    0.797256    0.730840    0.736296    3400        96.113074   0.040230    1.080235    130.823031 
[37m[36mINFO[0m[0m 03/18 22:28:52 | 0.743428    0.767407    0.993629    0.849885    0.726962    1.000000    1.000000    0.989647    0.764595    0.991241    0.785061    0.743428    0.767407    3600        101.766784  0.041647    1.111842    130.095393 
[37m[36mINFO[0m[0m 03/18 22:34:46 | 0.760829    0.752593    0.995997    0.851308    0.861846    1.000000    0.996466    0.992941    0.781544    0.995050    0.775915    0.760829    0.752593    3800        107.420495  0.025786    1.107616    132.298823 
[37m[36mINFO[0m[0m 03/18 22:40:38 | 0.778971    0.768889    0.985377    0.857496    0.927117    1.000000    0.996466    0.973647    0.790960    0.982483    0.785061    0.778971    0.768889    4000        113.074205  0.028650    1.097255    132.355319 
[37m[36mINFO[0m[0m 03/18 22:46:38 | 0.774158    0.767407    0.995639    0.856001    0.927025    1.000000    0.996466    0.995294    0.783427    0.991622    0.788110    0.774158    0.767407    4200        118.727915  0.028329    1.120207    136.146621 
[37m[36mINFO[0m[0m 03/18 22:52:38 | 0.721585    0.721481    0.982808    0.831263    1.053526    1.000000    1.000000    0.973176    0.751412    0.975248    0.742378    0.721585    0.721481    4400        124.381625  0.022345    1.122378    135.459958 
[37m[36mINFO[0m[0m 03/18 22:58:36 | 0.717512    0.739259    0.992434    0.834766    0.948582    1.000000    0.992933    0.986824    0.747646    0.990480    0.763720    0.717512    0.739259    4600        130.035336  0.025273    1.112120    135.328447 
[37m[36mINFO[0m[0m 03/18 23:04:34 | 0.743428    0.748148    0.996139    0.847662    0.912259    1.000000    0.996466    0.997176    0.772128    0.991241    0.774390    0.743428    0.748148    4800        135.689046  0.024308    1.118614    134.625075 
[37m[36mINFO[0m[0m 03/18 23:10:30 | 0.724917    0.731852    0.996005    0.843763    1.011844    0.998233    0.992933    0.994353    0.753296    0.995430    0.785061    0.724917    0.731852    5000        141.342756  0.013037    1.109807    133.801024 
[37m[36mINFO[0m[0m 03/18 23:10:30 | Cumulative gradient change saved at train_output/VLCS/ERM/[3]/250318_20-41-31_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/18 23:10:32 | ---
[37m[36mINFO[0m[0m 03/18 23:10:32 | test-domain validation(oracle) = 79.415%
[37m[36mINFO[0m[0m 03/18 23:10:32 | training-domain validation(iid) = 77.897%
[37m[36mINFO[0m[0m 03/18 23:10:32 | last = 72.492%
[37m[36mINFO[0m[0m 03/18 23:10:32 | last (inD) = 84.376%
[37m[36mINFO[0m[0m 03/18 23:10:32 | training-domain validation (iid, inD) = 85.750%
[37m[36mINFO[0m[0m 03/18 23:10:32 | === Summary ===
[37m[36mINFO[0m[0m 03/18 23:10:32 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS
[37m[36mINFO[0m[0m 03/18 23:10:32 | Unique name: 250318_20-41-31_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 23:10:32 | Out path: train_output/VLCS/ERM/[3]/250318_20-41-31_resnet50_GENIE
[37m[36mINFO[0m[0m 03/18 23:10:32 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/18 23:10:32 | Dataset: VLCS
