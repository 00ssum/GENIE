[37m[36mINFO[0m[0m 03/12 08:46:47 | Command :: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 1 --hparams_seed 11
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 11
	in_domain: False
	model_save: None
	mpa: False
	name: clip_vitb16_GENIE
	out_dir: train_output/VLCS/ERM/[3]/250312_08-46-47_clip_vitb16_GENIE
	out_root: train_output/VLCS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 1
	unique_name: 250312_08-46-47_clip_vitb16_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5.340583028754034e-05
	batch_size: 10
	weight_decay: 2.5000172438864113e-05
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: openclip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/12 08:46:47 | n_steps = 5001
[37m[36mINFO[0m[0m 03/12 08:46:47 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/12 08:46:47 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/12 08:46:47 | 
[37m[36mINFO[0m[0m 03/12 08:46:47 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/12 08:46:47 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/12 08:46:47 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/12 08:46:47 | Batch sizes for each domain: [10, 10, 10, 0] (total=30)
[37m[36mINFO[0m[0m 03/12 08:46:47 | steps-per-epoch for each domain: 113.20, 212.50, 262.60 -> min = 113.20
[37m[36mINFO[0m[0m 03/12 08:46:50 | # of params = 86195205
[37m[36mINFO[0m[0m 03/12 08:49:02 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/12 08:49:02 | 0.412810    0.380741    0.476062    0.488705    1.407694    0.590989    0.579505    0.480000    0.468927    0.357197    0.417683    0.412810    0.380741    0           0.000000    1.688094    1.151388    131.069797 
[37m[36mINFO[0m[0m 03/12 08:52:22 | 0.530544    0.503704    0.739338    0.738963    0.636450    0.944346    0.957597    0.648000    0.632768    0.625666    0.626524    0.530544    0.503704    200         1.766784    0.903388    0.347745    130.191832 
[37m[36mINFO[0m[0m 03/12 08:55:44 | 0.538319    0.539259    0.762130    0.768981    0.561858    0.969081    0.968198    0.661176    0.681733    0.656131    0.657012    0.538319    0.539259    400         3.533569    0.643573    0.358047    130.093370 
[37m[36mINFO[0m[0m 03/12 08:59:05 | 0.591633    0.595556    0.801506    0.801103    0.508065    0.972615    0.982332    0.704941    0.721281    0.726961    0.699695    0.591633    0.595556    600         5.300353    0.556017    0.352230    131.046061 
[37m[36mINFO[0m[0m 03/12 09:02:27 | 0.654202    0.634074    0.833324    0.811325    0.464323    0.992049    0.982332    0.742118    0.706215    0.765804    0.745427    0.654202    0.634074    800         7.067138    0.503504    0.359080    130.285441 
[37m[36mINFO[0m[0m 03/12 09:05:53 | 0.673454    0.666667    0.839914    0.823233    0.458812    0.990283    0.985866    0.773176    0.723164    0.756283    0.760671    0.673454    0.666667    1000        8.833922    0.448817    0.366296    132.446864 
[37m[36mINFO[0m[0m 03/12 09:09:13 | 0.637171    0.628148    0.807813    0.799893    0.509274    0.986749    0.961131    0.686118    0.685499    0.750571    0.753049    0.637171    0.628148    1200        10.600707   0.445293    0.348157    129.740745 
[37m[36mINFO[0m[0m 03/12 09:12:35 | 0.639023    0.641481    0.852992    0.832339    0.451058    0.989399    0.982332    0.769882    0.757062    0.799695    0.757622    0.639023    0.641481    1400        12.367491   0.438790    0.358118    130.913466 
[37m[36mINFO[0m[0m 03/12 09:15:58 | 0.634950    0.660741    0.845896    0.808832    0.471989    0.991166    0.978799    0.754824    0.717514    0.791698    0.730183    0.634950    0.660741    1600        14.134276   0.401526    0.362431    130.033803 
[37m[36mINFO[0m[0m 03/12 09:19:20 | 0.585709    0.577778    0.837895    0.796261    0.500475    0.985866    0.982332    0.758588    0.725047    0.769231    0.681402    0.585709    0.577778    1800        15.901060   0.383192    0.359442    130.735609 
[37m[36mINFO[0m[0m 03/12 09:22:43 | 0.644946    0.631111    0.878691    0.836391    0.444987    0.993816    0.978799    0.799529    0.751412    0.842727    0.778963    0.644946    0.631111    2000        17.667845   0.380991    0.361715    130.611311 
[37m[36mINFO[0m[0m 03/12 09:26:07 | 0.603850    0.579259    0.862435    0.801025    0.516296    0.993816    0.985866    0.793412    0.717514    0.800076    0.699695    0.603850    0.579259    2200        19.434629   0.345522    0.353515    133.232579 
[37m[36mINFO[0m[0m 03/12 09:29:33 | 0.647538    0.642963    0.882329    0.823689    0.476482    1.000000    0.996466    0.810353    0.730697    0.836634    0.743902    0.647538    0.642963    2400        21.201413   0.342934    0.367412    132.520349 
[37m[36mINFO[0m[0m 03/12 09:33:01 | 0.669011    0.668148    0.899699    0.837588    0.443259    0.995583    0.978799    0.837176    0.770245    0.866337    0.763720    0.669011    0.668148    2600        22.968198   0.341737    0.370117    133.757112 
[37m[36mINFO[0m[0m 03/12 09:36:25 | 0.626064    0.616296    0.871083    0.821883    0.510178    0.992049    0.975265    0.800941    0.749529    0.820259    0.740854    0.626064    0.616296    2800        24.734982   0.304533    0.363162    131.806429 
[37m[36mINFO[0m[0m 03/12 09:39:46 | 0.640874    0.613333    0.883660    0.813489    0.528424    0.989399    0.968198    0.840941    0.734463    0.820640    0.737805    0.640874    0.613333    3000        26.501767   0.294413    0.360469    128.588509 
[37m[36mINFO[0m[0m 03/12 09:43:08 | 0.620141    0.611852    0.896154    0.797910    0.562097    0.993816    0.975265    0.846588    0.708098    0.848058    0.710366    0.620141    0.611852    3200        28.268551   0.277716    0.364206    128.941995 
[37m[36mINFO[0m[0m 03/12 09:46:27 | 0.667531    0.638519    0.900974    0.833456    0.516680    0.997350    0.985866    0.832000    0.740113    0.873572    0.774390    0.667531    0.638519    3400        30.035336   0.258640    0.356489    127.549275 
[37m[36mINFO[0m[0m 03/12 09:49:51 | 0.594965    0.608889    0.919069    0.819934    0.495343    0.994700    0.982332    0.872941    0.745763    0.889566    0.731707    0.594965    0.608889    3600        31.802120   0.252355    0.370550    130.125147 
[37m[36mINFO[0m[0m 03/12 09:53:16 | 0.604591    0.573333    0.921022    0.816215    0.512402    0.992049    0.978799    0.869647    0.713748    0.901371    0.756098    0.604591    0.573333    3800        33.568905   0.240226    0.362504    133.028308 
[37m[36mINFO[0m[0m 03/12 09:56:40 | 0.637912    0.637037    0.931830    0.838461    0.523054    0.998233    0.989399    0.889412    0.768362    0.907845    0.757622    0.637912    0.637037    4000        35.335689   0.218106    0.367577    130.015187 
[37m[36mINFO[0m[0m 03/12 10:00:02 | 0.625324    0.617778    0.914751    0.809292    0.566952    0.988516    0.982332    0.857412    0.706215    0.898324    0.739329    0.625324    0.617778    4200        37.102473   0.199840    0.361801    129.433430 
[37m[36mINFO[0m[0m 03/12 10:03:23 | 0.616438    0.620741    0.923489    0.818631    0.593590    0.996466    0.985866    0.875294    0.730697    0.898705    0.739329    0.616438    0.620741    4400        38.869258   0.194124    0.367397    127.928342 
[37m[36mINFO[0m[0m 03/12 10:06:40 | 0.634210    0.632593    0.932149    0.825457    0.600548    0.998233    0.989399    0.872471    0.715631    0.925743    0.771341    0.634210    0.632593    4600        40.636042   0.177710    0.344955    127.938735 
[37m[36mINFO[0m[0m 03/12 10:10:02 | 0.613106    0.600000    0.941870    0.806883    0.619681    0.999117    0.985866    0.904941    0.713748    0.921554    0.721037    0.613106    0.600000    4800        42.402827   0.169906    0.366994    128.254689 
[37m[36mINFO[0m[0m 03/12 10:13:23 | 0.528323    0.539259    0.942244    0.815080    0.607044    0.999117    0.978799    0.906824    0.743879    0.920792    0.722561    0.528323    0.539259    5000        44.169611   0.172383    0.356696    130.040854 
[37m[36mINFO[0m[0m 03/12 10:13:24 | Cumulative gradient change saved at train_output/VLCS/ERM/[3]/250312_08-46-47_clip_vitb16_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/12 10:13:26 | ---
[37m[36mINFO[0m[0m 03/12 10:13:26 | test-domain validation(oracle) = 66.901%
[37m[36mINFO[0m[0m 03/12 10:13:26 | training-domain validation(iid) = 63.791%
[37m[36mINFO[0m[0m 03/12 10:13:26 | last = 52.832%
[37m[36mINFO[0m[0m 03/12 10:13:26 | last (inD) = 81.508%
[37m[36mINFO[0m[0m 03/12 10:13:26 | training-domain validation (iid, inD) = 83.846%
[37m[36mINFO[0m[0m 03/12 10:13:26 | === Summary ===
[37m[36mINFO[0m[0m 03/12 10:13:26 | Command: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 1 --hparams_seed 11
[37m[36mINFO[0m[0m 03/12 10:13:26 | Unique name: 250312_08-46-47_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/12 10:13:26 | Out path: train_output/VLCS/ERM/[3]/250312_08-46-47_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/12 10:13:26 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/12 10:13:26 | Dataset: VLCS
