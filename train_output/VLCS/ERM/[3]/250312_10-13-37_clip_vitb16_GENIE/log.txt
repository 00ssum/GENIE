[37m[36mINFO[0m[0m 03/12 10:13:37 | Command :: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 1 --hparams_seed 13
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 13
	in_domain: False
	model_save: None
	mpa: False
	name: clip_vitb16_GENIE
	out_dir: train_output/VLCS/ERM/[3]/250312_10-13-37_clip_vitb16_GENIE
	out_root: train_output/VLCS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 1
	unique_name: 250312_10-13-37_clip_vitb16_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 6.984990754803203e-05
	batch_size: 13
	weight_decay: 2.888048423418205e-06
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: openclip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/12 10:13:37 | n_steps = 5001
[37m[36mINFO[0m[0m 03/12 10:13:37 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/12 10:13:37 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/12 10:13:37 | 
[37m[36mINFO[0m[0m 03/12 10:13:37 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/12 10:13:37 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/12 10:13:37 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/12 10:13:37 | Batch sizes for each domain: [13, 13, 13, 0] (total=39)
[37m[36mINFO[0m[0m 03/12 10:13:37 | steps-per-epoch for each domain: 87.08, 163.46, 202.00 -> min = 87.08
[37m[36mINFO[0m[0m 03/12 10:13:40 | # of params = 86195205
[37m[36mINFO[0m[0m 03/12 10:15:50 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/12 10:15:50 | 0.379859    0.348148    0.460357    0.474967    1.428351    0.553887    0.565371    0.481412    0.470810    0.345773    0.388720    0.379859    0.348148    0           0.000000    1.698044    1.100606    129.097086 
[37m[36mINFO[0m[0m 03/12 10:19:32 | 0.520548    0.525926    0.750306    0.748850    0.623865    0.937279    0.929329    0.673882    0.664783    0.639756    0.652439    0.520548    0.525926    200         2.296820    0.921458    0.468613    128.253100 
[37m[36mINFO[0m[0m 03/12 10:23:16 | 0.592743    0.598519    0.798654    0.794079    0.502014    0.980565    0.982332    0.716235    0.730697    0.699162    0.669207    0.592743    0.598519    400         4.593640    0.572405    0.469320    129.975776 
[37m[36mINFO[0m[0m 03/12 10:27:01 | 0.555720    0.558519    0.790228    0.774605    0.534534    0.950530    0.943463    0.675294    0.676083    0.744859    0.704268    0.555720    0.558519    600         6.890459    0.498634    0.467135    132.391903 
[37m[36mINFO[0m[0m 03/12 10:30:46 | 0.708626    0.694815    0.849974    0.835800    0.426763    0.995583    0.989399    0.753882    0.734463    0.800457    0.783537    0.708626    0.694815    800         9.187279    0.451699    0.475416    130.016282 
[37m[36mINFO[0m[0m 03/12 10:34:27 | 0.602740    0.607407    0.829079    0.809329    0.465017    0.993816    0.992933    0.720000    0.723164    0.773420    0.711890    0.602740    0.607407    1000        11.484099   0.421899    0.453427    129.607946 
[37m[36mINFO[0m[0m 03/12 10:38:09 | 0.661237    0.677037    0.849285    0.812999    0.481560    0.996466    0.982332    0.770353    0.732580    0.781036    0.724085    0.661237    0.677037    1200        13.780919   0.406623    0.461223    129.787702 
[37m[36mINFO[0m[0m 03/12 10:41:50 | 0.654943    0.625185    0.837418    0.804097    0.482432    0.985866    0.957597    0.767059    0.738230    0.759330    0.716463    0.654943    0.625185    1400        16.077739   0.375926    0.457582    129.806239 
[37m[36mINFO[0m[0m 03/12 10:45:31 | 0.635690    0.614815    0.835698    0.791119    0.519419    0.990283    0.964664    0.796706    0.728814    0.720107    0.679878    0.635690    0.614815    1600        18.374558   0.360368    0.464471    127.634707 
[37m[36mINFO[0m[0m 03/12 10:49:11 | 0.654943    0.637037    0.888301    0.828106    0.477530    0.991166    0.985866    0.818824    0.743879    0.854912    0.754573    0.654943    0.637037    1800        20.671378   0.312327    0.456828    129.217211 
[37m[36mINFO[0m[0m 03/12 10:52:55 | 0.633469    0.629630    0.874727    0.799906    0.534661    1.000000    0.982332    0.806588    0.702448    0.817593    0.714939    0.633469    0.629630    2000        22.968198   0.306502    0.467755    130.234328 
[37m[36mINFO[0m[0m 03/12 10:56:36 | 0.606072    0.617778    0.900298    0.821494    0.498412    0.998233    0.992933    0.836706    0.730697    0.865956    0.740854    0.606072    0.617778    2200        25.265018   0.284770    0.463086    128.772538 
[37m[36mINFO[0m[0m 03/12 11:00:21 | 0.605702    0.616296    0.892616    0.818355    0.520556    0.991166    0.975265    0.818824    0.741996    0.867860    0.737805    0.605702    0.616296    2400        27.561837   0.274897    0.481483    127.782654 
[37m[36mINFO[0m[0m 03/12 11:04:07 | 0.645687    0.644444    0.914934    0.831717    0.486822    0.995583    0.992933    0.861176    0.747646    0.888043    0.754573    0.645687    0.644444    2600        29.858657   0.241307    0.480141    130.698755 
[37m[36mINFO[0m[0m 03/12 11:07:49 | 0.623103    0.623704    0.906647    0.804940    0.567175    0.997350    0.985866    0.817412    0.691149    0.905179    0.737805    0.623103    0.623704    2800        32.155477   0.223534    0.471706    127.862755 
[37m[36mINFO[0m[0m 03/12 11:11:36 | 0.571640    0.568889    0.908002    0.801981    0.600339    0.997350    0.985866    0.844706    0.700565    0.881950    0.719512    0.571640    0.568889    3000        34.452297   0.208500    0.470019    132.204471 
[37m[36mINFO[0m[0m 03/12 11:15:20 | 0.536838    0.537778    0.925695    0.802483    0.585455    0.995583    0.975265    0.868706    0.708098    0.912795    0.724085    0.536838    0.537778    3200        36.749117   0.205809    0.466854    131.272565 
[37m[36mINFO[0m[0m 03/12 11:19:01 | 0.627175    0.617778    0.914172    0.816430    0.614579    0.992933    0.971731    0.879059    0.738230    0.870526    0.739329    0.627175    0.617778    3400        39.045936   0.179038    0.457449    129.145559 
[37m[36mINFO[0m[0m 03/12 11:22:46 | 0.565346    0.576296    0.929090    0.808233    0.640758    0.992049    0.961131    0.892706    0.728814    0.902513    0.734756    0.565346    0.576296    3600        41.342756   0.165834    0.472727    130.674217 
[37m[36mINFO[0m[0m 03/12 11:26:28 | 0.596446    0.597037    0.943055    0.821142    0.587840    0.996466    0.985866    0.903529    0.738230    0.929170    0.739329    0.596446    0.597037    3800        43.639576   0.164565    0.459430    130.050427 
[37m[36mINFO[0m[0m 03/12 11:30:12 | 0.617549    0.629630    0.958266    0.823143    0.568498    1.000000    0.985866    0.926588    0.713748    0.948210    0.769817    0.617549    0.629630    4000        45.936396   0.143445    0.462298    131.396551 
[37m[36mINFO[0m[0m 03/12 11:33:55 | 0.630507    0.610370    0.953538    0.813059    0.658846    0.997350    0.982332    0.928000    0.717514    0.935263    0.739329    0.630507    0.610370    4200        48.233216   0.145051    0.462591    130.742939 
[37m[36mINFO[0m[0m 03/12 11:37:38 | 0.637171    0.642963    0.948510    0.815838    0.617547    0.992933    0.982332    0.920000    0.721281    0.932597    0.743902    0.637171    0.642963    4400        50.530035   0.125591    0.468676    128.749254 
[37m[36mINFO[0m[0m 03/12 11:41:18 | 0.607553    0.605926    0.958076    0.830880    0.614999    0.999117    0.992933    0.924235    0.734463    0.950876    0.765244    0.607553    0.605926    4600        52.826855   0.136171    0.457529    128.869159 
[37m[36mINFO[0m[0m 03/12 11:44:59 | 0.611625    0.608889    0.959920    0.823306    0.590707    0.998233    0.989399    0.945882    0.745763    0.935644    0.734756    0.611625    0.608889    4800        55.123675   0.117217    0.454784    129.704751 
[37m[36mINFO[0m[0m 03/12 11:48:43 | 0.623473    0.600000    0.971708    0.822756    0.646889    0.997350    0.985866    0.956235    0.747646    0.961538    0.734756    0.623473    0.600000    5000        57.420495   0.100026    0.471363    130.036708 
[37m[36mINFO[0m[0m 03/12 11:48:44 | Cumulative gradient change saved at train_output/VLCS/ERM/[3]/250312_10-13-37_clip_vitb16_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/12 11:48:46 | ---
[37m[36mINFO[0m[0m 03/12 11:48:46 | test-domain validation(oracle) = 70.863%
[37m[36mINFO[0m[0m 03/12 11:48:46 | training-domain validation(iid) = 70.863%
[37m[36mINFO[0m[0m 03/12 11:48:46 | last = 62.347%
[37m[36mINFO[0m[0m 03/12 11:48:46 | last (inD) = 82.276%
[37m[36mINFO[0m[0m 03/12 11:48:46 | training-domain validation (iid, inD) = 83.580%
[37m[36mINFO[0m[0m 03/12 11:48:46 | === Summary ===
[37m[36mINFO[0m[0m 03/12 11:48:46 | Command: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 1 --hparams_seed 13
[37m[36mINFO[0m[0m 03/12 11:48:46 | Unique name: 250312_10-13-37_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/12 11:48:46 | Out path: train_output/VLCS/ERM/[3]/250312_10-13-37_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/12 11:48:46 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/12 11:48:46 | Dataset: VLCS
