[37m[36mINFO[0m[0m 03/29 17:54:27 | Command :: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.22.4
	PIL: 9.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: sgd_sharpness
	out_dir: train_output/VLCS/ERM/[3]/250329_17-54-27_sgd_sharpness
	out_root: train_output/VLCS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250329_17-54-27_sgd_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/29 17:54:27 | n_steps = 5001
[37m[36mINFO[0m[0m 03/29 17:54:27 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/29 17:54:27 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/29 17:54:27 | 
[37m[36mINFO[0m[0m 03/29 17:54:27 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/29 17:54:27 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/29 17:54:27 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/29 17:54:27 | Batch sizes for each domain: [32, 32, 32, 0] (total=96)
[37m[36mINFO[0m[0m 03/29 17:54:27 | steps-per-epoch for each domain: 35.38, 66.41, 82.06 -> min = 35.38
[37m[36mINFO[0m[0m 03/29 17:54:28 | # of params = 23518277
[37m[36mINFO[0m[0m 03/29 17:56:35 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/29 17:56:35 | 0.168456    0.154074    0.146234    0.152897    1.831307    0.142226    0.141343    0.072941    0.073446    0.223534    0.243902    0.168456    0.154074    0           0.000000    1.809193    1.573343    124.890327 
[37m[36mINFO[0m[0m 03/29 18:02:36 | 0.479082    0.474074    0.512636    0.517893    1.176358    0.631625    0.636042    0.482824    0.487759    0.423458    0.429878    0.479082    0.474074    200         5.653710    1.422737    1.070651    126.127368 
[37m[36mINFO[0m[0m 03/29 18:08:38 | 0.549056    0.555556    0.616894    0.625630    0.967628    0.696113    0.699647    0.606588    0.610169    0.547982    0.567073    0.549056    0.555556    400         11.307420   1.063666    1.078084    124.554917 
[37m[36mINFO[0m[0m 03/29 18:14:41 | 0.621992    0.628148    0.685992    0.684773    0.843428    0.731449    0.731449    0.637647    0.638418    0.688880    0.684451    0.621992    0.628148    600         16.961131   0.909623    1.068874    127.495461 
[37m[36mINFO[0m[0m 03/29 18:20:41 | 0.635690    0.650370    0.722067    0.725146    0.751274    0.772085    0.780919    0.654588    0.670433    0.739528    0.724085    0.635690    0.650370    800         22.614841   0.800362    1.064388    125.899663 
[37m[36mINFO[0m[0m 03/29 18:26:39 | 0.651981    0.659259    0.747293    0.746992    0.682697    0.810954    0.823322    0.665882    0.679849    0.765042    0.737805    0.651981    0.659259    1000        28.268551   0.716660    1.056767    124.939451 
[37m[36mINFO[0m[0m 03/29 18:32:40 | 0.660126    0.668148    0.775410    0.769986    0.628591    0.868375    0.869258    0.683294    0.696798    0.774562    0.743902    0.660126    0.668148    1200        33.922261   0.660723    1.055943    128.413107 
[37m[36mINFO[0m[0m 03/29 18:38:45 | 0.671603    0.681481    0.793110    0.784778    0.588989    0.905477    0.893993    0.691294    0.711864    0.782559    0.748476    0.671603    0.681481    1400        39.575972   0.610769    1.082092    126.299832 
[37m[36mINFO[0m[0m 03/29 18:44:43 | 0.678638    0.684444    0.800775    0.796634    0.556992    0.926678    0.925795    0.692706    0.715631    0.782940    0.748476    0.678638    0.684444    1600        45.229682   0.584635    1.059696    123.922670 
[37m[36mINFO[0m[0m 03/29 18:50:45 | 0.688264    0.700741    0.809597    0.803455    0.532635    0.943463    0.936396    0.703529    0.719397    0.781797    0.754573    0.688264    0.700741    1800        50.883392   0.552329    1.075317    126.051822 
[37m[36mINFO[0m[0m 03/29 18:56:45 | 0.690115    0.705185    0.816516    0.809853    0.512942    0.948763    0.954064    0.708706    0.719397    0.792079    0.756098    0.690115    0.705185    2000        56.537102   0.515699    1.064888    124.767914 
[37m[36mINFO[0m[0m 03/29 19:02:43 | 0.701222    0.711111    0.823397    0.815700    0.498418    0.969965    0.968198    0.715765    0.721281    0.784463    0.757622    0.701222    0.711111    2200        62.190813   0.509808    1.055759    125.345858 
[37m[36mINFO[0m[0m 03/29 19:08:46 | 0.704554    0.714074    0.827491    0.819234    0.485491    0.980565    0.978799    0.718588    0.721281    0.783321    0.757622    0.704554    0.714074    2400        67.844523   0.496604    1.069823    127.315578 
[37m[36mINFO[0m[0m 03/29 19:14:49 | 0.707146    0.711111    0.828972    0.827072    0.477434    0.983216    0.989399    0.720000    0.725047    0.783701    0.766768    0.707146    0.711111    2600        73.498233   0.473801    1.078403    126.389721 
[37m[36mINFO[0m[0m 03/29 19:20:49 | 0.713069    0.718519    0.830708    0.827861    0.466428    0.990283    0.992933    0.716235    0.726930    0.785605    0.763720    0.713069    0.718519    2800        79.151943   0.470207    1.073695    123.397419 
[37m[36mINFO[0m[0m 03/29 19:26:50 | 0.718623    0.725926    0.832604    0.827741    0.459186    0.992049    0.992933    0.722824    0.725047    0.782940    0.765244    0.718623    0.725926    3000        84.805654   0.462689    1.071954    125.209061 
[37m[36mINFO[0m[0m 03/29 19:32:49 | 0.723066    0.721481    0.839885    0.829625    0.452783    0.993816    0.992933    0.732235    0.730697    0.793602    0.765244    0.723066    0.721481    3200        90.459364   0.446723    1.063521    123.946146 
[37m[36mINFO[0m[0m 03/29 19:38:47 | 0.729359    0.725926    0.836522    0.830922    0.446828    0.995583    0.996466    0.720000    0.732580    0.793983    0.763720    0.729359    0.725926    3400        96.113074   0.439045    1.061046    124.090802 
[37m[36mINFO[0m[0m 03/29 19:44:48 | 0.734543    0.727407    0.840296    0.833863    0.441715    0.996466    1.000000    0.722824    0.736347    0.801599    0.765244    0.734543    0.727407    3600        101.766784  0.440374    1.065263    126.188726 
[37m[36mINFO[0m[0m 03/29 19:50:49 | 0.738986    0.730370    0.838822    0.836016    0.437086    0.997350    1.000000    0.728941    0.738230    0.790175    0.769817    0.738986    0.730370    3800        107.420495  0.427186    1.067947    126.020393 
[37m[36mINFO[0m[0m 03/29 19:56:53 | 0.743799    0.736296    0.838893    0.835896    0.432557    0.996466    1.000000    0.724706    0.736347    0.795506    0.771341    0.743799    0.736296    4000        113.074205  0.421199    1.087972    125.033516 
[37m[36mINFO[0m[0m 03/29 20:02:56 | 0.743428    0.734815    0.841163    0.838407    0.429895    0.998233    1.000000    0.733176    0.743879    0.792079    0.771341    0.743428    0.734815    4200        118.727915  0.432496    1.078754    125.290177 
[37m[36mINFO[0m[0m 03/29 20:08:58 | 0.743428    0.730370    0.842078    0.839035    0.427670    0.997350    1.000000    0.732235    0.745763    0.796649    0.771341    0.743428    0.730370    4400        124.381625  0.419748    1.077359    125.373942 
[37m[36mINFO[0m[0m 03/29 20:15:02 | 0.749352    0.742222    0.845706    0.839931    0.424163    0.999117    1.000000    0.734118    0.743879    0.803884    0.775915    0.749352    0.742222    4600        130.035336  0.424731    1.077320    127.146660 
[37m[36mINFO[0m[0m 03/29 20:21:00 | 0.748241    0.736296    0.841977    0.839423    0.422126    0.998233    1.000000    0.736000    0.743879    0.791698    0.774390    0.748241    0.736296    4800        135.689046  0.422013    1.061420    124.078909 
[37m[36mINFO[0m[0m 03/29 20:27:03 | 0.746020    0.739259    0.844932    0.839692    0.419479    0.993816    1.000000    0.731765    0.740113    0.809216    0.778963    0.746020    0.739259    5000        141.342756  0.411502    1.074860    125.324268 
[37m[36mINFO[0m[0m 03/29 20:27:24 | Cumulative gradient change saved at train_output/VLCS/ERM/[3]/250329_17-54-27_sgd_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/29 20:27:26 | ---
[37m[36mINFO[0m[0m 03/29 20:27:26 | test-domain validation(oracle) = 74.935%
[37m[36mINFO[0m[0m 03/29 20:27:26 | training-domain validation(iid) = 74.935%
[37m[36mINFO[0m[0m 03/29 20:27:26 | last = 74.602%
[37m[36mINFO[0m[0m 03/29 20:27:26 | last (inD) = 83.969%
[37m[36mINFO[0m[0m 03/29 20:27:26 | training-domain validation (iid, inD) = 83.993%
[37m[36mINFO[0m[0m 03/29 20:27:26 | === Summary ===
[37m[36mINFO[0m[0m 03/29 20:27:26 | Command: /jsm0707/GENIE/train_all.py sgd_sharpness config/resnet50_sgd.yaml --algorithm ERM --test_envs 3 --dataset VLCS
[37m[36mINFO[0m[0m 03/29 20:27:26 | Unique name: 250329_17-54-27_sgd_sharpness
[37m[36mINFO[0m[0m 03/29 20:27:26 | Out path: train_output/VLCS/ERM/[3]/250329_17-54-27_sgd_sharpness
[37m[36mINFO[0m[0m 03/29 20:27:26 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/29 20:27:26 | Dataset: VLCS
