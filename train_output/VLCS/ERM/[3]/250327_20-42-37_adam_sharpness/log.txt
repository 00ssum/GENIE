[37m[36mINFO[0m[0m 03/27 20:42:37 | Command :: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: adam_sharpness
	out_dir: train_output/VLCS/ERM/[3]/250327_20-42-37_adam_sharpness
	out_root: train_output/VLCS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250327_20-42-37_adam_sharpness
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/27 20:42:37 | n_steps = 5001
[37m[36mINFO[0m[0m 03/27 20:42:37 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/27 20:42:37 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/27 20:42:37 | 
[37m[36mINFO[0m[0m 03/27 20:42:37 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/27 20:42:37 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/27 20:42:37 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/27 20:42:37 | Batch sizes for each domain: [32, 32, 32, 0] (total=96)
[37m[36mINFO[0m[0m 03/27 20:42:37 | steps-per-epoch for each domain: 35.38, 66.41, 82.06 -> min = 35.38
[37m[36mINFO[0m[0m 03/27 20:42:38 | # of params = 23518277
[37m[36mINFO[0m[0m 03/27 20:44:58 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/27 20:44:58 | 0.393558    0.413333    0.468913    0.422612    1.400666    0.477032    0.325088    0.531765    0.523540    0.397944    0.419207    0.393558    0.413333    0           0.000000    1.809193    1.380975    138.121174 
[37m[36mINFO[0m[0m 03/27 20:51:18 | 0.768604    0.789630    0.873657    0.851410    0.387845    0.998233    1.000000    0.783059    0.764595    0.839680    0.789634    0.768604    0.789630    200         5.653710    0.430135    1.101481    137.773544 
[37m[36mINFO[0m[0m 03/27 20:57:46 | 0.768604    0.764444    0.894391    0.840039    0.421969    0.999117    0.996466    0.799059    0.740113    0.884996    0.783537    0.768604    0.764444    400         11.307420   0.298172    1.137106    138.979925 
[37m[36mINFO[0m[0m 03/27 21:04:08 | 0.722695    0.733333    0.936958    0.844672    0.431442    1.000000    0.996466    0.880941    0.757062    0.929931    0.780488    0.722695    0.733333    600         16.961131   0.233833    1.142186    131.185861 
[37m[36mINFO[0m[0m 03/27 21:10:23 | 0.733802    0.745185    0.952120    0.835764    0.498149    1.000000    0.996466    0.915765    0.728814    0.940594    0.782012    0.733802    0.745185    800         22.614841   0.173473    1.104118    132.424551 
[37m[36mINFO[0m[0m 03/27 21:16:38 | 0.762310    0.797037    0.957519    0.858691    0.559387    1.000000    0.996466    0.922824    0.777778    0.949733    0.801829    0.762310    0.797037    1000        28.268551   0.135691    1.118845    130.364708 
[37m[36mINFO[0m[0m 03/27 21:22:51 | 0.729730    0.737778    0.969264    0.846304    0.706966    0.995583    0.992933    0.935059    0.753296    0.977152    0.792683    0.729730    0.737778    1200        33.922261   0.117160    1.090561    132.400937 
[37m[36mINFO[0m[0m 03/27 21:29:02 | 0.758238    0.762963    0.974100    0.853861    0.663834    1.000000    1.000000    0.960000    0.787194    0.962300    0.774390    0.758238    0.762963    1400        39.575972   0.093725    1.094584    131.025590 
[37m[36mINFO[0m[0m 03/27 21:35:18 | 0.656424    0.648889    0.974973    0.832818    0.719619    1.000000    1.000000    0.955765    0.743879    0.969155    0.754573    0.656424    0.648889    1600        45.229682   0.068587    1.118052    129.973685 
[37m[36mINFO[0m[0m 03/27 21:41:41 | 0.722695    0.734815    0.961588    0.842562    0.689792    1.000000    1.000000    0.920941    0.753296    0.963823    0.774390    0.722695    0.734815    1800        50.883392   0.062541    1.134818    134.346604 
[37m[36mINFO[0m[0m 03/27 21:48:01 | 0.743799    0.755556    0.959662    0.824914    0.843084    0.997350    0.996466    0.944471    0.741996    0.937167    0.736280    0.743799    0.755556    2000        56.537102   0.048510    1.122560    134.264946 
[37m[36mINFO[0m[0m 03/27 21:54:22 | 0.768604    0.758519    0.991001    0.854878    0.745594    1.000000    1.000000    0.988235    0.787194    0.984768    0.777439    0.768604    0.758519    2200        62.190813   0.049758    1.128663    133.318546 
[37m[36mINFO[0m[0m 03/27 22:00:40 | 0.719363    0.724444    0.991733    0.856671    0.753194    1.000000    1.000000    0.987765    0.783427    0.987433    0.786585    0.719363    0.724444    2400        67.844523   0.043324    1.137210    128.987857 
[37m[36mINFO[0m[0m 03/27 22:06:54 | 0.711218    0.728889    0.987763    0.857089    0.883117    0.999117    1.000000    0.977882    0.774011    0.986291    0.797256    0.711218    0.728889    2600        73.498233   0.030036    1.110438    130.108230 
[37m[36mINFO[0m[0m 03/27 22:13:05 | 0.718252    0.718519    0.972568    0.821578    0.815938    1.000000    1.000000    0.945882    0.726930    0.971820    0.737805    0.718252    0.718519    2800        79.151943   0.033963    1.103408    128.745431 
[37m[36mINFO[0m[0m 03/27 22:19:21 | 0.711959    0.715556    0.987335    0.832548    0.997287    0.996466    1.000000    0.975059    0.715631    0.990480    0.782012    0.711959    0.715556    3000        84.805654   0.031679    1.087398    137.163781 
[37m[36mINFO[0m[0m 03/27 22:25:40 | 0.748612    0.749630    0.995713    0.844565    0.895598    1.000000    1.000000    0.992471    0.760829    0.994669    0.772866    0.748612    0.749630    3200        90.459364   0.038141    1.117006    133.819978 
[37m[36mINFO[0m[0m 03/27 22:31:44 | 0.713069    0.705185    0.990119    0.845461    0.930533    1.000000    1.000000    0.978353    0.758945    0.992003    0.777439    0.713069    0.705185    3400        96.113074   0.024245    1.052547    131.826749 
[37m[36mINFO[0m[0m 03/27 22:38:01 | 0.743428    0.739259    0.985392    0.833523    0.987550    1.000000    0.996466    0.977882    0.749529    0.978294    0.754573    0.743428    0.739259    3600        101.766784  0.022245    1.101984    134.509630 
[37m[36mINFO[0m[0m 03/27 22:44:06 | 0.715661    0.715556    0.990687    0.834073    0.832354    1.000000    1.000000    0.987294    0.747646    0.984768    0.754573    0.715661    0.715556    3800        107.420495  0.030090    1.057744    132.146080 
[37m[36mINFO[0m[0m 03/27 22:50:09 | 0.707146    0.737778    0.993727    0.850441    0.939951    1.000000    0.996466    0.989176    0.775895    0.992003    0.778963    0.707146    0.737778    4000        113.074205  0.029570    1.069988    127.013367 
[37m[36mINFO[0m[0m 03/27 22:56:25 | 0.711588    0.703704    0.992842    0.840081    0.800082    0.999117    1.000000    0.985882    0.738230    0.993526    0.782012    0.711588    0.703704    4200        118.727915  0.040628    1.126125    128.677558 
[37m[36mINFO[0m[0m 03/27 23:02:30 | 0.707146    0.720000    0.992494    0.840559    0.894705    1.000000    1.000000    0.987765    0.745763    0.989718    0.775915    0.707146    0.720000    4400        124.381625  0.021409    1.075377    128.633263 
[37m[36mINFO[0m[0m 03/27 23:08:48 | 0.710478    0.731852    0.992861    0.833433    0.832286    1.000000    0.996466    0.991529    0.740113    0.987053    0.763720    0.710478    0.731852    4600        130.035336  0.020434    1.090766    137.424643 
[37m[36mINFO[0m[0m 03/27 23:15:00 | 0.694558    0.714074    0.992316    0.845881    0.972995    1.000000    1.000000    0.992941    0.781544    0.984006    0.756098    0.694558    0.714074    4800        135.689046  0.024159    1.080862    134.891815 
[37m[36mINFO[0m[0m 03/27 23:21:11 | 0.723806    0.725926    0.989910    0.830038    1.108590    1.000000    1.000000    0.983059    0.740113    0.986672    0.750000    0.723806    0.725926    5000        141.342756  0.020126    1.091118    130.605269 
[37m[36mINFO[0m[0m 03/27 23:21:33 | Cumulative gradient change saved at train_output/VLCS/ERM/[3]/250327_20-42-37_adam_sharpness/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/27 23:21:34 | ---
[37m[36mINFO[0m[0m 03/27 23:21:34 | test-domain validation(oracle) = 76.231%
[37m[36mINFO[0m[0m 03/27 23:21:34 | training-domain validation(iid) = 76.231%
[37m[36mINFO[0m[0m 03/27 23:21:34 | last = 72.381%
[37m[36mINFO[0m[0m 03/27 23:21:34 | last (inD) = 83.004%
[37m[36mINFO[0m[0m 03/27 23:21:34 | training-domain validation (iid, inD) = 85.869%
[37m[36mINFO[0m[0m 03/27 23:21:34 | === Summary ===
[37m[36mINFO[0m[0m 03/27 23:21:34 | Command: /jsm0707/GENIE/train_all.py adam_sharpness config/resnet50_adam.yaml --algorithm ERM --test_envs 3 --dataset VLCS
[37m[36mINFO[0m[0m 03/27 23:21:34 | Unique name: 250327_20-42-37_adam_sharpness
[37m[36mINFO[0m[0m 03/27 23:21:34 | Out path: train_output/VLCS/ERM/[3]/250327_20-42-37_adam_sharpness
[37m[36mINFO[0m[0m 03/27 23:21:34 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/27 23:21:34 | Dataset: VLCS
