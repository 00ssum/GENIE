[37m[36mINFO[0m[0m 03/12 04:30:56 | Command :: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 1 --hparams_seed 4
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: ERM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/clip_vitb16_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 4
	in_domain: False
	model_save: None
	mpa: False
	name: clip_vitb16_GENIE
	out_dir: train_output/VLCS/ERM/[3]/250312_04-30-56_clip_vitb16_GENIE
	out_root: train_output/VLCS/ERM/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 1
	unique_name: 250312_04-30-56_clip_vitb16_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 7.71168362195819e-05
	batch_size: 8
	weight_decay: 0.0007466379205516084
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: openclip_vit-b16
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/12 04:30:56 | n_steps = 5001
[37m[36mINFO[0m[0m 03/12 04:30:56 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/12 04:30:56 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/12 04:30:56 | 
[37m[36mINFO[0m[0m 03/12 04:30:56 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/12 04:30:56 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/12 04:30:56 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/12 04:30:56 | Batch sizes for each domain: [8, 8, 8, 0] (total=24)
[37m[36mINFO[0m[0m 03/12 04:30:56 | steps-per-epoch for each domain: 141.50, 265.62, 328.25 -> min = 141.50
[37m[36mINFO[0m[0m 03/12 04:30:58 | # of params = 86195205
[37m[36mINFO[0m[0m 03/12 04:33:13 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/12 04:33:13 | 0.375787    0.354074    0.446248    0.476479    1.412340    0.500000    0.544170    0.486118    0.485876    0.352628    0.399390    0.375787    0.354074    0           0.000000    1.708399    1.039890    133.385647 
[37m[36mINFO[0m[0m 03/12 04:36:21 | 0.453536    0.401481    0.531744    0.551545    1.142990    0.642226    0.643110    0.534118    0.540490    0.418888    0.471037    0.453536    0.401481    200         1.413428    1.250309    0.285262    131.139308 
[37m[36mINFO[0m[0m 03/12 04:39:29 | 0.414291    0.426667    0.579230    0.593229    0.929603    0.751767    0.770318    0.503059    0.497175    0.482864    0.512195    0.414291    0.426667    400         2.826855    1.096295    0.284916    130.647839 
[37m[36mINFO[0m[0m 03/12 04:42:38 | 0.482044    0.466667    0.667093    0.676889    0.752323    0.887809    0.925795    0.562824    0.542373    0.550647    0.562500    0.482044    0.466667    600         4.240283    0.896911    0.293227    130.275924 
[37m[36mINFO[0m[0m 03/12 04:45:43 | 0.549056    0.531852    0.726050    0.741839    0.646953    0.954947    0.950530    0.656941    0.659134    0.566260    0.615854    0.549056    0.531852    800         5.653710    0.727320    0.296734    126.472279 
[37m[36mINFO[0m[0m 03/12 04:48:54 | 0.556090    0.540741    0.771770    0.756843    0.593518    0.958481    0.932862    0.684706    0.676083    0.672125    0.661585    0.556090    0.540741    1000        7.067138    0.636206    0.295693    131.511085 
[37m[36mINFO[0m[0m 03/12 04:52:02 | 0.569049    0.552593    0.786573    0.787287    0.536758    0.971731    0.989399    0.682353    0.698682    0.705636    0.673780    0.569049    0.552593    1200        8.480565    0.589051    0.294585    129.497282 
[37m[36mINFO[0m[0m 03/12 04:55:11 | 0.614217    0.611852    0.791491    0.806854    0.494119    0.984982    0.985866    0.684235    0.721281    0.705255    0.713415    0.614217    0.611852    1400        9.893993    0.509233    0.291776    130.503968 
[37m[36mINFO[0m[0m 03/12 04:58:20 | 0.586820    0.602963    0.777389    0.783012    0.570218    0.973498    0.971731    0.690353    0.708098    0.668317    0.669207    0.586820    0.602963    1600        11.307420   0.507991    0.294720    130.110444 
[37m[36mINFO[0m[0m 03/12 05:01:29 | 0.642355    0.631111    0.825406    0.804288    0.477008    0.984982    0.978799    0.752471    0.709981    0.738766    0.724085    0.642355    0.631111    1800        12.720848   0.476394    0.288675    130.843433 
[37m[36mINFO[0m[0m 03/12 05:04:39 | 0.612366    0.613333    0.824474    0.812987    0.479496    0.984099    0.978799    0.728471    0.726930    0.760853    0.733232    0.612366    0.613333    2000        14.134276   0.476875    0.293824    130.749103 
[37m[36mINFO[0m[0m 03/12 05:07:50 | 0.470566    0.482963    0.733211    0.733041    0.703474    0.899293    0.886926    0.630118    0.638418    0.670221    0.673780    0.470566    0.482963    2200        15.547703   0.471923    0.301569    131.054383 
[37m[36mINFO[0m[0m 03/12 05:11:03 | 0.587560    0.597037    0.818319    0.782288    0.551909    0.987633    0.961131    0.720941    0.704331    0.746382    0.681402    0.587560    0.597037    2400        16.961131   0.448518    0.301208    132.634598 
[37m[36mINFO[0m[0m 03/12 05:14:11 | 0.607183    0.583704    0.798813    0.765029    0.559126    0.974382    0.957597    0.733176    0.691149    0.688880    0.646341    0.607183    0.583704    2600        18.374558   0.433217    0.284305    131.739742 
[37m[36mINFO[0m[0m 03/12 05:17:22 | 0.614957    0.582222    0.840408    0.812981    0.488121    0.986749    0.968198    0.767529    0.734463    0.766946    0.736280    0.614957    0.582222    2800        19.787986   0.418138    0.294243    131.438791 
[37m[36mINFO[0m[0m 03/12 05:20:29 | 0.626064    0.604444    0.855703    0.827455    0.442060    0.997350    0.996466    0.778824    0.741996    0.790937    0.743902    0.626064    0.604444    3000        21.201413   0.430167    0.289705    129.306445 
[37m[36mINFO[0m[0m 03/12 05:23:39 | 0.599778    0.583704    0.858155    0.807869    0.484800    0.993816    0.985866    0.784000    0.689266    0.796649    0.748476    0.599778    0.583704    3200        22.614841   0.394035    0.285910    133.346216 
[37m[36mINFO[0m[0m 03/12 05:26:50 | 0.613476    0.607407    0.866290    0.819874    0.470605    0.996466    0.982332    0.796235    0.728814    0.806169    0.748476    0.613476    0.607407    3400        24.028269   0.402353    0.297594    130.660247 
[37m[36mINFO[0m[0m 03/12 05:30:00 | 0.622732    0.591111    0.845168    0.809831    0.512636    0.992933    0.982332    0.792000    0.730697    0.750571    0.716463    0.622732    0.591111    3600        25.441696   0.371971    0.287261    132.983175 
[37m[36mINFO[0m[0m 03/12 05:33:10 | 0.533506    0.542222    0.833011    0.803261    0.517787    0.992933    0.992933    0.726588    0.715631    0.779513    0.701220    0.533506    0.542222    3800        26.855124   0.360527    0.293134    131.616681 
[37m[36mINFO[0m[0m 03/12 05:36:20 | 0.598297    0.600000    0.862060    0.815701    0.488052    0.996466    0.985866    0.765647    0.732580    0.824067    0.728659    0.598297    0.600000    4000        28.268551   0.368713    0.295302    130.413008 
[37m[36mINFO[0m[0m 03/12 05:39:27 | 0.563125    0.551111    0.882517    0.811565    0.516745    0.996466    0.982332    0.809882    0.741996    0.841203    0.710366    0.563125    0.551111    4200        29.681979   0.330271    0.287261    130.218040 
[37m[36mINFO[0m[0m 03/12 05:42:36 | 0.621622    0.626667    0.862723    0.814230    0.529506    0.995583    0.975265    0.780706    0.725047    0.811881    0.742378    0.621622    0.626667    4400        31.095406   0.335688    0.282141    132.029053 
[37m[36mINFO[0m[0m 03/12 05:45:46 | 0.565346    0.579259    0.859832    0.785440    0.546212    0.997350    0.982332    0.777882    0.698682    0.804265    0.675305    0.565346    0.579259    4600        32.508834   0.352466    0.290356    131.918115 
[37m[36mINFO[0m[0m 03/12 05:48:49 | 0.569419    0.573333    0.874124    0.804325    0.519391    0.987633    0.989399    0.824000    0.726930    0.810739    0.696646    0.569419    0.573333    4800        33.922261   0.315542    0.275709    127.810418 
[37m[36mINFO[0m[0m 03/12 05:51:57 | 0.632358    0.613333    0.881036    0.804989    0.523429    0.997350    0.982332    0.817882    0.734463    0.827875    0.698171    0.632358    0.613333    5000        35.335689   0.295668    0.279057    132.489944 
[37m[36mINFO[0m[0m 03/12 05:51:58 | Cumulative gradient change saved at train_output/VLCS/ERM/[3]/250312_04-30-56_clip_vitb16_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/12 05:52:00 | ---
[37m[36mINFO[0m[0m 03/12 05:52:00 | test-domain validation(oracle) = 64.235%
[37m[36mINFO[0m[0m 03/12 05:52:00 | training-domain validation(iid) = 62.606%
[37m[36mINFO[0m[0m 03/12 05:52:00 | last = 63.236%
[37m[36mINFO[0m[0m 03/12 05:52:00 | last (inD) = 80.499%
[37m[36mINFO[0m[0m 03/12 05:52:00 | training-domain validation (iid, inD) = 82.746%
[37m[36mINFO[0m[0m 03/12 05:52:00 | === Summary ===
[37m[36mINFO[0m[0m 03/12 05:52:00 | Command: /jsm0707/GENIE/train_all.py clip_vitb16_GENIE config/clip_vitb16_GENIE.yaml --algorithm ERM --test_envs 3 --dataset VLCS --trial_seed 1 --hparams_seed 4
[37m[36mINFO[0m[0m 03/12 05:52:00 | Unique name: 250312_04-30-56_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/12 05:52:00 | Out path: train_output/VLCS/ERM/[3]/250312_04-30-56_clip_vitb16_GENIE
[37m[36mINFO[0m[0m 03/12 05:52:00 | Algorithm: ERM
[37m[36mINFO[0m[0m 03/12 05:52:00 | Dataset: VLCS
