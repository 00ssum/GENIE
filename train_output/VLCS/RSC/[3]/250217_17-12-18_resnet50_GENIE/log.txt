[37m[36mINFO[0m[0m 02/17 17:12:18 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 3 --dataset VLCS --trial_seed 2 --hparams_seed 11
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 11
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/RSC/[3]/250217_17-12-18_resnet50_GENIE
	out_root: train_output/VLCS/RSC/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 2
	unique_name: 250217_17-12-18_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.00030079935666912124
	batch_size: 16
	weight_decay: 6.276702207693199e-06
	rsc_f_drop_factor: 0.4308864514160952
	rsc_b_drop_factor: 0.36746877772024616
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/17 17:12:18 | n_steps = 5001
[37m[36mINFO[0m[0m 02/17 17:12:18 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/17 17:12:18 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/17 17:12:18 | 
[37m[36mINFO[0m[0m 02/17 17:12:18 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 02/17 17:12:18 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 02/17 17:12:18 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 02/17 17:12:18 | Batch sizes for each domain: [16, 16, 16, 0] (total=48)
[37m[36mINFO[0m[0m 02/17 17:12:18 | steps-per-epoch for each domain: 70.75, 132.81, 164.12 -> min = 70.75
[37m[36mINFO[0m[0m 02/17 17:12:19 | # of params = 23518277
[37m[36mINFO[0m[0m 02/17 17:14:41 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/17 17:14:41 | 0.457238    0.484444    0.498200    0.485274    1.356938    0.621908    0.590106    0.456471    0.463277    0.416222    0.402439    0.457238    0.484444    0           0.000000    3.408393    0.916117    141.443829 
[37m[36mINFO[0m[0m 02/17 17:19:09 | 0.673084    0.648889    0.771237    0.770909    0.599056    0.931979    0.911661    0.663529    0.696798    0.718203    0.704268    0.673084    0.648889    200         2.826855    1.258227    0.634540    141.035568 
[37m[36mINFO[0m[0m 02/17 17:23:38 | 0.681970    0.672593    0.800514    0.804014    0.573171    0.992933    0.985866    0.710588    0.732580    0.698020    0.693598    0.681970    0.672593    400         5.653710    1.018797    0.619407    144.369422 
[37m[36mINFO[0m[0m 02/17 17:28:02 | 0.721955    0.703704    0.834764    0.826426    0.495458    0.996466    0.992933    0.725647    0.736347    0.782178    0.750000    0.721955    0.703704    600         8.480565    0.950183    0.630927    138.584645 
[37m[36mINFO[0m[0m 02/17 17:32:26 | 0.709367    0.697778    0.845799    0.836936    0.479380    0.998233    0.989399    0.724235    0.736347    0.814928    0.785061    0.709367    0.697778    800         11.307420   0.918742    0.606099    142.956788 
[37m[36mINFO[0m[0m 02/17 17:36:53 | 0.747871    0.749630    0.846189    0.834378    0.454861    0.996466    0.992933    0.733647    0.749529    0.808454    0.760671    0.747871    0.749630    1000        14.134276   0.871367    0.609720    144.480187 
[37m[36mINFO[0m[0m 02/17 17:41:14 | 0.731951    0.718519    0.846309    0.839729    0.473824    0.995583    0.992933    0.766118    0.777778    0.777228    0.748476    0.731951    0.718519    1200        16.961131   0.859308    0.624190    136.593338 
[37m[36mINFO[0m[0m 02/17 17:45:49 | 0.731951    0.718519    0.876730    0.846788    0.473555    0.999117    0.985866    0.786824    0.774011    0.844250    0.780488    0.731951    0.718519    1400        19.787986   0.836175    0.659543    143.022176 
[37m[36mINFO[0m[0m 02/17 17:50:20 | 0.760459    0.745185    0.855159    0.833063    0.463653    1.000000    0.992933    0.762353    0.760829    0.803123    0.745427    0.760459    0.745185    1600        22.614841   0.829255    0.645018    141.604501 
[37m[36mINFO[0m[0m 02/17 17:54:55 | 0.713069    0.721481    0.881164    0.832805    0.508064    0.996466    0.978799    0.798588    0.758945    0.848439    0.760671    0.713069    0.721481    1800        25.441696   0.797961    0.704126    134.280123 
[37m[36mINFO[0m[0m 02/17 17:59:41 | 0.758978    0.757037    0.883665    0.844571    0.476907    1.000000    0.992933    0.809412    0.774011    0.841584    0.766768    0.758978    0.757037    2000        28.268551   0.787624    0.722231    141.464669 
[37m[36mINFO[0m[0m 02/17 18:04:24 | 0.753054    0.740741    0.895826    0.847189    0.504188    0.999117    0.989399    0.813647    0.777778    0.874714    0.774390    0.753054    0.740741    2200        31.095406   0.746883    0.651994    152.745593 
