[37m[36mINFO[0m[0m 02/11 08:52:44 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 3 --dataset VLCS --trial_seed 0 --hparams_seed 14
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 14
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/RSC/[3]/250211_08-52-44_resnet50_GENIE
	out_root: train_output/VLCS/RSC/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250211_08-52-44_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 2.1399727535070965e-05
	batch_size: 16
	weight_decay: 0.00022842646099519332
	rsc_f_drop_factor: 0.03663071938022161
	rsc_b_drop_factor: 0.4317269967185047
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/11 08:52:44 | n_steps = 5001
[37m[36mINFO[0m[0m 02/11 08:52:44 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/11 08:52:44 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/11 08:52:44 | 
[37m[36mINFO[0m[0m 02/11 08:52:44 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 02/11 08:52:44 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 02/11 08:52:44 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 02/11 08:52:44 | Batch sizes for each domain: [16, 16, 16, 0] (total=48)
[37m[36mINFO[0m[0m 02/11 08:52:44 | steps-per-epoch for each domain: 70.75, 132.81, 164.12 -> min = 70.75
[37m[36mINFO[0m[0m 02/11 08:52:46 | # of params = 23518277
[37m[36mINFO[0m[0m 02/11 08:55:11 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/11 08:55:11 | 0.465013    0.469630    0.506338    0.501286    1.273643    0.632509    0.639576    0.488941    0.487759    0.397563    0.376524    0.465013    0.469630    0           0.000000    2.088452    1.421736    144.229335 
[37m[36mINFO[0m[0m 02/11 08:59:29 | 0.733802    0.711111    0.834071    0.828961    0.449521    0.999117    1.000000    0.722824    0.723164    0.780274    0.763720    0.733802    0.711111    200         2.826855    0.668111    0.586544    140.442094 
[37m[36mINFO[0m[0m 02/11 09:03:54 | 0.728989    0.731852    0.855174    0.846238    0.410839    0.998233    1.000000    0.761882    0.755179    0.805407    0.783537    0.728989    0.731852    400         5.653710    0.398047    0.638870    137.007506 
[37m[36mINFO[0m[0m 02/11 09:08:17 | 0.786005    0.794074    0.872167    0.849945    0.418148    0.999117    1.000000    0.776941    0.749529    0.840442    0.800305    0.786005    0.794074    600         8.480565    0.361327    0.621428    138.475870 
[37m[36mINFO[0m[0m 02/11 09:12:40 | 0.768604    0.758519    0.844529    0.818378    0.477367    0.999117    1.000000    0.693647    0.668550    0.840823    0.786585    0.768604    0.758519    800         11.307420   0.348005    0.629974    137.108227 
[37m[36mINFO[0m[0m 02/11 09:16:57 | 0.773787    0.783704    0.885010    0.846036    0.432957    0.993816    0.992933    0.815059    0.789077    0.846154    0.756098    0.773787    0.783704    1000        14.134276   0.323362    0.611137    135.329544 
[37m[36mINFO[0m[0m 02/11 09:21:20 | 0.786746    0.780741    0.894151    0.843609    0.439941    1.000000    1.000000    0.830588    0.777778    0.851866    0.753049    0.786746    0.780741    1200        16.961131   0.295217    0.620739    138.631162 
[37m[36mINFO[0m[0m 02/11 09:25:39 | 0.770826    0.757037    0.903744    0.847225    0.419231    1.000000    1.000000    0.811765    0.762712    0.899467    0.778963    0.770826    0.757037    1400        19.787986   0.282113    0.586122    141.680103 
[37m[36mINFO[0m[0m 02/11 09:29:59 | 0.748612    0.749630    0.874273    0.822863    0.489039    1.000000    1.000000    0.759529    0.691149    0.863290    0.777439    0.748612    0.749630    1600        22.614841   0.266595    0.614183    136.856441 
[37m[36mINFO[0m[0m 02/11 09:34:23 | 0.756387    0.765926    0.911194    0.844147    0.490497    0.998233    1.000000    0.850353    0.770245    0.884996    0.762195    0.756387    0.765926    1800        25.441696   0.253877    0.632428    137.892661 
[37m[36mINFO[0m[0m 02/11 09:38:44 | 0.727879    0.731852    0.915524    0.847494    0.454955    1.000000    1.000000    0.851294    0.758945    0.895278    0.783537    0.727879    0.731852    2000        28.268551   0.226501    0.612205    138.319644 
