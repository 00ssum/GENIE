[37m[36mINFO[0m[0m 02/11 08:35:09 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 2 --dataset VLCS --trial_seed 0 --hparams_seed 10
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 10
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/RSC/[2]/250211_08-35-09_resnet50_GENIE
	out_root: train_output/VLCS/RSC/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 0
	unique_name: 250211_08-35-09_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 2.5980189819232268e-05
	batch_size: 22
	weight_decay: 1.2831747186887755e-05
	rsc_f_drop_factor: 0.49049801895660045
	rsc_b_drop_factor: 0.13734507450069044
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/11 08:35:09 | n_steps = 5001
[37m[36mINFO[0m[0m 02/11 08:35:09 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/11 08:35:09 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/11 08:35:09 | 
[37m[36mINFO[0m[0m 02/11 08:35:09 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 02/11 08:35:09 | Test envs = [2], name = te_S
[37m[36mINFO[0m[0m 02/11 08:35:09 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 02/11 08:35:09 | Batch sizes for each domain: [22, 22, 0, 22] (total=66)
[37m[36mINFO[0m[0m 02/11 08:35:09 | steps-per-epoch for each domain: 51.45, 96.59, 122.77 -> min = 51.45
[37m[36mINFO[0m[0m 02/11 08:35:11 | # of params = 23518277
[37m[36mINFO[0m[0m 02/11 08:37:27 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/11 08:37:27 | 0.384615    0.387195    0.504871    0.521514    1.281939    0.611307    0.628975    0.459765    0.489642    0.384615    0.387195    0.443539    0.445926    0           0.000000    2.542882    1.354942    134.810206 
[37m[36mINFO[0m[0m 02/11 08:42:32 | 0.699924    0.666159    0.855461    0.852764    0.401952    0.995583    1.000000    0.742588    0.741996    0.699924    0.666159    0.828212    0.816296    200         3.886926    0.838625    0.820263    141.645485 
[37m[36mINFO[0m[0m 02/11 08:47:34 | 0.710967    0.687500    0.867121    0.854873    0.376952    1.000000    1.000000    0.766118    0.743879    0.710967    0.687500    0.835246    0.820741    400         7.773852    0.599671    0.787217    144.228536 
[37m[36mINFO[0m[0m 02/11 08:52:39 | 0.754379    0.713415    0.890661    0.872911    0.338346    1.000000    1.000000    0.788235    0.768362    0.754379    0.713415    0.883747    0.850370    600         11.660777   0.559996    0.833124    138.033424 
[37m[36mINFO[0m[0m 02/11 08:57:42 | 0.771135    0.742378    0.882877    0.857485    0.387430    0.993816    1.000000    0.763294    0.725047    0.771135    0.742378    0.891522    0.847407    800         15.547703   0.518667    0.844484    134.614774 
[37m[36mINFO[0m[0m 02/11 09:02:46 | 0.746382    0.717988    0.915513    0.871429    0.339881    1.000000    1.000000    0.835765    0.768362    0.746382    0.717988    0.910774    0.845926    1000        19.434629   0.492501    0.825507    138.168150 
[37m[36mINFO[0m[0m 02/11 09:07:51 | 0.667174    0.623476    0.887848    0.861427    0.430011    1.000000    1.000000    0.781647    0.745763    0.667174    0.623476    0.881896    0.838519    1200        23.321555   0.471537    0.835395    138.575501 
[37m[36mINFO[0m[0m 02/11 09:12:57 | 0.714395    0.678354    0.919560    0.864022    0.369401    1.000000    1.000000    0.842353    0.768362    0.714395    0.678354    0.916327    0.823704    1400        27.208481   0.446704    0.830676    139.347783 
[37m[36mINFO[0m[0m 02/11 09:18:07 | 0.707159    0.670732    0.927307    0.867479    0.396958    1.000000    1.000000    0.854118    0.768362    0.707159    0.670732    0.927805    0.834074    1600        31.095406   0.426581    0.855787    139.245894 
[37m[36mINFO[0m[0m 02/11 09:23:14 | 0.755522    0.719512    0.937701    0.863438    0.370417    1.000000    0.996466    0.870118    0.741996    0.755522    0.719512    0.942984    0.851852    1800        34.982332   0.410984    0.834023    140.243662 
[37m[36mINFO[0m[0m 02/11 09:28:17 | 0.706398    0.679878    0.947243    0.876225    0.375659    1.000000    1.000000    0.882824    0.787194    0.706398    0.679878    0.958904    0.841481    2000        38.869258   0.394740    0.814423    140.235961 
[37m[36mINFO[0m[0m 02/11 09:33:22 | 0.688880    0.672256    0.949894    0.867386    0.396834    1.000000    1.000000    0.899294    0.774011    0.688880    0.672256    0.950389    0.828148    2200        42.756184   0.375274    0.830713    138.795852 
[37m[36mINFO[0m[0m 02/11 09:38:26 | 0.731531    0.696646    0.956109    0.868023    0.414000    1.000000    1.000000    0.901647    0.755179    0.731531    0.696646    0.966679    0.848889    2400        46.643110   0.353677    0.824186    138.559333 
