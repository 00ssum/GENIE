[37m[36mINFO[0m[0m 02/17 15:31:27 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 2 --dataset VLCS --trial_seed 1 --hparams_seed 15
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 15
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/RSC/[2]/250217_15-31-27_resnet50_GENIE
	out_root: train_output/VLCS/RSC/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 1
	unique_name: 250217_15-31-27_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.7812453400894684e-05
	batch_size: 36
	weight_decay: 3.1651536009272826e-06
	rsc_f_drop_factor: 0.384156695834526
	rsc_b_drop_factor: 0.10268662025909236
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/17 15:31:27 | n_steps = 5001
[37m[36mINFO[0m[0m 02/17 15:31:27 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/17 15:31:27 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/17 15:31:27 | 
[37m[36mINFO[0m[0m 02/17 15:31:27 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 02/17 15:31:27 | Test envs = [2], name = te_S
[37m[36mINFO[0m[0m 02/17 15:31:27 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 02/17 15:31:27 | Batch sizes for each domain: [36, 36, 0, 36] (total=108)
[37m[36mINFO[0m[0m 02/17 15:31:27 | steps-per-epoch for each domain: 31.44, 59.03, 75.03 -> min = 31.44
[37m[36mINFO[0m[0m 02/17 15:31:28 | # of params = 23518277
[37m[36mINFO[0m[0m 02/17 15:33:56 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/17 15:33:56 | 0.376618    0.419207    0.511439    0.495231    1.320716    0.613958    0.618375    0.466824    0.461394    0.376618    0.419207    0.453536    0.405926    0           0.000000    2.307427    1.831521    146.610236 
[37m[36mINFO[0m[0m 02/17 15:40:53 | 0.725819    0.748476    0.866128    0.864206    0.384950    1.000000    1.000000    0.753882    0.757062    0.725819    0.748476    0.844502    0.835556    200         6.360424    0.709532    1.372748    142.505252 
[37m[36mINFO[0m[0m 02/17 15:48:03 | 0.718203    0.737805    0.884708    0.869080    0.365092    1.000000    0.996466    0.777412    0.779661    0.718203    0.737805    0.876712    0.831111    400         12.720848   0.489322    1.425794    145.031874 
[37m[36mINFO[0m[0m 02/17 15:55:15 | 0.761615    0.797256    0.893869    0.872304    0.390640    1.000000    0.992933    0.788235    0.772128    0.761615    0.797256    0.893373    0.851852    600         19.081272   0.435780    1.413718    148.915612 
[37m[36mINFO[0m[0m 02/17 16:02:31 | 0.766946    0.766768    0.918420    0.882229    0.344506    1.000000    0.996466    0.831529    0.790960    0.766946    0.766768    0.923732    0.859259    800         25.441696   0.404041    1.450297    145.503914 
[37m[36mINFO[0m[0m 02/17 16:09:32 | 0.746002    0.768293    0.928205    0.882976    0.379487    1.000000    0.992933    0.854588    0.804143    0.746002    0.768293    0.930026    0.851852    1000        31.802120   0.371548    1.400775    141.721323 
[37m[36mINFO[0m[0m 02/17 16:16:50 | 0.729627    0.745427    0.925381    0.871986    0.493826    1.000000    0.992933    0.846118    0.781544    0.729627    0.745427    0.930026    0.841481    1200        38.162544   0.347867    1.439330    149.967699 
[37m[36mINFO[0m[0m 02/17 16:24:01 | 0.730008    0.757622    0.937019    0.878927    0.474595    1.000000    0.989399    0.864000    0.798493    0.730008    0.757622    0.947057    0.848889    1400        44.522968   0.327842    1.399972    150.308820 
[37m[36mINFO[0m[0m 02/17 16:31:09 | 0.740289    0.757622    0.956299    0.875301    0.543035    1.000000    0.992933    0.902588    0.800377    0.740289    0.757622    0.966309    0.832593    1600        50.883392   0.291263    1.413608    145.321414 
[37m[36mINFO[0m[0m 02/17 16:38:17 | 0.715918    0.730183    0.967708    0.876663    0.503917    1.000000    0.996466    0.929412    0.789077    0.715918    0.730183    0.973713    0.844444    1800        57.243816   0.263124    1.382215    152.398829 
[37m[36mINFO[0m[0m 02/17 16:45:26 | 0.714775    0.714939    0.969635    0.876288    0.492087    1.000000    0.992933    0.937412    0.800377    0.714775    0.714939    0.971492    0.835556    2000        63.604240   0.255010    1.411216    146.415559 
[37m[36mINFO[0m[0m 02/17 16:52:32 | 0.725057    0.748476    0.957765    0.861116    0.756476    1.000000    0.989399    0.911059    0.770245    0.725057    0.748476    0.962236    0.823704    2200        69.964664   0.238565    1.407185    143.954532 
[37m[36mINFO[0m[0m 02/17 16:59:32 | 0.695354    0.724085    0.963354    0.867866    0.812786    0.999117    0.996466    0.926118    0.783427    0.695354    0.724085    0.964828    0.823704    2400        76.325088   0.224277    1.385495    143.741000 
[37m[36mINFO[0m[0m 02/17 17:06:38 | 0.707159    0.717988    0.983780    0.879040    0.674115    1.000000    0.996466    0.968000    0.794727    0.707159    0.717988    0.983340    0.845926    2600        82.685512   0.230141    1.402288    145.535624 
[37m[36mINFO[0m[0m 02/17 17:13:28 | 0.685072    0.727134    0.979611    0.865179    0.906767    1.000000    0.996466    0.956235    0.766478    0.685072    0.727134    0.982599    0.832593    2800        89.045936   0.202835    1.390633    131.881582 
[37m[36mINFO[0m[0m 02/17 17:20:33 | 0.690404    0.693598    0.988128    0.858929    0.804034    1.000000    0.992933    0.978824    0.764595    0.690404    0.693598    0.985561    0.819259    3000        95.406360   0.184235    1.387606    147.378734 
[37m[36mINFO[0m[0m 02/17 17:27:29 | 0.705255    0.740854    0.990944    0.867294    0.729167    1.000000    1.000000    0.980235    0.779661    0.705255    0.740854    0.992595    0.822222    3200        101.766784  0.193156    1.369323    141.969432 
[37m[36mINFO[0m[0m 02/17 17:34:20 | 0.650800    0.673780    0.976390    0.849427    0.761152    1.000000    0.996466    0.959529    0.753296    0.650800    0.673780    0.969641    0.798519    3400        108.127208  0.180575    1.352850    140.435878 
[37m[36mINFO[0m[0m 02/17 17:41:19 | 0.679741    0.682927    0.958838    0.835305    0.925963    0.999117    0.985866    0.940706    0.736347    0.679741    0.682927    0.936690    0.783704    3600        114.487633  0.182477    1.411259    136.533411 
[37m[36mINFO[0m[0m 02/17 17:48:29 | 0.707921    0.727134    0.987950    0.863909    1.312688    1.000000    0.992933    0.974588    0.772128    0.707921    0.727134    0.989263    0.826667    3800        120.848057  0.177247    1.392291    151.554279 
[37m[36mINFO[0m[0m 02/17 17:55:32 | 0.686596    0.704268    0.987501    0.852503    1.222846    1.000000    0.989399    0.976941    0.753296    0.686596    0.704268    0.985561    0.814815    4000        127.208481  0.169053    1.413348    140.182709 
[37m[36mINFO[0m[0m 02/17 18:02:42 | 0.711348    0.716463    0.978387    0.863711    1.155498    0.996466    0.989399    0.959059    0.792844    0.711348    0.716463    0.979637    0.808889    4200        133.568905  0.155185    1.432199    143.438892 
