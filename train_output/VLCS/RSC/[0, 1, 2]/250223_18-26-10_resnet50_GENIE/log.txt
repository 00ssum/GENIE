[37m[36mINFO[0m[0m 02/23 18:26:10 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 1 2 --dataset VLCS --trial_seed 0 --hparams_seed 10
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 10
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/RSC/[0, 1, 2]/250223_18-26-10_resnet50_GENIE
	out_root: train_output/VLCS/RSC/[0, 1, 2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 2]
	trial_seed: 0
	unique_name: 250223_18-26-10_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 2.5980189819232268e-05
	batch_size: 22
	weight_decay: 1.2831747186887755e-05
	rsc_f_drop_factor: 0.49049801895660045
	rsc_b_drop_factor: 0.13734507450069044
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/23 18:26:10 | n_steps = 5001
[37m[36mINFO[0m[0m 02/23 18:26:10 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/23 18:26:10 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/23 18:26:10 | 
[37m[36mINFO[0m[0m 02/23 18:26:10 | Testenv name escaping te_C_L_S -> te_C_L_S
[37m[36mINFO[0m[0m 02/23 18:26:10 | Test envs = [0, 1, 2], name = te_C_L_S
[37m[36mINFO[0m[0m 02/23 18:26:10 | Train environments: [3], Test environments: [0, 1, 2]
[37m[36mINFO[0m[0m 02/23 18:26:10 | Batch sizes for each domain: [0, 0, 0, 22] (total=22)
[37m[36mINFO[0m[0m 02/23 18:26:10 | steps-per-epoch for each domain: 122.77 -> min = 122.77
[37m[36mINFO[0m[0m 02/23 18:26:12 | # of params = 23518277
[37m[36mINFO[0m[0m 02/23 18:28:27 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/23 18:28:27 | 0.490130    0.509906    0.447242    0.474074    1.391621    0.599823    0.625442    0.459294    0.489642    0.411272    0.414634    0.447242    0.474074    0           0.000000    2.514988    2.603652    131.960587 
[37m[36mINFO[0m[0m 02/23 18:31:26 | 0.779057    0.766986    0.841170    0.822222    0.517858    0.942580    0.936396    0.605176    0.593220    0.789414    0.771341    0.841170    0.822222    200         1.629026    1.023931    0.235980    132.094564 
[37m[36mINFO[0m[0m 02/23 18:34:26 | 0.737332    0.728804    0.802666    0.761481    0.649980    0.977915    0.985866    0.522353    0.508475    0.711729    0.692073    0.802666    0.761481    400         3.258053    0.674781    0.199835    140.135556 
[37m[36mINFO[0m[0m 02/23 18:37:26 | 0.757588    0.754016    0.873380    0.829630    0.568208    0.977915    0.989399    0.569412    0.566855    0.725438    0.705793    0.873380    0.829630    600         4.887079    0.626595    0.193301    141.146307 
[37m[36mINFO[0m[0m 02/23 18:40:26 | 0.730001    0.720579    0.849685    0.783704    0.597555    0.919611    0.922261    0.634824    0.632768    0.635567    0.606707    0.849685    0.783704    800         6.516105    0.568754    0.186154    142.313393 
[37m[36mINFO[0m[0m 02/23 18:43:17 | 0.749917    0.728452    0.924102    0.840000    0.503390    0.988516    0.978799    0.528941    0.516008    0.732292    0.690549    0.924102    0.840000    1000        8.145131    0.544454    0.181738    135.264325 
[37m[36mINFO[0m[0m 02/23 18:46:17 | 0.790699    0.783739    0.934839    0.842963    0.483490    0.980565    0.975265    0.631059    0.629002    0.760472    0.746951    0.934839    0.842963    1200        9.774158    0.477190    0.203005    139.274144 
[37m[36mINFO[0m[0m 02/23 18:49:12 | 0.729943    0.731176    0.870048    0.771852    0.642541    0.868375    0.883392    0.607059    0.619586    0.714395    0.690549    0.870048    0.771852    1400        11.403184   0.458099    0.176969    139.674121 
[37m[36mINFO[0m[0m 02/23 18:52:08 | 0.772248    0.765393    0.932247    0.835556    0.656005    0.981449    0.985866    0.603765    0.604520    0.731531    0.705793    0.932247    0.835556    1600        13.032210   0.444763    0.199662    135.741338 
[37m[36mINFO[0m[0m 02/23 18:55:04 | 0.759916    0.754098    0.958534    0.844444    0.523077    0.971731    0.978799    0.558588    0.551789    0.749429    0.731707    0.958534    0.844444    1800        14.661237   0.407715    0.189121    138.518244 
[37m[36mINFO[0m[0m 02/23 18:57:54 | 0.756709    0.748556    0.969641    0.835556    0.527510    0.958481    0.957597    0.557647    0.551789    0.753998    0.736280    0.969641    0.835556    2000        16.290263   0.336151    0.173255    135.341525 
[37m[36mINFO[0m[0m 02/23 19:00:43 | 0.789605    0.770252    0.967790    0.838519    0.618735    0.971731    0.964664    0.620235    0.608286    0.776847    0.737805    0.967790    0.838519    2200        17.919289   0.359019    0.174713    134.046307 
[37m[36mINFO[0m[0m 02/23 19:03:32 | 0.744976    0.737663    0.969641    0.820741    0.521483    0.960247    0.971731    0.543529    0.514124    0.731150    0.727134    0.969641    0.820741    2400        19.548315   0.337117    0.171257    134.105404 
[37m[36mINFO[0m[0m 02/23 19:06:26 | 0.773335    0.756430    0.970381    0.834074    0.773346    0.964664    0.961131    0.599059    0.593220    0.756283    0.714939    0.970381    0.834074    2600        21.177342   0.308870    0.210455    132.630789 
[37m[36mINFO[0m[0m 02/23 19:09:16 | 0.764950    0.754779    0.984820    0.811852    0.549511    0.971731    0.950530    0.569882    0.566855    0.753237    0.746951    0.984820    0.811852    2800        22.806368   0.296229    0.205539    128.936802 
[37m[36mINFO[0m[0m 02/23 19:12:13 | 0.741793    0.729910    0.935579    0.795556    0.753939    0.973498    0.968198    0.555765    0.538606    0.696116    0.682927    0.935579    0.795556    3000        24.435394   0.306483    0.228374    130.494541 
[37m[36mINFO[0m[0m 02/23 19:15:07 | 0.756238    0.744717    0.987782    0.837037    0.591248    0.964664    0.954064    0.555765    0.549906    0.748286    0.730183    0.987782    0.837037    3200        26.064421   0.328061    0.221909    130.292100 
