[37m[36mINFO[0m[0m 02/11 07:42:56 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 --dataset VLCS --trial_seed 1 --hparams_seed 0
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/RSC/[0]/250211_07-42-56_resnet50_GENIE
	out_root: train_output/VLCS/RSC/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 1
	unique_name: 250211_07-42-56_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	rsc_f_drop_factor: 0.3333333333333333
	rsc_b_drop_factor: 0.3333333333333333
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/11 07:42:56 | n_steps = 5001
[37m[36mINFO[0m[0m 02/11 07:42:56 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/11 07:42:56 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/11 07:42:56 | 
[37m[36mINFO[0m[0m 02/11 07:42:56 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 02/11 07:42:56 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 02/11 07:42:56 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 02/11 07:42:56 | Batch sizes for each domain: [0, 32, 32, 32] (total=96)
[37m[36mINFO[0m[0m 02/11 07:42:56 | steps-per-epoch for each domain: 66.41, 82.06, 84.41 -> min = 66.41
[37m[36mINFO[0m[0m 02/11 07:42:57 | # of params = 23518277
[37m[36mINFO[0m[0m 02/11 07:45:26 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/11 07:45:26 | 0.613958    0.618375    0.432326    0.428842    1.342751    0.613958    0.618375    0.466824    0.461394    0.376618    0.419207    0.453536    0.405926    0           0.000000    3.194570    1.671393    146.667333 
[37m[36mINFO[0m[0m 02/11 07:51:53 | 0.823322    0.802120    0.656016    0.665638    0.864480    0.823322    0.802120    0.648471    0.627119    0.642422    0.716463    0.677157    0.653333    200         3.011765    1.249331    1.231955    141.281263 
[37m[36mINFO[0m[0m 02/11 07:58:37 | 0.900177    0.897527    0.754082    0.736320    0.707833    0.900177    0.897527    0.721882    0.711864    0.766946    0.765244    0.773417    0.731852    400         6.023529    1.058519    1.278268    148.326877 
[37m[36mINFO[0m[0m 02/11 08:05:00 | 0.962898    0.957597    0.783040    0.765984    0.645482    0.962898    0.957597    0.699294    0.693032    0.815689    0.806402    0.834136    0.798519    600         9.035294    0.954334    1.221279    138.492414 
[37m[36mINFO[0m[0m 02/11 08:11:36 | 0.964664    0.971731    0.812622    0.787290    0.609441    0.964664    0.971731    0.740706    0.719397    0.840442    0.824695    0.856720    0.817778    800         12.047059   0.908455    1.220891    151.975224 
[37m[36mINFO[0m[0m 02/11 08:18:12 | 0.975265    0.978799    0.834174    0.808683    0.605992    0.975265    0.978799    0.772706    0.738230    0.843107    0.856707    0.886709    0.831111    1000        15.058824   0.852417    1.261339    143.358111 
[37m[36mINFO[0m[0m 02/11 08:24:45 | 0.974382    0.975265    0.833740    0.785587    0.650595    0.974382    0.975265    0.770824    0.732580    0.850724    0.806402    0.879674    0.817778    1200        18.070588   0.802426    1.250986    143.194307 
[37m[36mINFO[0m[0m 02/11 08:31:20 | 0.968198    0.975265    0.833753    0.780256    0.653417    0.968198    0.975265    0.728941    0.698682    0.867098    0.810976    0.905220    0.831111    1400        21.082353   0.788733    1.265741    141.577768 
[37m[36mINFO[0m[0m 02/11 08:37:50 | 0.968198    0.964664    0.852737    0.791476    0.704650    0.968198    0.964664    0.810824    0.753296    0.849200    0.803354    0.898186    0.817778    1600        24.094118   0.714887    1.230364    144.026363 
[37m[36mINFO[0m[0m 02/11 08:44:30 | 0.960247    0.971731    0.873256    0.790472    0.707633    0.960247    0.971731    0.836235    0.768362    0.870906    0.792683    0.912625    0.810370    1800        27.105882   0.661595    1.259104    148.515801 
[37m[36mINFO[0m[0m 02/11 08:51:01 | 0.962898    0.968198    0.879046    0.793927    0.905287    0.962898    0.968198    0.828706    0.736347    0.887662    0.824695    0.920770    0.820741    2000        30.117647   0.629184    1.220167    146.715510 
[37m[36mINFO[0m[0m 02/11 08:57:25 | 0.962014    0.957597    0.887670    0.796592    0.939981    0.962014    0.957597    0.858353    0.747646    0.884996    0.812500    0.919659    0.829630    2200        33.129412   0.624527    1.203270    142.815265 
[37m[36mINFO[0m[0m 02/11 09:03:54 | 0.963781    0.968198    0.867695    0.762758    1.013296    0.963781    0.968198    0.831529    0.738230    0.853008    0.751524    0.918549    0.798519    2400        36.141176   0.570890    1.250666    139.044738 
[37m[36mINFO[0m[0m 02/11 09:10:29 | 0.975265    0.989399    0.888663    0.783301    0.907901    0.975265    0.989399    0.808941    0.728814    0.918507    0.801829    0.938541    0.819259    2600        39.152941   0.564968    1.229459    149.468294 
[37m[36mINFO[0m[0m 02/11 09:16:56 | 0.963781    0.954064    0.919543    0.788894    0.979546    0.963781    0.954064    0.895529    0.764595    0.923077    0.810976    0.940022    0.791111    2800        42.164706   0.542867    1.229060    140.751278 
[37m[36mINFO[0m[0m 02/11 09:23:22 | 0.944346    0.954064    0.914379    0.758251    0.925808    0.944346    0.954064    0.899765    0.736347    0.919269    0.759146    0.924102    0.779259    3000        45.176471   0.481504    1.222157    142.103662 
[37m[36mINFO[0m[0m 02/11 09:29:50 | 0.962014    0.975265    0.952176    0.795690    1.284252    0.962014    0.975265    0.932235    0.747646    0.953542    0.821646    0.970752    0.817778    3200        48.188235   0.476616    1.230318    141.366240 
[37m[36mINFO[0m[0m 02/11 09:36:23 | 0.968198    0.968198    0.935196    0.784385    1.058634    0.968198    0.968198    0.915294    0.751412    0.942498    0.798780    0.947797    0.802963    3400        51.200000   0.478720    1.228325    147.728293 
