[37m[36mINFO[0m[0m 02/20 16:18:21 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 2 3 --dataset VLCS --trial_seed 0 --hparams_seed 2
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 2
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/RSC/[0, 2, 3]/250220_16-18-21_resnet50_GENIE
	out_root: train_output/VLCS/RSC/[0, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 2, 3]
	trial_seed: 0
	unique_name: 250220_16-18-21_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.9041073434446342e-05
	batch_size: 9
	weight_decay: 0.0006566989842279891
	rsc_f_drop_factor: 0.2998869208594543
	rsc_b_drop_factor: 0.46590533469384005
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/20 16:18:21 | n_steps = 5001
[37m[36mINFO[0m[0m 02/20 16:18:21 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/20 16:18:21 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/20 16:18:21 | 
[37m[36mINFO[0m[0m 02/20 16:18:21 | Testenv name escaping te_C_S_V -> te_C_S_V
[37m[36mINFO[0m[0m 02/20 16:18:21 | Test envs = [0, 2, 3], name = te_C_S_V
[37m[36mINFO[0m[0m 02/20 16:18:21 | Train environments: [1], Test environments: [0, 2, 3]
[37m[36mINFO[0m[0m 02/20 16:18:21 | Batch sizes for each domain: [0, 9, 0, 0] (total=9)
[37m[36mINFO[0m[0m 02/20 16:18:21 | steps-per-epoch for each domain: 236.11 -> min = 236.11
[37m[36mINFO[0m[0m 02/20 16:18:22 | # of params = 23518277
[37m[36mINFO[0m[0m 02/20 16:20:39 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/20 16:20:39 | 0.477044    0.485010    0.459765    0.489642    1.137452    0.603357    0.621908    0.459765    0.489642    0.384235    0.387195    0.443539    0.445926    0           0.000000    3.253674    1.694830    135.164805 
[37m[36mINFO[0m[0m 02/20 16:24:15 | 0.479821    0.487365    0.459765    0.489642    0.989687    0.611307    0.628975    0.459765    0.489642    0.384615    0.387195    0.443539    0.445926    200         0.847059    1.218440    0.390962    137.527447 
[37m[36mINFO[0m[0m 02/20 16:27:52 | 0.473900    0.483811    0.568000    0.557439    0.983875    0.628092    0.643110    0.568000    0.557439    0.393755    0.400915    0.399852    0.407407    400         1.694118    1.091531    0.405742    136.152258 
[37m[36mINFO[0m[0m 02/20 16:31:33 | 0.232892    0.231315    0.528471    0.495292    0.990712    0.158127    0.151943    0.528471    0.495292    0.306931    0.307927    0.233617    0.234074    600         2.541176    1.039073    0.395540    141.359842 
[37m[36mINFO[0m[0m 02/20 16:35:10 | 0.197865    0.195658    0.469176    0.450094    0.934243    0.092756    0.077739    0.469176    0.450094    0.290175    0.301829    0.210663    0.207407    800         3.388235    1.055916    0.366698    144.192797 
[37m[36mINFO[0m[0m 02/20 16:38:54 | 0.455277    0.472444    0.576471    0.546139    0.887197    0.565371    0.586572    0.576471    0.546139    0.387281    0.408537    0.413180    0.422222    1000        4.235294    1.020824    0.362660    151.345438 
[37m[36mINFO[0m[0m 02/20 16:42:41 | 0.512268    0.512471    0.675294    0.689266    0.803693    0.627208    0.604240    0.675294    0.689266    0.450876    0.467988    0.458719    0.465185    1200        5.082353    0.959812    0.388219    149.437196 
[37m[36mINFO[0m[0m 02/20 16:46:33 | 0.391199    0.400585    0.691765    0.683616    0.782121    0.366608    0.356890    0.691765    0.683616    0.408987    0.435976    0.398001    0.408889    1400        5.929412    0.950446    0.394235    153.032724 
[37m[36mINFO[0m[0m 02/20 16:50:27 | 0.565614    0.567887    0.691294    0.698682    0.749140    0.696996    0.696113    0.691294    0.698682    0.474486    0.478659    0.525361    0.528889    1600        6.776471    0.922662    0.359390    161.454087 
[37m[36mINFO[0m[0m 02/20 16:54:24 | 0.498979    0.508898    0.699294    0.698682    0.743038    0.567138    0.572438    0.699294    0.698682    0.458492    0.480183    0.471307    0.474074    1800        7.623529    0.795365    0.365750    163.924024 
[37m[36mINFO[0m[0m 02/20 16:58:11 | 0.532044    0.528525    0.702118    0.693032    0.750433    0.671378    0.678445    0.702118    0.693032    0.450114    0.437500    0.474639    0.469630    2000        8.470588    0.921064    0.370154    153.332802 
[37m[36mINFO[0m[0m 02/20 17:02:09 | 0.504103    0.514272    0.582588    0.568738    0.852574    0.615724    0.632509    0.582588    0.568738    0.435644    0.445122    0.460940    0.465185    2200        9.317647    0.834213    0.401366    157.502154 
[37m[36mINFO[0m[0m 02/20 17:06:03 | 0.579405    0.579621    0.730353    0.726930    0.705339    0.700530    0.699647    0.730353    0.726930    0.490480    0.498476    0.547205    0.540741    2400        10.164706   0.832873    0.361292    162.212486 
[37m[36mINFO[0m[0m 02/20 17:09:43 | 0.394104    0.402130    0.728471    0.713748    0.676547    0.304770    0.307420    0.728471    0.713748    0.456588    0.463415    0.420955    0.435556    2600        11.011765   0.773564    0.341187    151.845197 
[37m[36mINFO[0m[0m 02/20 17:13:29 | 0.442481    0.459088    0.738353    0.730697    0.691179    0.340989    0.378092    0.738353    0.730697    0.498858    0.496951    0.487597    0.502222    2800        11.858824   0.751970    0.374470    150.364473 
[37m[36mINFO[0m[0m 02/20 17:17:20 | 0.486038    0.482865    0.738824    0.713748    0.691727    0.478799    0.459364    0.738824    0.713748    0.508378    0.512195    0.470937    0.477037    3000        12.705882   0.777087    0.417239    147.795623 
[37m[36mINFO[0m[0m 02/20 17:21:26 | 0.561341    0.571669    0.752000    0.751412    0.621220    0.631625    0.650177    0.752000    0.751412    0.513709    0.513720    0.538689    0.551111    3200        13.552941   0.712185    0.451771    155.652488 
[37m[36mINFO[0m[0m 02/20 17:25:12 | 0.559324    0.562376    0.760000    0.732580    0.665483    0.667845    0.653710    0.760000    0.732580    0.484768    0.503049    0.525361    0.530370    3400        14.400000   0.705520    0.386499    148.661109 
[37m[36mINFO[0m[0m 02/20 17:29:08 | 0.554184    0.549752    0.762824    0.757062    0.654483    0.631625    0.611307    0.762824    0.757062    0.487053    0.506098    0.543873    0.531852    3600        15.247059   0.767086    0.422452    151.414442 
[37m[36mINFO[0m[0m 02/20 17:32:53 | 0.431849    0.436788    0.757176    0.691149    0.688170    0.378092    0.360424    0.757176    0.691149    0.467251    0.484756    0.450204    0.465185    3800        16.094118   0.704329    0.387271    147.405820 
[37m[36mINFO[0m[0m 02/20 17:36:30 | 0.515634    0.509760    0.773647    0.721281    0.676158    0.562721    0.544170    0.773647    0.721281    0.470297    0.471037    0.513884    0.514074    4000        16.941176   0.681801    0.389293    139.061057 
[37m[36mINFO[0m[0m 02/20 17:40:07 | 0.525642    0.522802    0.783059    0.728814    0.709650    0.516784    0.491166    0.783059    0.728814    0.525895    0.533537    0.534247    0.543704    4200        17.788235   0.647142    0.346979    147.767222 
[37m[36mINFO[0m[0m 02/20 17:43:42 | 0.446470    0.434091    0.732706    0.672316    0.757195    0.396643    0.367491    0.732706    0.672316    0.485529    0.472561    0.457238    0.462222    4400        18.635294   0.639958    0.356888    143.476571 
