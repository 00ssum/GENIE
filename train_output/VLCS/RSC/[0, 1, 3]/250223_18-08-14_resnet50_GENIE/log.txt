[37m[36mINFO[0m[0m 02/23 18:08:14 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 1 3 --dataset VLCS --trial_seed 1 --hparams_seed 13
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 13
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/RSC/[0, 1, 3]/250223_18-08-14_resnet50_GENIE
	out_root: train_output/VLCS/RSC/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 1
	unique_name: 250223_18-08-14_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 6.984990754803203e-05
	batch_size: 13
	weight_decay: 2.888048423418205e-06
	rsc_f_drop_factor: 0.4658647922146352
	rsc_b_drop_factor: 0.2950956246651242
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/23 18:08:14 | n_steps = 5001
[37m[36mINFO[0m[0m 02/23 18:08:14 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/23 18:08:14 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/23 18:08:14 | 
[37m[36mINFO[0m[0m 02/23 18:08:14 | Testenv name escaping te_C_L_V -> te_C_L_V
[37m[36mINFO[0m[0m 02/23 18:08:14 | Test envs = [0, 1, 3], name = te_C_L_V
[37m[36mINFO[0m[0m 02/23 18:08:14 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/23 18:08:14 | Batch sizes for each domain: [0, 0, 13, 0] (total=13)
[37m[36mINFO[0m[0m 02/23 18:08:14 | steps-per-epoch for each domain: 202.00 -> min = 202.00
[37m[36mINFO[0m[0m 02/23 18:08:16 | # of params = 23518277
[37m[36mINFO[0m[0m 02/23 18:10:27 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/23 18:10:27 | 0.123077    0.139088    0.343107    0.346037    1.285610    0.088339    0.095406    0.138353    0.133710    0.343107    0.346037    0.142540    0.188148    0           0.000000    2.985180    1.696663    129.064985 
[37m[36mINFO[0m[0m 02/23 18:13:06 | 0.523281    0.518934    0.779893    0.785061    0.566100    0.351590    0.363958    0.603294    0.570621    0.779893    0.785061    0.614957    0.622222    200         0.990099    1.180795    0.135253    132.434886 
[37m[36mINFO[0m[0m 02/23 18:15:45 | 0.622535    0.612713    0.811881    0.803354    0.532332    0.644876    0.643110    0.577412    0.546139    0.811881    0.803354    0.645317    0.648889    400         1.980198    1.003259    0.136804    131.018829 
[37m[36mINFO[0m[0m 02/23 18:18:22 | 0.604160    0.602910    0.792460    0.826220    0.497608    0.529152    0.530035    0.642824    0.632768    0.792460    0.826220    0.640504    0.645926    600         2.970297    0.942115    0.131344    130.898334 
[37m[36mINFO[0m[0m 02/23 18:20:59 | 0.641801    0.632996    0.826733    0.803354    0.554187    0.674912    0.664311    0.632941    0.613936    0.826733    0.803354    0.617549    0.620741    800         3.960396    0.903645    0.131957    131.236570 
[37m[36mINFO[0m[0m 02/23 18:23:42 | 0.524831    0.513009    0.840061    0.818598    0.503536    0.404594    0.413428    0.617882    0.602637    0.840061    0.818598    0.552018    0.522963    1000        4.950495    0.837889    0.133483    135.445194 
[37m[36mINFO[0m[0m 02/23 18:26:25 | 0.373476    0.368844    0.734958    0.647866    0.662594    0.180212    0.159011    0.520000    0.517891    0.734958    0.647866    0.420215    0.429630    1200        5.940594    0.799325    0.132672    136.418749 
[37m[36mINFO[0m[0m 02/23 18:29:21 | 0.636862    0.619786    0.863290    0.835366    0.482491    0.694346    0.667845    0.571294    0.557439    0.863290    0.835366    0.644946    0.634074    1400        6.930693    0.798042    0.135132    148.956696 
[37m[36mINFO[0m[0m 02/23 18:32:17 | 0.542284    0.531482    0.851866    0.803354    0.587524    0.428445    0.438163    0.645647    0.621469    0.851866    0.803354    0.552758    0.534815    1600        7.920792    0.714345    0.155304    145.251029 
[37m[36mINFO[0m[0m 02/23 18:35:03 | 0.519122    0.515038    0.785986    0.766768    0.659615    0.359541    0.356890    0.599529    0.604520    0.785986    0.766768    0.598297    0.583704    1800        8.910891    0.736629    0.165054    133.373105 
[37m[36mINFO[0m[0m 02/23 18:38:04 | 0.593073    0.576627    0.825209    0.785061    0.565809    0.633392    0.583039    0.522353    0.521657    0.825209    0.785061    0.623473    0.625185    2000        9.900990    0.756015    0.174339    146.136430 
[37m[36mINFO[0m[0m 02/23 18:40:59 | 0.508238    0.494119    0.851485    0.795732    0.572947    0.326855    0.321555    0.601412    0.559322    0.851485    0.795732    0.596446    0.601481    2200        10.891089   0.695135    0.160313    142.572656 
[37m[36mINFO[0m[0m 02/23 18:43:58 | 0.641370    0.618026    0.672506    0.696646    1.270955    0.704064    0.696113    0.594353    0.566855    0.672506    0.696646    0.625694    0.591111    2400        11.881188   0.679655    0.159716    147.359653 
[37m[36mINFO[0m[0m 02/23 18:46:55 | 0.478485    0.465356    0.892232    0.823171    0.620476    0.246466    0.226148    0.613647    0.595104    0.892232    0.823171    0.575342    0.574815    2600        12.871287   0.611089    0.134187    149.759317 
[37m[36mINFO[0m[0m 02/23 18:49:52 | 0.480951    0.471339    0.897182    0.804878    0.604917    0.231449    0.215548    0.614588    0.596987    0.897182    0.804878    0.596816    0.601481    2800        13.861386   0.575279    0.138696    149.900010 
[37m[36mINFO[0m[0m 02/23 18:52:45 | 0.465239    0.442862    0.860244    0.804878    0.550235    0.218198    0.180212    0.608471    0.591337    0.860244    0.804878    0.569049    0.557037    3000        14.851485   0.638543    0.145665    143.498867 
[37m[36mINFO[0m[0m 02/23 18:55:31 | 0.404579    0.395567    0.819117    0.722561    0.630354    0.183746    0.162544    0.536471    0.516008    0.819117    0.722561    0.493521    0.508148    3200        15.841584   0.649937    0.134753    138.849969 
[37m[36mINFO[0m[0m 02/23 18:58:24 | 0.481512    0.482382    0.896420    0.792683    0.837257    0.273852    0.261484    0.609412    0.606403    0.896420    0.792683    0.561274    0.579259    3400        16.831683   0.623618    0.145260    143.975017 
[37m[36mINFO[0m[0m 02/23 19:01:13 | 0.533994    0.531007    0.889947    0.807927    0.566759    0.385159    0.409894    0.589647    0.587571    0.889947    0.807927    0.627175    0.595556    3600        17.821782   0.472968    0.140534    141.132348 
[37m[36mINFO[0m[0m 02/23 19:04:03 | 0.478482    0.457729    0.889185    0.769817    0.705844    0.275618    0.215548    0.624471    0.613936    0.889185    0.769817    0.535357    0.543704    3800        18.811881   0.477995    0.156482    138.400483 
[37m[36mINFO[0m[0m 02/23 19:06:52 | 0.604794    0.588152    0.792841    0.762195    0.786837    0.538869    0.519435    0.646118    0.634652    0.792841    0.762195    0.629397    0.610370    4000        19.801980   0.472783    0.152388    138.962034 
[37m[36mINFO[0m[0m 02/23 19:09:40 | 0.558378    0.562895    0.899848    0.821646    0.638471    0.440813    0.462898    0.617882    0.613936    0.899848    0.821646    0.616438    0.611852    4200        20.792079   0.521548    0.162898    134.818804 
[37m[36mINFO[0m[0m 02/23 19:12:33 | 0.480160    0.474927    0.873572    0.772866    0.716159    0.256184    0.257951    0.598588    0.587571    0.873572    0.772866    0.585709    0.579259    4400        21.782178   0.583020    0.192035    134.827540 
[37m[36mINFO[0m[0m 02/23 19:15:26 | 0.551643    0.543830    0.913938    0.795732    0.672931    0.440813    0.434629    0.599529    0.589454    0.913938    0.795732    0.614587    0.607407    4600        22.772277   0.552562    0.200352    133.017348 
