[37m[36mINFO[0m[0m 02/23 17:59:28 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm RSC --test_envs 0 1 3 --dataset VLCS --trial_seed 2 --hparams_seed 20
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: RSC
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 20
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/RSC/[0, 1, 3]/250223_17-59-28_resnet50_GENIE
	out_root: train_output/VLCS/RSC/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 2
	unique_name: 250223_17-59-28_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 3.0575564760860844e-05
	batch_size: 27
	weight_decay: 7.45235722180653e-06
	rsc_f_drop_factor: 0.29037470564885265
	rsc_b_drop_factor: 0.006830956171108737
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/23 17:59:28 | n_steps = 5001
[37m[36mINFO[0m[0m 02/23 17:59:28 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/23 17:59:28 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/23 17:59:28 | 
[37m[36mINFO[0m[0m 02/23 17:59:28 | Testenv name escaping te_C_L_V -> te_C_L_V
[37m[36mINFO[0m[0m 02/23 17:59:28 | Test envs = [0, 1, 3], name = te_C_L_V
[37m[36mINFO[0m[0m 02/23 17:59:28 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/23 17:59:28 | Batch sizes for each domain: [0, 0, 27, 0] (total=27)
[37m[36mINFO[0m[0m 02/23 17:59:28 | steps-per-epoch for each domain: 97.26 -> min = 97.26
[37m[36mINFO[0m[0m 02/23 17:59:30 | # of params = 23518277
[37m[36mINFO[0m[0m 02/23 18:01:45 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 02/23 18:01:45 | 0.387425    0.399174    0.484387    0.515244    1.099527    0.259717    0.265018    0.464941    0.461394    0.484387    0.515244    0.437616    0.471111    0           0.000000    1.778983    3.406878    131.875277 
[37m[36mINFO[0m[0m 02/23 18:04:50 | 0.647459    0.659059    0.840823    0.798780    0.547083    0.720848    0.717314    0.594353    0.613936    0.840823    0.798780    0.627175    0.645926    200         2.056359    0.607124    0.258323    133.342355 
[37m[36mINFO[0m[0m 02/23 18:07:57 | 0.675147    0.673893    0.869764    0.801829    0.527602    0.734099    0.713781    0.640471    0.651601    0.869764    0.801829    0.650870    0.656296    400         4.112719    0.494599    0.265836    133.705052 
[37m[36mINFO[0m[0m 02/23 18:10:54 | 0.666516    0.676178    0.853008    0.801829    0.654279    0.749117    0.738516    0.601412    0.623352    0.853008    0.801829    0.649019    0.666667    600         6.169078    0.426072    0.264372    124.231010 
[37m[36mINFO[0m[0m 02/23 18:13:51 | 0.590921    0.606696    0.895278    0.809451    0.564871    0.550353    0.551237    0.614118    0.642185    0.895278    0.809451    0.608293    0.626667    800         8.225438    0.373597    0.257873    125.658223 
[37m[36mINFO[0m[0m 02/23 18:16:50 | 0.573603    0.586160    0.893755    0.803354    0.576082    0.578622    0.597173    0.576471    0.589454    0.893755    0.803354    0.565716    0.571852    1000        10.281797   0.298662    0.249975    128.903174 
[37m[36mINFO[0m[0m 02/23 18:19:42 | 0.652330    0.659075    0.927266    0.801829    0.612441    0.675795    0.653710    0.625882    0.655367    0.927266    0.801829    0.655313    0.668148    1200        12.338157   0.270247    0.233625    125.435186 
[37m[36mINFO[0m[0m 02/23 18:22:38 | 0.600052    0.630970    0.956969    0.830793    0.596101    0.522085    0.561837    0.621647    0.642185    0.956969    0.830793    0.656424    0.688889    1400        14.394516   0.256242    0.236682    128.188947 
[37m[36mINFO[0m[0m 02/23 18:25:34 | 0.554902    0.570936    0.952780    0.817073    0.652588    0.432862    0.466431    0.608000    0.627119    0.952780    0.817073    0.623843    0.619259    1600        16.450876   0.211053    0.243048    127.768028 
[37m[36mINFO[0m[0m 02/23 18:28:34 | 0.636153    0.638499    0.952780    0.791159    0.920797    0.685512    0.671378    0.590588    0.602637    0.952780    0.791159    0.632358    0.641481    1800        18.507235   0.152741    0.254236    128.755727 
[37m[36mINFO[0m[0m 02/23 18:31:54 | 0.522404    0.543288    0.949353    0.809451    0.713796    0.333922    0.367491    0.633882    0.653484    0.949353    0.809451    0.599408    0.608889    2000        20.563595   0.176206    0.326473    134.661169 
[37m[36mINFO[0m[0m 02/23 18:35:08 | 0.585654    0.585787    0.984006    0.800305    0.710823    0.484099    0.462898    0.614588    0.623352    0.984006    0.800305    0.658275    0.671111    2200        22.619954   0.145399    0.269729    140.499301 
[37m[36mINFO[0m[0m 02/23 18:38:14 | 0.594501    0.598069    0.969535    0.809451    0.922017    0.587456    0.586572    0.570353    0.591337    0.969535    0.809451    0.625694    0.616296    2400        24.676314   0.121950    0.243699    137.171277 
[37m[36mINFO[0m[0m 02/23 18:41:22 | 0.599949    0.601077    0.980960    0.812500    0.797238    0.544170    0.530035    0.605176    0.613936    0.980960    0.812500    0.650500    0.659259    2600        26.732673   0.127119    0.247664    137.961186 
[37m[36mINFO[0m[0m 02/23 18:44:29 | 0.467811    0.485677    0.976390    0.803354    0.871436    0.273852    0.279152    0.564235    0.583804    0.976390    0.803354    0.565346    0.594074    2800        28.789033   0.137290    0.244494    138.337884 
[37m[36mINFO[0m[0m 02/23 18:47:40 | 0.494577    0.513617    0.983244    0.820122    0.938117    0.221731    0.250883    0.634824    0.664783    0.983244    0.820122    0.627175    0.625185    3000        30.845392   0.107844    0.253562    139.973169 
[37m[36mINFO[0m[0m 02/23 18:50:53 | 0.468086    0.507479    0.972201    0.803354    0.914354    0.251767    0.307420    0.572706    0.612053    0.972201    0.803354    0.579785    0.602963    3200        32.901752   0.113354    0.261251    141.233945 
[37m[36mINFO[0m[0m 02/23 18:53:57 | 0.509435    0.528522    0.961158    0.797256    0.774303    0.277385    0.293286    0.614118    0.647834    0.961158    0.797256    0.636801    0.644444    3400        34.958111   0.099204    0.245321    134.791760 
[37m[36mINFO[0m[0m 02/23 18:56:59 | 0.625314    0.627186    0.982483    0.804878    0.847667    0.673145    0.653710    0.581176    0.581921    0.982483    0.804878    0.621622    0.645926    3600        37.014471   0.081935    0.241369    133.771101 
[37m[36mINFO[0m[0m 02/23 18:59:59 | 0.599831    0.621913    0.986672    0.826220    1.212418    0.496466    0.537102    0.636235    0.651601    0.986672    0.826220    0.666790    0.677037    3800        39.070830   0.127019    0.230707    133.578215 
[37m[36mINFO[0m[0m 02/23 19:03:02 | 0.471041    0.502568    0.977152    0.804878    1.121767    0.221731    0.265018    0.630118    0.672316    0.977152    0.804878    0.561274    0.570370    4000        41.127190   0.076141    0.233201    136.044482 
[37m[36mINFO[0m[0m 02/23 19:06:05 | 0.555508    0.568891    0.983625    0.798780    0.931477    0.471731    0.484099    0.578353    0.598870    0.983625    0.798780    0.616438    0.623704    4200        43.183549   0.073002    0.260188    131.302588 
[37m[36mINFO[0m[0m 02/23 19:09:07 | 0.615706    0.632540    0.995050    0.812500    1.348166    0.630742    0.618375    0.596235    0.621469    0.995050    0.812500    0.620141    0.657778    4400        45.239909   0.059007    0.256255    130.950607 
[37m[36mINFO[0m[0m 02/23 19:12:15 | 0.551952    0.571775    0.995050    0.814024    1.239805    0.469965    0.515901    0.588706    0.587571    0.995050    0.814024    0.597186    0.611852    4600        47.296268   0.060533    0.298913    128.259269 
[37m[36mINFO[0m[0m 02/23 19:15:22 | 0.584891    0.609850    0.982864    0.803354    1.415385    0.526502    0.540636    0.609882    0.645951    0.982864    0.803354    0.618290    0.642963    4800        49.352628   0.066357    0.285372    130.179952 
