[37m[36mINFO[0m[0m 03/15 09:54:10 | Command :: /jsm0707/GENIE/train_all.py B_VLCS2_adam config/resnet50_adam.yaml --trial_seed 2 --hparams_seed 10 --algorithm GENIE --test_envs 2 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 10
	in_domain: False
	model_save: None
	mpa: False
	name: B_VLCS2_adam
	out_dir: train_output/VLCS/GENIE/[2]/250315_09-54-10_B_VLCS2_adam
	out_root: train_output/VLCS/GENIE/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 2
	unique_name: 250315_09-54-10_B_VLCS2_adam
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 7.494887873901297e-05
	batch_size: 23
	weight_decay: 0.000495139494108363
	momentum: 0.9161886820913123
	convergence_rate: 0.003714112431183281
	moving_avg: 0.9624846363789884
	p: 0.14674963900384252
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/15 09:54:10 | n_steps = 5001
[37m[36mINFO[0m[0m 03/15 09:54:10 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/15 09:54:10 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/15 09:54:10 | 
[37m[36mINFO[0m[0m 03/15 09:54:10 | Testenv name escaping te_S -> te_S
[37m[36mINFO[0m[0m 03/15 09:54:10 | Test envs = [2], name = te_S
[37m[36mINFO[0m[0m 03/15 09:54:10 | Train environments: [0, 1, 3], Test environments: [2]
[37m[36mINFO[0m[0m 03/15 09:54:10 | Batch sizes for each domain: [23, 23, 0, 23] (total=69)
[37m[36mINFO[0m[0m 03/15 09:54:10 | steps-per-epoch for each domain: 49.22, 92.39, 117.43 -> min = 49.22
[37m[36mINFO[0m[0m 03/15 09:54:11 | # of params = 23518277
[37m[36mINFO[0m[0m 03/15 09:56:21 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/15 09:56:21 | 0.366717    0.399390    0.282072    0.307310    1.562365    0.146643    0.155477    0.408941    0.453861    0.366717    0.399390    0.290633    0.312593    0           0.000000    1.820274    1.157467    128.653931 
[37m[36mINFO[0m[0m 03/15 10:01:16 | 0.774943    0.748476    0.857624    0.862591    0.352272    1.000000    1.000000    0.728000    0.755179    0.774943    0.748476    0.844872    0.832593    200         4.063604    0.555898    0.814223    132.225039 
[37m[36mINFO[0m[0m 03/15 10:06:11 | 0.736101    0.716463    0.878787    0.879732    0.315063    1.000000    1.000000    0.772235    0.774011    0.736101    0.716463    0.864124    0.865185    400         8.127208    0.354583    0.820149    130.404511 
[37m[36mINFO[0m[0m 03/15 10:11:06 | 0.744097    0.730183    0.889617    0.881122    0.306174    1.000000    1.000000    0.789176    0.779661    0.744097    0.730183    0.879674    0.863704    600         12.190813   0.330124    0.818077    131.837130 
[37m[36mINFO[0m[0m 03/15 10:15:51 | 0.736101    0.730183    0.892458    0.880402    0.313102    1.000000    1.000000    0.784000    0.783427    0.736101    0.730183    0.893373    0.857778    800         16.254417   0.306142    0.775561    129.922466 
[37m[36mINFO[0m[0m 03/15 10:20:37 | 0.749429    0.739329    0.897791    0.882017    0.311323    1.000000    1.000000    0.800000    0.785311    0.749429    0.739329    0.893373    0.860741    1000        20.318021   0.284495    0.787619    127.883499 
[37m[36mINFO[0m[0m 03/15 10:25:30 | 0.754760    0.742378    0.905658    0.883499    0.297740    0.999117    1.000000    0.814118    0.785311    0.754760    0.742378    0.903739    0.865185    1200        24.381625   0.267714    0.808010    131.531006 
[37m[36mINFO[0m[0m 03/15 10:30:15 | 0.768850    0.725610    0.911052    0.886539    0.299548    1.000000    0.996466    0.826824    0.789077    0.768850    0.725610    0.906331    0.874074    1400        28.445230   0.263482    0.778070    129.745152 
[37m[36mINFO[0m[0m 03/15 10:34:59 | 0.761995    0.756098    0.917536    0.880946    0.304004    1.000000    1.000000    0.827765    0.770245    0.761995    0.756098    0.924843    0.872593    1600        32.508834   0.239430    0.772664    129.329993 
[37m[36mINFO[0m[0m 03/15 10:39:44 | 0.757426    0.754573    0.921359    0.878144    0.306826    1.000000    0.996466    0.832941    0.789077    0.757426    0.754573    0.931137    0.848889    1800        36.572438   0.226714    0.783279    127.842381 
[37m[36mINFO[0m[0m 03/15 10:44:30 | 0.746002    0.724085    0.928711    0.880479    0.310211    1.000000    0.996466    0.851294    0.787194    0.746002    0.724085    0.934839    0.857778    2000        40.636042   0.213863    0.787664    128.988051 
[37m[36mINFO[0m[0m 03/15 10:49:12 | 0.739909    0.722561    0.935154    0.879506    0.336867    0.999117    1.000000    0.864471    0.777778    0.739909    0.722561    0.941873    0.860741    2200        44.699647   0.194331    0.776049    126.987661 
[37m[36mINFO[0m[0m 03/15 10:53:57 | 0.736481    0.727134    0.934494    0.878766    0.341951    1.000000    0.992933    0.864941    0.779661    0.736481    0.727134    0.938541    0.863704    2400        48.763251   0.182912    0.780404    128.471559 
[37m[36mINFO[0m[0m 03/15 10:58:43 | 0.742193    0.721037    0.938932    0.882856    0.346336    1.000000    0.996466    0.875294    0.792844    0.742193    0.721037    0.941503    0.859259    2600        52.826855   0.169136    0.781247    129.768072 
[37m[36mINFO[0m[0m 03/15 11:03:28 | 0.714775    0.698171    0.942252    0.870626    0.383766    1.000000    1.000000    0.878588    0.757062    0.714775    0.698171    0.948167    0.854815    2800        56.890459   0.153424    0.778694    129.330181 
[37m[36mINFO[0m[0m 03/15 11:08:12 | 0.757045    0.746951    0.942547    0.870709    0.398973    1.000000    1.000000    0.870588    0.772128    0.757045    0.746951    0.957053    0.840000    3000        60.954064   0.159576    0.773179    129.296511 
[37m[36mINFO[0m[0m 03/15 11:12:56 | 0.726961    0.705793    0.952113    0.876192    0.428465    1.000000    1.000000    0.904471    0.758945    0.726961    0.705793    0.951870    0.869630    3200        65.017668   0.132770    0.777012    129.208021 
[37m[36mINFO[0m[0m 03/15 11:17:44 | 0.688119    0.673780    0.951308    0.863388    0.397634    1.000000    0.996466    0.898353    0.755179    0.688119    0.673780    0.955572    0.838519    3400        69.081272   0.130648    0.794529    128.987407 
[37m[36mINFO[0m[0m 03/15 11:22:27 | 0.706398    0.687500    0.954399    0.875154    0.438481    1.000000    1.000000    0.910588    0.772128    0.706398    0.687500    0.952610    0.853333    3600        73.144876   0.126197    0.779723    126.801548 
[37m[36mINFO[0m[0m 03/15 11:27:15 | 0.741432    0.717988    0.967225    0.876100    0.406346    1.000000    1.000000    0.931294    0.764595    0.741432    0.717988    0.970381    0.863704    3800        77.208481   0.112617    0.788293    130.381467 
[37m[36mINFO[0m[0m 03/15 11:31:58 | 0.698781    0.664634    0.957096    0.877983    0.464031    1.000000    1.000000    0.901647    0.770245    0.698781    0.664634    0.969641    0.863704    4000        81.272085   0.097931    0.776190    127.491681 
[37m[36mINFO[0m[0m 03/15 11:36:46 | 0.726961    0.725610    0.972640    0.866259    0.475064    1.000000    0.996466    0.939765    0.760829    0.726961    0.725610    0.978156    0.841481    4200        85.335689   0.096025    0.783990    131.365505 
[37m[36mINFO[0m[0m 03/15 11:41:30 | 0.736481    0.699695    0.971078    0.875909    0.496151    1.000000    0.996466    0.947294    0.764595    0.736481    0.699695    0.965939    0.866667    4400        89.399293   0.089411    0.770582    129.965086 
[37m[36mINFO[0m[0m 03/15 11:46:15 | 0.739528    0.733232    0.973293    0.868954    0.453627    1.000000    0.996466    0.935059    0.757062    0.739528    0.733232    0.984820    0.853333    4600        93.462898   0.114193    0.775463    130.096260 
[37m[36mINFO[0m[0m 03/15 11:50:59 | 0.728865    0.733232    0.978660    0.874794    0.434868    1.000000    1.000000    0.951529    0.774011    0.728865    0.733232    0.984450    0.850370    4800        97.526502   0.082760    0.772945    129.138324 
[37m[36mINFO[0m[0m 03/15 11:55:44 | 0.710967    0.734756    0.979542    0.859889    0.531202    1.000000    0.996466    0.960471    0.747646    0.710967    0.734756    0.978156    0.835556    5000        101.590106  0.065954    0.771839    130.970002 
[37m[36mINFO[0m[0m 03/15 11:55:45 | Cumulative gradient change saved at train_output/VLCS/GENIE/[2]/250315_09-54-10_B_VLCS2_adam/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/15 11:55:46 | ---
[37m[36mINFO[0m[0m 03/15 11:55:46 | test-domain validation(oracle) = 76.200%
[37m[36mINFO[0m[0m 03/15 11:55:46 | training-domain validation(iid) = 76.885%
[37m[36mINFO[0m[0m 03/15 11:55:46 | last = 71.097%
[37m[36mINFO[0m[0m 03/15 11:55:46 | last (inD) = 85.989%
[37m[36mINFO[0m[0m 03/15 11:55:46 | training-domain validation (iid, inD) = 88.654%
[37m[36mINFO[0m[0m 03/15 11:55:46 | === Summary ===
[37m[36mINFO[0m[0m 03/15 11:55:46 | Command: /jsm0707/GENIE/train_all.py B_VLCS2_adam config/resnet50_adam.yaml --trial_seed 2 --hparams_seed 10 --algorithm GENIE --test_envs 2 --dataset VLCS
[37m[36mINFO[0m[0m 03/15 11:55:46 | Unique name: 250315_09-54-10_B_VLCS2_adam
[37m[36mINFO[0m[0m 03/15 11:55:46 | Out path: train_output/VLCS/GENIE/[2]/250315_09-54-10_B_VLCS2_adam
[37m[36mINFO[0m[0m 03/15 11:55:46 | Algorithm: GENIE
[37m[36mINFO[0m[0m 03/15 11:55:46 | Dataset: VLCS
