[37m[36mINFO[0m[0m 03/15 05:58:52 | Command :: /jsm0707/GENIE/train_all.py B_VLCS0_adam config/resnet50_adam.yaml --trial_seed 1 --hparams_seed 0 --algorithm GENIE --test_envs 0 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: B_VLCS0_adam
	out_dir: train_output/VLCS/GENIE/[0]/250315_05-58-52_B_VLCS0_adam
	out_root: train_output/VLCS/GENIE/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 1
	unique_name: 250315_05-58-52_B_VLCS0_adam
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	momentum: 0.9
	convergence_rate: 0.015
	moving_avg: 0.95
	p: 0.4
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/15 05:58:52 | n_steps = 5001
[37m[36mINFO[0m[0m 03/15 05:58:52 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/15 05:58:52 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/15 05:58:52 | 
[37m[36mINFO[0m[0m 03/15 05:58:52 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 03/15 05:58:52 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 03/15 05:58:52 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 03/15 05:58:52 | Batch sizes for each domain: [0, 32, 32, 32] (total=96)
[37m[36mINFO[0m[0m 03/15 05:58:52 | steps-per-epoch for each domain: 66.41, 82.06, 84.41 -> min = 66.41
[37m[36mINFO[0m[0m 03/15 05:58:53 | # of params = 23518277
[37m[36mINFO[0m[0m 03/15 06:01:11 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/15 06:01:11 | 0.613958    0.618375    0.432326    0.428842    1.449173    0.613958    0.618375    0.466824    0.461394    0.376618    0.419207    0.453536    0.405926    0           0.000000    1.781322    1.402170    136.575690 
[37m[36mINFO[0m[0m 03/15 06:07:15 | 0.980565    0.978799    0.817102    0.801276    0.541798    0.980565    0.978799    0.776941    0.751412    0.806169    0.809451    0.868197    0.842963    200         3.011765    0.621089    1.131690    137.336677 
[37m[36mINFO[0m[0m 03/15 06:13:13 | 0.975265    0.971731    0.828413    0.793921    0.533863    0.975265    0.971731    0.782588    0.755179    0.824829    0.786585    0.877823    0.840000    400         6.023529    0.460313    1.113766    135.443289 
[37m[36mINFO[0m[0m 03/15 06:19:12 | 0.987633    0.982332    0.855159    0.810948    0.535776    0.987633    0.982332    0.811765    0.760829    0.850343    0.821646    0.903369    0.850370    600         9.035294    0.385340    1.100916    138.640690 
[37m[36mINFO[0m[0m 03/15 06:25:12 | 0.994700    0.992933    0.885459    0.811832    0.536545    0.994700    0.992933    0.813176    0.751412    0.899848    0.829268    0.943354    0.854815    800         12.047059   0.337943    1.123111    135.953803 
[37m[36mINFO[0m[0m 03/15 06:31:17 | 0.983216    0.968198    0.892417    0.803265    0.584823    0.983216    0.968198    0.826824    0.751412    0.906702    0.810976    0.943725    0.847407    1000        15.058824   0.289261    1.131110    138.626391 
[37m[36mINFO[0m[0m 03/15 06:37:22 | 0.981449    0.975265    0.901841    0.800598    0.657346    0.981449    0.975265    0.886588    0.762712    0.879284    0.809451    0.939652    0.829630    1200        18.070588   0.261020    1.131197    138.892369 
[37m[36mINFO[0m[0m 03/15 06:43:24 | 0.977915    0.971731    0.907002    0.798136    0.620809    0.977915    0.971731    0.869647    0.749529    0.900228    0.804878    0.951129    0.840000    1400        21.082353   0.235503    1.119355    137.342470 
[37m[36mINFO[0m[0m 03/15 06:49:26 | 0.986749    0.978799    0.954028    0.797087    0.776731    0.986749    0.978799    0.926588    0.740113    0.956969    0.817073    0.978526    0.834074    1600        24.094118   0.171078    1.126295    137.436049 
[37m[36mINFO[0m[0m 03/15 06:55:26 | 0.986749    0.975265    0.952280    0.798163    0.755242    0.986749    0.975265    0.919059    0.757062    0.959254    0.803354    0.978526    0.834074    1800        27.105882   0.146227    1.133874    133.283666 
[37m[36mINFO[0m[0m 03/15 07:01:21 | 0.974382    0.964664    0.942681    0.793381    0.789812    0.974382    0.964664    0.929412    0.745763    0.936024    0.800305    0.962606    0.834074    2000        30.117647   0.144459    1.101632    133.941838 
[37m[36mINFO[0m[0m 03/15 07:07:22 | 0.986749    0.985866    0.971354    0.796041    0.701360    0.986749    0.985866    0.948706    0.747646    0.979056    0.806402    0.986301    0.834074    2200        33.129412   0.107406    1.112210    139.203980 
[37m[36mINFO[0m[0m 03/15 07:13:24 | 0.987633    0.975265    0.975512    0.794666    0.978493    0.987633    0.975265    0.964235    0.741996    0.975628    0.807927    0.986672    0.834074    2400        36.141176   0.091597    1.124303    137.156183 
[37m[36mINFO[0m[0m 03/15 07:19:19 | 0.975265    0.968198    0.978055    0.796106    0.994748    0.975265    0.968198    0.969412    0.753296    0.984006    0.823171    0.980748    0.811852    2600        39.152941   0.088414    1.100625    134.245899 
[37m[36mINFO[0m[0m 03/15 07:25:14 | 0.981449    0.961131    0.986511    0.791766    0.915514    0.981449    0.961131    0.980235    0.736347    0.987814    0.804878    0.991485    0.834074    2800        42.164706   0.062649    1.103141    134.872238 
[37m[36mINFO[0m[0m 03/15 07:31:11 | 0.979682    0.954064    0.970932    0.789868    0.864309    0.979682    0.954064    0.953882    0.738230    0.973724    0.798780    0.985191    0.832593    3000        45.176471   0.069484    1.114619    134.341565 
[37m[36mINFO[0m[0m 03/15 07:37:12 | 0.981449    0.971731    0.977912    0.794813    0.999528    0.981449    0.971731    0.973647    0.751412    0.976009    0.804878    0.984080    0.828148    3200        48.188235   0.053284    1.122999    136.379884 
[37m[36mINFO[0m[0m 03/15 07:43:13 | 0.974382    0.975265    0.987997    0.806716    0.972599    0.974382    0.975265    0.989176    0.757062    0.986291    0.820122    0.988523    0.842963    3400        51.200000   0.057482    1.130722    134.209652 
[37m[36mINFO[0m[0m 03/15 07:49:08 | 0.979682    0.982332    0.990687    0.793362    0.968028    0.979682    0.982332    0.992000    0.736347    0.988576    0.817073    0.991485    0.826667    3600        54.211765   0.039942    1.103158    134.899970 
[37m[36mINFO[0m[0m 03/15 07:55:05 | 0.967314    0.954064    0.983652    0.796324    1.028195    0.967314    0.954064    0.978353    0.743879    0.986672    0.812500    0.985931    0.832593    3800        57.223529   0.048232    1.121795    132.390546 
[37m[36mINFO[0m[0m 03/15 08:01:03 | 0.986749    0.975265    0.992145    0.802469    1.072041    0.986749    0.975265    0.991529    0.753296    0.989718    0.817073    0.995187    0.837037    4000        60.235294   0.040410    1.114834    134.850543 
[37m[36mINFO[0m[0m 03/15 08:06:57 | 0.969965    0.978799    0.989138    0.795777    1.123915    0.969965    0.978799    0.985882    0.753296    0.988195    0.788110    0.993336    0.845926    4200        63.247059   0.045241    1.096645    135.273649 
[37m[36mINFO[0m[0m 03/15 08:12:58 | 0.982332    0.975265    0.993337    0.791941    1.298321    0.982332    0.975265    0.990588    0.753296    0.992384    0.800305    0.997038    0.822222    4400        66.258824   0.028380    1.126860    135.227359 
[37m[36mINFO[0m[0m 03/15 08:18:55 | 0.985866    0.982332    0.990486    0.795634    1.394645    0.985866    0.982332    0.987294    0.740113    0.989718    0.820122    0.994447    0.826667    4600        69.270588   0.042910    1.107254    135.630402 
[37m[36mINFO[0m[0m 03/15 08:24:51 | 0.990283    0.971731    0.980363    0.786475    1.250050    0.990283    0.971731    0.968941    0.725047    0.983625    0.800305    0.988523    0.834074    4800        72.282353   0.033873    1.100857    135.854875 
[37m[36mINFO[0m[0m 03/15 08:30:50 | 0.990283    0.978799    0.991154    0.792523    1.305170    0.990283    0.978799    0.987765    0.738230    0.991622    0.818598    0.994076    0.820741    5000        75.294118   0.034818    1.118308    135.057153 
[37m[36mINFO[0m[0m 03/15 08:30:50 | Cumulative gradient change saved at train_output/VLCS/GENIE/[0]/250315_05-58-52_B_VLCS0_adam/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/15 08:30:51 | ---
[37m[36mINFO[0m[0m 03/15 08:30:51 | test-domain validation(oracle) = 99.470%
[37m[36mINFO[0m[0m 03/15 08:30:51 | training-domain validation(iid) = 99.470%
[37m[36mINFO[0m[0m 03/15 08:30:51 | last = 99.028%
[37m[36mINFO[0m[0m 03/15 08:30:51 | last (inD) = 79.252%
[37m[36mINFO[0m[0m 03/15 08:30:51 | training-domain validation (iid, inD) = 81.183%
[37m[36mINFO[0m[0m 03/15 08:30:52 | === Summary ===
[37m[36mINFO[0m[0m 03/15 08:30:52 | Command: /jsm0707/GENIE/train_all.py B_VLCS0_adam config/resnet50_adam.yaml --trial_seed 1 --hparams_seed 0 --algorithm GENIE --test_envs 0 --dataset VLCS
[37m[36mINFO[0m[0m 03/15 08:30:52 | Unique name: 250315_05-58-52_B_VLCS0_adam
[37m[36mINFO[0m[0m 03/15 08:30:52 | Out path: train_output/VLCS/GENIE/[0]/250315_05-58-52_B_VLCS0_adam
[37m[36mINFO[0m[0m 03/15 08:30:52 | Algorithm: GENIE
[37m[36mINFO[0m[0m 03/15 08:30:52 | Dataset: VLCS
