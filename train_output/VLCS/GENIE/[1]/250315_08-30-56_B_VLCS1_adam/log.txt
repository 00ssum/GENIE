[37m[36mINFO[0m[0m 03/15 08:30:56 | Command :: /jsm0707/GENIE/train_all.py B_VLCS1_adam config/resnet50_adam.yaml --trial_seed 1 --hparams_seed 2 --algorithm GENIE --test_envs 1 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 2
	in_domain: False
	model_save: None
	mpa: False
	name: B_VLCS1_adam
	out_dir: train_output/VLCS/GENIE/[1]/250315_08-30-56_B_VLCS1_adam
	out_root: train_output/VLCS/GENIE/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 1
	unique_name: 250315_08-30-56_B_VLCS1_adam
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 0.0001126313085293539
	batch_size: 38
	weight_decay: 0.006639128805224463
	momentum: 0.844808580961198
	convergence_rate: 0.0072584840023010066
	moving_avg: 0.9222220369334958
	p: 0.1472838851744452
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/15 08:30:56 | n_steps = 5001
[37m[36mINFO[0m[0m 03/15 08:30:56 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/15 08:30:56 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/15 08:30:56 | 
[37m[36mINFO[0m[0m 03/15 08:30:56 | Testenv name escaping te_L -> te_L
[37m[36mINFO[0m[0m 03/15 08:30:56 | Test envs = [1], name = te_L
[37m[36mINFO[0m[0m 03/15 08:30:56 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 03/15 08:30:56 | Batch sizes for each domain: [38, 0, 38, 38] (total=114)
[37m[36mINFO[0m[0m 03/15 08:30:56 | steps-per-epoch for each domain: 29.79, 69.11, 71.08 -> min = 29.79
[37m[36mINFO[0m[0m 03/15 08:30:58 | # of params = 23518277
[37m[36mINFO[0m[0m 03/15 08:33:11 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/15 08:33:11 | 0.467294    0.461394    0.499304    0.509453    1.334253    0.613958    0.618375    0.467294    0.461394    0.420792    0.486280    0.463162    0.423704    0           0.000000    1.863111    1.216211    131.944909 
[37m[36mINFO[0m[0m 03/15 08:36:22 | 0.693647    0.681733    0.895247    0.891033    0.307050    0.997350    0.996466    0.693647    0.681733    0.817974    0.827744    0.870418    0.848889    200         6.713781    0.411090    0.294599    131.681211 
[37m[36mINFO[0m[0m 03/15 08:39:33 | 0.635294    0.615819    0.917849    0.907587    0.273004    0.999117    0.996466    0.635294    0.615819    0.849581    0.855183    0.904850    0.871111    400         13.427562   0.249750    0.291517    132.947809 
[37m[36mINFO[0m[0m 03/15 08:42:54 | 0.648471    0.645951    0.931336    0.910882    0.253772    0.999117    0.992933    0.648471    0.645951    0.880046    0.858232    0.914846    0.881481    600         20.141343   0.225608    0.349077    130.759808 
[37m[36mINFO[0m[0m 03/15 08:46:04 | 0.685647    0.674200    0.934527    0.910925    0.287407    0.999117    0.992933    0.685647    0.674200    0.878142    0.862805    0.926324    0.877037    800         26.855124   0.190454    0.294775    130.907912 
[37m[36mINFO[0m[0m 03/15 08:49:17 | 0.595294    0.576271    0.947660    0.897453    0.276807    1.000000    0.996466    0.595294    0.576271    0.905179    0.827744    0.937801    0.868148    1000        33.568905   0.173498    0.300716    133.284911 
[37m[36mINFO[0m[0m 03/15 08:52:37 | 0.657882    0.659134    0.953686    0.905426    0.286668    1.000000    0.996466    0.657882    0.659134    0.916222    0.835366    0.944835    0.884444    1200        40.282686   0.143746    0.339657    132.303826 
[37m[36mINFO[0m[0m 03/15 08:55:50 | 0.677647    0.676083    0.958377    0.908171    0.313561    0.999117    1.000000    0.677647    0.676083    0.921554    0.844512    0.954461    0.880000    1400        46.996466   0.125522    0.292535    134.644916 
[37m[36mINFO[0m[0m 03/15 08:59:12 | 0.666353    0.661017    0.965650    0.905407    0.311145    1.000000    0.992933    0.666353    0.661017    0.928789    0.853659    0.968160    0.869630    1600        53.710247   0.109737    0.336294    134.181967 
[37m[36mINFO[0m[0m 03/15 09:02:28 | 0.651765    0.644068    0.978671    0.896113    0.320950    1.000000    0.989399    0.651765    0.644068    0.962300    0.830793    0.973713    0.868148    1800        60.424028   0.092265    0.310917    133.909920 
[37m[36mINFO[0m[0m 03/15 09:05:38 | 0.640000    0.630885    0.978802    0.891161    0.354824    1.000000    0.989399    0.640000    0.630885    0.963062    0.829268    0.973343    0.854815    2000        67.137809   0.078254    0.286735    132.930515 
[37m[36mINFO[0m[0m 03/15 09:08:50 | 0.664941    0.642185    0.980653    0.897857    0.393208    1.000000    0.992933    0.664941    0.642185    0.963062    0.838415    0.978897    0.862222    2200        73.851590   0.067240    0.293019    133.399539 
[37m[36mINFO[0m[0m 03/15 09:12:08 | 0.630588    0.610169    0.985445    0.891427    0.374989    1.000000    0.996466    0.630588    0.610169    0.974105    0.817073    0.982229    0.860741    2400        80.565371   0.066752    0.315939    134.770334 
[37m[36mINFO[0m[0m 03/15 09:15:23 | 0.653176    0.653484    0.985924    0.899281    0.418510    1.000000    0.992933    0.653176    0.653484    0.972582    0.832317    0.985191    0.872593    2600        87.279152   0.051388    0.319365    131.376897 
[37m[36mINFO[0m[0m 03/15 09:18:40 | 0.649882    0.625235    0.991314    0.894152    0.441578    0.999117    0.989399    0.649882    0.625235    0.986672    0.832317    0.988153    0.860741    2800        93.992933   0.044150    0.311790    134.409341 
[37m[36mINFO[0m[0m 03/15 09:21:57 | 0.652235    0.629002    0.990420    0.896346    0.463847    1.000000    0.992933    0.652235    0.629002    0.978294    0.835366    0.992966    0.860741    3000        100.706714  0.040703    0.323727    131.618591 
[37m[36mINFO[0m[0m 03/15 09:25:14 | 0.691294    0.664783    0.981305    0.884413    0.600785    1.000000    0.996466    0.691294    0.664783    0.966870    0.806402    0.977046    0.850370    3200        107.420495  0.033162    0.322684    132.512068 
[37m[36mINFO[0m[0m 03/15 09:28:31 | 0.636235    0.621469    0.991947    0.891802    0.495612    1.000000    0.992933    0.636235    0.621469    0.983244    0.824695    0.992595    0.857778    3400        114.134276  0.035592    0.324020    132.362354 
[37m[36mINFO[0m[0m 03/15 09:31:48 | 0.626353    0.610169    0.995219    0.891308    0.512160    1.000000    0.992933    0.626353    0.610169    0.990099    0.824695    0.995557    0.856296    3600        120.848057  0.028203    0.327752    131.420151 
[37m[36mINFO[0m[0m 03/15 09:35:05 | 0.625412    0.604520    0.989080    0.887419    0.522377    1.000000    0.996466    0.625412    0.604520    0.980198    0.810976    0.987042    0.854815    3800        127.561837  0.027090    0.311786    135.044786 
[37m[36mINFO[0m[0m 03/15 09:38:18 | 0.618824    0.617702    0.995222    0.892281    0.479047    1.000000    0.992933    0.618824    0.617702    0.990480    0.823171    0.995187    0.860741    4000        134.275618  0.024572    0.311834    130.226656 
[37m[36mINFO[0m[0m 03/15 09:41:28 | 0.661176    0.644068    0.996104    0.902866    0.518465    1.000000    0.992933    0.661176    0.644068    0.992384    0.846037    0.995927    0.869630    4200        140.989399  0.021932    0.302076    129.958330 
[37m[36mINFO[0m[0m 03/15 09:44:38 | 0.655529    0.634652    0.993346    0.892006    0.542667    1.000000    0.996466    0.655529    0.634652    0.987814    0.826220    0.992225    0.853333    4400        147.703180  0.019019    0.283866    132.931261 
[37m[36mINFO[0m[0m 03/15 09:47:48 | 0.659765    0.632768    0.988625    0.894647    0.648191    1.000000    0.996466    0.659765    0.632768    0.984387    0.844512    0.981488    0.842963    4600        154.416961  0.016427    0.289713    132.335922 
[37m[36mINFO[0m[0m 03/15 09:50:55 | 0.667765    0.659134    0.996858    0.900459    0.571388    1.000000    0.996466    0.667765    0.659134    0.993907    0.832317    0.996668    0.872593    4800        161.130742  0.020887    0.283323    130.035787 
[37m[36mINFO[0m[0m 03/15 09:54:04 | 0.595294    0.581921    0.994115    0.884717    0.551162    1.000000    0.992933    0.595294    0.581921    0.990861    0.806402    0.991485    0.854815    5000        167.844523  0.015959    0.297862    129.406083 
[37m[36mINFO[0m[0m 03/15 09:54:04 | Cumulative gradient change saved at train_output/VLCS/GENIE/[1]/250315_08-30-56_B_VLCS1_adam/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/15 09:54:05 | ---
[37m[36mINFO[0m[0m 03/15 09:54:05 | test-domain validation(oracle) = 69.365%
[37m[36mINFO[0m[0m 03/15 09:54:05 | training-domain validation(iid) = 68.565%
[37m[36mINFO[0m[0m 03/15 09:54:05 | last = 59.529%
[37m[36mINFO[0m[0m 03/15 09:54:05 | last (inD) = 88.472%
[37m[36mINFO[0m[0m 03/15 09:54:05 | training-domain validation (iid, inD) = 91.092%
[37m[36mINFO[0m[0m 03/15 09:54:05 | === Summary ===
[37m[36mINFO[0m[0m 03/15 09:54:05 | Command: /jsm0707/GENIE/train_all.py B_VLCS1_adam config/resnet50_adam.yaml --trial_seed 1 --hparams_seed 2 --algorithm GENIE --test_envs 1 --dataset VLCS
[37m[36mINFO[0m[0m 03/15 09:54:05 | Unique name: 250315_08-30-56_B_VLCS1_adam
[37m[36mINFO[0m[0m 03/15 09:54:05 | Out path: train_output/VLCS/GENIE/[1]/250315_08-30-56_B_VLCS1_adam
[37m[36mINFO[0m[0m 03/15 09:54:05 | Algorithm: GENIE
[37m[36mINFO[0m[0m 03/15 09:54:05 | Dataset: VLCS
