[37m[36mINFO[0m[0m 03/15 11:55:50 | Command :: /jsm0707/GENIE/train_all.py B_VLCS3_adam config/resnet50_adam.yaml --trial_seed 0 --hparams_seed 18 --algorithm GENIE --test_envs 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 18
	in_domain: False
	model_save: None
	mpa: False
	name: B_VLCS3_adam
	out_dir: train_output/VLCS/GENIE/[3]/250315_11-55-50_B_VLCS3_adam
	out_root: train_output/VLCS/GENIE/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250315_11-55-50_B_VLCS3_adam
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 0.00021894640154701087
	batch_size: 13
	weight_decay: 7.1565884139944e-05
	momentum: 0.8683802947171396
	convergence_rate: 0.012803366168469622
	moving_avg: 0.9263892108829195
	p: 0.24651630496904078
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 03/15 11:55:50 | n_steps = 5001
[37m[36mINFO[0m[0m 03/15 11:55:50 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 03/15 11:55:50 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 03/15 11:55:50 | 
[37m[36mINFO[0m[0m 03/15 11:55:50 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 03/15 11:55:50 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 03/15 11:55:50 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 03/15 11:55:50 | Batch sizes for each domain: [13, 13, 13, 0] (total=39)
[37m[36mINFO[0m[0m 03/15 11:55:50 | steps-per-epoch for each domain: 87.08, 163.46, 202.00 -> min = 87.08
[37m[36mINFO[0m[0m 03/15 11:55:51 | # of params = 23518277
[37m[36mINFO[0m[0m 03/15 11:58:00 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 03/15 11:58:00 | 0.463902    0.482963    0.553608    0.552755    1.252554    0.699647    0.699647    0.519059    0.502825    0.442117    0.455793    0.463902    0.482963    0           0.000000    1.863750    1.024055    128.552766 
[37m[36mINFO[0m[0m 03/15 12:01:41 | 0.778230    0.788148    0.862250    0.851649    0.396544    0.997350    1.000000    0.768000    0.768362    0.821401    0.786585    0.778230    0.788148    200         2.296820    0.508366    0.448170    130.452101 
[37m[36mINFO[0m[0m 03/15 12:05:19 | 0.803036    0.802963    0.856167    0.847542    0.435178    0.996466    0.996466    0.754824    0.770245    0.817212    0.775915    0.803036    0.802963    400         4.593640    0.368448    0.451988    128.366066 
[37m[36mINFO[0m[0m 03/15 12:08:59 | 0.803776    0.807407    0.880296    0.862380    0.383983    1.000000    1.000000    0.794353    0.785311    0.846535    0.801829    0.803776    0.807407    600         6.890459    0.356277    0.448974    130.406954 
[37m[36mINFO[0m[0m 03/15 12:12:36 | 0.743428    0.754074    0.870632    0.840081    0.400231    0.998233    1.000000    0.769412    0.738230    0.844250    0.782012    0.743428    0.754074    800         9.187279    0.319271    0.449738    126.619464 
[37m[36mINFO[0m[0m 03/15 12:16:11 | 0.768604    0.785185    0.874630    0.833116    0.435286    0.999117    1.000000    0.771765    0.732580    0.853008    0.766768    0.768604    0.785185    1000        11.484099   0.309692    0.445611    126.053062 
[37m[36mINFO[0m[0m 03/15 12:19:48 | 0.762310    0.786667    0.881598    0.838335    0.428431    0.996466    0.996466    0.801412    0.753296    0.846915    0.765244    0.762310    0.786667    1200        13.780919   0.292546    0.446123    127.492805 
[37m[36mINFO[0m[0m 03/15 12:23:29 | 0.745280    0.773333    0.902024    0.855475    0.403012    0.999117    1.000000    0.819294    0.764595    0.887662    0.801829    0.745280    0.773333    1400        16.077739   0.276439    0.442774    132.797530 
[37m[36mINFO[0m[0m 03/15 12:27:07 | 0.742318    0.752593    0.920819    0.854177    0.416254    1.000000    0.996466    0.860706    0.762712    0.901752    0.803354    0.742318    0.752593    1600        18.374558   0.245947    0.447368    128.018951 
[37m[36mINFO[0m[0m 03/15 12:30:46 | 0.744169    0.733333    0.893535    0.847153    0.437522    0.995583    0.996466    0.806118    0.772128    0.878903    0.772866    0.744169    0.733333    1800        20.671378   0.226934    0.447744    129.755004 
[37m[36mINFO[0m[0m 03/15 12:34:21 | 0.689374    0.674074    0.902514    0.834372    0.439829    0.997350    1.000000    0.844235    0.736347    0.865956    0.766768    0.689374    0.674074    2000        22.968198   0.238222    0.442923    126.511620 
[37m[36mINFO[0m[0m 03/15 12:37:57 | 0.724917    0.721481    0.936420    0.851888    0.452662    1.000000    1.000000    0.880471    0.772128    0.928789    0.783537    0.724917    0.721481    2200        25.265018   0.211991    0.444233    126.656937 
[37m[36mINFO[0m[0m 03/15 12:41:36 | 0.733802    0.748148    0.930227    0.842233    0.466898    1.000000    1.000000    0.856941    0.740113    0.933740    0.786585    0.733802    0.748148    2400        27.561837   0.210251    0.447166    130.032011 
[37m[36mINFO[0m[0m 03/15 12:45:15 | 0.764532    0.765926    0.934736    0.850423    0.458975    0.999117    1.000000    0.878588    0.757062    0.926504    0.794207    0.764532    0.765926    2600        29.858657   0.179934    0.453673    128.390901 
[37m[36mINFO[0m[0m 03/15 12:48:51 | 0.702703    0.717037    0.949795    0.845957    0.515429    0.997350    0.996466    0.911059    0.753296    0.940975    0.788110    0.702703    0.717037    2800        32.155477   0.161230    0.448676    126.341546 
[37m[36mINFO[0m[0m 03/15 12:52:37 | 0.654202    0.672593    0.916820    0.822153    0.612559    1.000000    0.992933    0.885647    0.757062    0.864813    0.716463    0.654202    0.672593    3000        34.452297   0.164387    0.444889    136.124511 
[37m[36mINFO[0m[0m 03/15 12:56:18 | 0.731951    0.722963    0.939677    0.835329    0.533135    1.000000    1.000000    0.891765    0.751412    0.927266    0.754573    0.731951    0.722963    3200        36.749117   0.147420    0.464446    128.645823 
[37m[36mINFO[0m[0m 03/15 12:59:58 | 0.755646    0.751111    0.955608    0.839710    0.553420    0.998233    0.996466    0.920000    0.758945    0.948591    0.763720    0.755646    0.751111    3400        39.045936   0.131379    0.452290    129.310554 
[37m[36mINFO[0m[0m 03/15 13:03:41 | 0.747871    0.755556    0.953425    0.841815    0.527470    1.000000    1.000000    0.906353    0.749529    0.953922    0.775915    0.747871    0.755556    3600        41.342756   0.131317    0.452293    133.088630 
[37m[36mINFO[0m[0m 03/15 13:07:25 | 0.704554    0.730370    0.932080    0.820477    0.671230    0.998233    0.992933    0.857412    0.698682    0.940594    0.769817    0.704554    0.730370    3800        43.639576   0.122738    0.452539    133.213353 
[37m[36mINFO[0m[0m 03/15 13:11:06 | 0.767123    0.758519    0.960709    0.835956    0.659984    1.000000    1.000000    0.935059    0.753296    0.947068    0.754573    0.767123    0.758519    4000        45.936396   0.116153    0.446861    131.710356 
[37m[36mINFO[0m[0m 03/15 13:14:47 | 0.712699    0.677037    0.976512    0.849885    0.725211    1.000000    1.000000    0.960000    0.764595    0.969535    0.785061    0.712699    0.677037    4200        48.233216   0.111208    0.457590    129.310874 
[37m[36mINFO[0m[0m 03/15 13:18:27 | 0.753054    0.745185    0.970452    0.842980    0.626322    0.999117    1.000000    0.949176    0.743879    0.963062    0.785061    0.753054    0.745185    4400        50.530035   0.111645    0.453512    129.686926 
[37m[36mINFO[0m[0m 03/15 13:22:11 | 0.728249    0.718519    0.973759    0.833869    0.687056    0.999117    0.992933    0.947294    0.717514    0.974867    0.791159    0.728249    0.718519    4600        52.826855   0.088159    0.461037    131.635309 
[37m[36mINFO[0m[0m 03/15 13:25:49 | 0.755646    0.754074    0.976033    0.839602    0.626765    1.000000    1.000000    0.952471    0.730697    0.975628    0.788110    0.755646    0.754074    4800        55.123675   0.081251    0.437154    130.096531 
[37m[36mINFO[0m[0m 03/15 13:29:28 | 0.764532    0.739259    0.972784    0.843967    0.682089    1.000000    1.000000    0.947294    0.751412    0.971059    0.780488    0.764532    0.739259    5000        57.420495   0.083802    0.451553    129.090236 
[37m[36mINFO[0m[0m 03/15 13:29:28 | Cumulative gradient change saved at train_output/VLCS/GENIE/[3]/250315_11-55-50_B_VLCS3_adam/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 03/15 13:29:29 | ---
[37m[36mINFO[0m[0m 03/15 13:29:29 | test-domain validation(oracle) = 80.378%
[37m[36mINFO[0m[0m 03/15 13:29:29 | training-domain validation(iid) = 80.378%
[37m[36mINFO[0m[0m 03/15 13:29:29 | last = 76.453%
[37m[36mINFO[0m[0m 03/15 13:29:29 | last (inD) = 84.397%
[37m[36mINFO[0m[0m 03/15 13:29:29 | training-domain validation (iid, inD) = 86.238%
[37m[36mINFO[0m[0m 03/15 13:29:29 | === Summary ===
[37m[36mINFO[0m[0m 03/15 13:29:29 | Command: /jsm0707/GENIE/train_all.py B_VLCS3_adam config/resnet50_adam.yaml --trial_seed 0 --hparams_seed 18 --algorithm GENIE --test_envs 3 --dataset VLCS
[37m[36mINFO[0m[0m 03/15 13:29:29 | Unique name: 250315_11-55-50_B_VLCS3_adam
[37m[36mINFO[0m[0m 03/15 13:29:29 | Out path: train_output/VLCS/GENIE/[3]/250315_11-55-50_B_VLCS3_adam
[37m[36mINFO[0m[0m 03/15 13:29:29 | Algorithm: GENIE
[37m[36mINFO[0m[0m 03/15 13:29:29 | Dataset: VLCS
