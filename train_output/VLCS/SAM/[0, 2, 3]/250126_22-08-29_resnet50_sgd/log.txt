[37m[36mINFO[0m[0m 01/26 22:08:29 | Command :: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm SAM --test_envs 0 2 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: SAM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/VLCS/SAM/[0, 2, 3]/250126_22-08-29_resnet50_sgd
	out_root: train_output/VLCS/SAM/[0, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 2, 3]
	trial_seed: 0
	unique_name: 250126_22-08-29_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	rho: 0.05
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 01/26 22:08:29 | n_steps = 5001
[37m[36mINFO[0m[0m 01/26 22:08:29 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 01/26 22:08:29 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 01/26 22:08:29 | 
[37m[36mINFO[0m[0m 01/26 22:08:29 | Testenv name escaping te_C_S_V -> te_C_S_V
[37m[36mINFO[0m[0m 01/26 22:08:29 | Test envs = [0, 2, 3], name = te_C_S_V
[37m[36mINFO[0m[0m 01/26 22:08:29 | Train environments: [1], Test environments: [0, 2, 3]
[37m[36mINFO[0m[0m 01/26 22:08:29 | Batch sizes for each domain: [0, 32, 0, 0] (total=32)
[37m[36mINFO[0m[0m 01/26 22:08:29 | steps-per-epoch for each domain: 66.41 -> min = 66.41
[37m[36mINFO[0m[0m 01/26 22:08:30 | # of params = 23518277
[37m[36mINFO[0m[0m 01/26 22:10:41 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 01/26 22:10:41 | 0.144266    0.124149    0.155765    0.178908    1.656834    0.094523    0.084806    0.155765    0.178908    0.195735    0.166159    0.142540    0.121481    0           0.000000    2.564448    1.790675    128.604457 
[37m[36mINFO[0m[0m 01/26 22:16:53 | 0.298214    0.297303    0.668706    0.649718    0.875431    0.113074    0.098940    0.668706    0.649718    0.440213    0.434451    0.341355    0.358519    200         3.011765    1.233230    1.195062    133.403141 
[37m[36mINFO[0m[0m 01/26 22:22:57 | 0.339627    0.337813    0.679529    0.666667    0.828166    0.180212    0.180212    0.679529    0.666667    0.456588    0.443598    0.382081    0.389630    400         6.023529    1.061520    1.162398    131.081629 
[37m[36mINFO[0m[0m 01/26 22:29:02 | 0.401356    0.412052    0.684235    0.683616    0.791535    0.307420    0.346290    0.684235    0.683616    0.464585    0.455793    0.432062    0.434074    600         9.035294    1.001634    1.182506    127.668324 
[37m[36mINFO[0m[0m 01/26 22:34:57 | 0.428969    0.450829    0.692706    0.694915    0.760567    0.354240    0.420495    0.692706    0.694915    0.481721    0.478659    0.450944    0.453333    800         12.047059   0.965942    1.146619    125.873931 
[37m[36mINFO[0m[0m 01/26 22:41:00 | 0.494362    0.499047    0.701647    0.706215    0.741547    0.506184    0.526502    0.701647    0.706215    0.488195    0.483232    0.488708    0.487407    1000        15.058824   0.953833    1.162724    130.018287 
[37m[36mINFO[0m[0m 01/26 22:47:06 | 0.563037    0.576378    0.704471    0.717514    0.729761    0.667845    0.717314    0.704471    0.717514    0.485910    0.472561    0.535357    0.539259    1200        18.070588   0.937934    1.192263    126.993000 
[37m[36mINFO[0m[0m 01/26 22:53:09 | 0.591330    0.596034    0.727059    0.730697    0.708941    0.709364    0.738516    0.727059    0.730697    0.500762    0.498476    0.563865    0.551111    1400        21.082353   0.900526    1.161770    130.855034 
[37m[36mINFO[0m[0m 01/26 22:59:14 | 0.610660    0.609706    0.732706    0.728814    0.699123    0.743816    0.752650    0.732706    0.728814    0.508378    0.506098    0.579785    0.570370    1600        24.094118   0.900279    1.167225    131.351144 
[37m[36mINFO[0m[0m 01/26 23:05:16 | 0.647320    0.645144    0.730353    0.734463    0.679786    0.814488    0.823322    0.730353    0.734463    0.510663    0.509146    0.616809    0.602963    1800        27.105882   0.869748    1.174779    126.729748 
[37m[36mINFO[0m[0m 01/26 23:11:22 | 0.662146    0.657438    0.726118    0.749529    0.672976    0.841873    0.840989    0.726118    0.749529    0.512947    0.507622    0.631618    0.623704    2000        30.117647   0.884455    1.164088    132.820415 
[37m[36mINFO[0m[0m 01/26 23:17:22 | 0.666917    0.661460    0.735059    0.743879    0.663422    0.847173    0.840989    0.735059    0.743879    0.517517    0.515244    0.636061    0.628148    2200        33.129412   0.866889    1.167778    125.999272 
[37m[36mINFO[0m[0m 01/26 23:23:27 | 0.669379    0.668555    0.730824    0.747646    0.654753    0.856007    0.855124    0.730824    0.747646    0.505331    0.506098    0.646797    0.644444    2400        36.141176   0.853414    1.166770    131.514721 
[37m[36mINFO[0m[0m 01/26 23:29:26 | 0.675443    0.669163    0.741176    0.749529    0.648563    0.852473    0.855124    0.741176    0.749529    0.527799    0.518293    0.646057    0.634074    2600        39.152941   0.833464    1.163737    126.150013 
[37m[36mINFO[0m[0m 01/26 23:35:37 | 0.676353    0.673770    0.748235    0.749529    0.642357    0.857774    0.858657    0.748235    0.749529    0.515232    0.515244    0.656053    0.647407    2800        42.164706   0.857863    1.186040    133.649882 
[37m[36mINFO[0m[0m 01/26 23:41:47 | 0.675652    0.670573    0.747294    0.753296    0.640228    0.859541    0.855124    0.747294    0.753296    0.509139    0.510671    0.658275    0.645926    3000        45.176471   0.831123    1.179748    133.468680 
[37m[36mINFO[0m[0m 01/26 23:47:49 | 0.674825    0.669225    0.745882    0.745763    0.634798    0.860424    0.858657    0.745882    0.745763    0.507997    0.504573    0.656053    0.644444    3200        48.188235   0.814281    1.172457    127.204547 
[37m[36mINFO[0m[0m 01/26 23:53:51 | 0.677859    0.674596    0.752471    0.753296    0.627697    0.857774    0.855124    0.752471    0.753296    0.517898    0.518293    0.657904    0.650370    3400        51.200000   0.820603    1.169276    127.919879 
[37m[36mINFO[0m[0m 01/26 23:59:58 | 0.678938    0.674363    0.751529    0.747646    0.628069    0.856007    0.858657    0.751529    0.747646    0.524752    0.525915    0.656053    0.638519    3600        54.211765   0.817804    1.179640    130.913411 
[37m[36mINFO[0m[0m 01/27 00:06:07 | 0.682153    0.677820    0.746353    0.751412    0.625727    0.857774    0.858657    0.746353    0.751412    0.528561    0.525915    0.660126    0.648889    3800        57.223529   0.811211    1.188046    131.456859 
[37m[36mINFO[0m[0m 01/27 00:12:09 | 0.676380    0.671215    0.752941    0.757062    0.616057    0.862191    0.858657    0.752941    0.757062    0.505712    0.506098    0.661237    0.648889    4000        60.235294   0.782708    1.159471    130.114592 
[37m[36mINFO[0m[0m 01/27 00:18:15 | 0.677094    0.674249    0.755765    0.758945    0.613685    0.861307    0.858657    0.755765    0.758945    0.507997    0.513720    0.661977    0.650370    4200        63.247059   0.794685    1.178869    129.866004 
[37m[36mINFO[0m[0m 01/27 00:24:22 | 0.683832    0.674349    0.757647    0.753296    0.611046    0.861307    0.858657    0.757647    0.753296    0.529322    0.524390    0.660866    0.640000    4400        66.258824   0.782326    1.185584    130.256122 
[37m[36mINFO[0m[0m 01/27 00:30:26 | 0.683490    0.678314    0.752941    0.757062    0.609385    0.863074    0.858657    0.752941    0.757062    0.522087    0.525915    0.665309    0.650370    4600        69.270588   0.800097    1.173946    129.354649 
[37m[36mINFO[0m[0m 01/27 00:36:26 | 0.684389    0.678822    0.760941    0.762712    0.603551    0.863074    0.858657    0.760941    0.762712    0.525895    0.527439    0.664198    0.650370    4800        72.282353   0.791654    1.151895    129.554007 
[37m[36mINFO[0m[0m 01/27 00:42:31 | 0.680719    0.675759    0.762353    0.764595    0.602108    0.863074    0.858657    0.762353    0.764595    0.515994    0.516768    0.663088    0.651852    5000        75.294118   0.766386    1.173928    129.531553 
[37m[36mINFO[0m[0m 01/27 00:42:31 | Cumulative gradient change saved at train_output/VLCS/SAM/[0, 2, 3]/250126_22-08-29_resnet50_sgd/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 01/27 00:42:33 | ---
[37m[36mINFO[0m[0m 01/27 00:42:33 | test-domain validation(oracle) = 68.439%
[37m[36mINFO[0m[0m 01/27 00:42:33 | training-domain validation(iid) = 68.072%
[37m[36mINFO[0m[0m 01/27 00:42:33 | last = 68.072%
[37m[36mINFO[0m[0m 01/27 00:42:33 | last (inD) = 76.460%
[37m[36mINFO[0m[0m 01/27 00:42:33 | training-domain validation (iid, inD) = 76.460%
[37m[36mINFO[0m[0m 01/27 00:42:33 | === Summary ===
[37m[36mINFO[0m[0m 01/27 00:42:33 | Command: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm SAM --test_envs 0 2 3 --dataset VLCS
[37m[36mINFO[0m[0m 01/27 00:42:33 | Unique name: 250126_22-08-29_resnet50_sgd
[37m[36mINFO[0m[0m 01/27 00:42:33 | Out path: train_output/VLCS/SAM/[0, 2, 3]/250126_22-08-29_resnet50_sgd
[37m[36mINFO[0m[0m 01/27 00:42:33 | Algorithm: SAM
[37m[36mINFO[0m[0m 01/27 00:42:33 | Dataset: VLCS
