[37m[36mINFO[0m[0m 01/27 01:59:17 | Command :: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm SAM --test_envs 0 1 2 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: SAM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/VLCS/SAM/[0, 1, 2]/250127_01-59-17_resnet50_sgd
	out_root: train_output/VLCS/SAM/[0, 1, 2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 2]
	trial_seed: 0
	unique_name: 250127_01-59-17_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	rho: 0.05
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 01/27 01:59:17 | n_steps = 5001
[37m[36mINFO[0m[0m 01/27 01:59:17 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 01/27 01:59:17 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 01/27 01:59:17 | 
[37m[36mINFO[0m[0m 01/27 01:59:17 | Testenv name escaping te_C_L_S -> te_C_L_S
[37m[36mINFO[0m[0m 01/27 01:59:17 | Test envs = [0, 1, 2], name = te_C_L_S
[37m[36mINFO[0m[0m 01/27 01:59:17 | Train environments: [3], Test environments: [0, 1, 2]
[37m[36mINFO[0m[0m 01/27 01:59:17 | Batch sizes for each domain: [0, 0, 0, 32] (total=32)
[37m[36mINFO[0m[0m 01/27 01:59:17 | steps-per-epoch for each domain: 84.41 -> min = 84.41
[37m[36mINFO[0m[0m 01/27 01:59:19 | # of params = 23518277
[37m[36mINFO[0m[0m 01/27 02:01:28 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 01/27 02:01:28 | 0.156367    0.131997    0.132914    0.121481    1.832258    0.091873    0.077739    0.186824    0.158192    0.190404    0.160061    0.132914    0.121481    0           0.000000    2.591939    1.183773    128.371909 
[37m[36mINFO[0m[0m 01/27 02:04:11 | 0.616006    0.610588    0.556461    0.565926    1.143219    0.681979    0.671378    0.610824    0.617702    0.555217    0.542683    0.556461    0.565926    200         2.369493    1.547551    0.173994    127.440115 
[37m[36mINFO[0m[0m 01/27 02:06:59 | 0.722953    0.709835    0.774528    0.757037    0.796154    0.837456    0.823322    0.561412    0.566855    0.769992    0.739329    0.774528    0.757037    400         4.738986    1.230051    0.184465    130.950578 
[37m[36mINFO[0m[0m 01/27 02:09:41 | 0.775653    0.764547    0.854869    0.840000    0.568340    0.960247    0.957597    0.570824    0.555556    0.795887    0.780488    0.854869    0.840000    600         7.108478    0.942460    0.180474    125.575315 
[37m[36mINFO[0m[0m 01/27 02:12:27 | 0.784968    0.774184    0.867086    0.850370    0.491433    0.969081    0.961131    0.581176    0.568738    0.804646    0.792683    0.867086    0.850370    800         9.477971    0.790725    0.178110    130.537768 
[37m[36mINFO[0m[0m 01/27 02:15:15 | 0.788378    0.774824    0.870789    0.850370    0.465068    0.968198    0.964664    0.591529    0.576271    0.805407    0.783537    0.870789    0.850370    1000        11.847464   0.753860    0.182342    130.563171 
[37m[36mINFO[0m[0m 01/27 02:18:00 | 0.785550    0.774776    0.879304    0.857778    0.444613    0.975265    0.968198    0.577882    0.564972    0.803503    0.791159    0.879304    0.857778    1200        14.216957   0.698392    0.183574    128.741860 
[37m[36mINFO[0m[0m 01/27 02:20:44 | 0.785748    0.780491    0.884117    0.859259    0.429865    0.982332    0.978799    0.577882    0.559322    0.797030    0.803354    0.884117    0.859259    1400        16.586449   0.678807    0.182338    127.581544 
[37m[36mINFO[0m[0m 01/27 02:23:34 | 0.785312    0.775679    0.889300    0.850370    0.428151    0.985866    0.978799    0.568471    0.555556    0.801599    0.792683    0.889300    0.850370    1600        18.955942   0.649861    0.213529    126.623334 
[37m[36mINFO[0m[0m 01/27 02:26:23 | 0.789959    0.781209    0.888190    0.859259    0.410088    0.984099    0.978799    0.584941    0.570621    0.800838    0.794207    0.888190    0.859259    1800        21.325435   0.629029    0.206257    128.155539 
[37m[36mINFO[0m[0m 01/27 02:29:10 | 0.790493    0.779685    0.885228    0.851852    0.409133    0.984982    0.978799    0.588706    0.570621    0.797791    0.789634    0.885228    0.851852    2000        23.694928   0.618258    0.184522    129.599699 
[37m[36mINFO[0m[0m 01/27 02:32:02 | 0.784455    0.779135    0.893003    0.857778    0.405233    0.988516    0.992933    0.567059    0.551789    0.797791    0.792683    0.893003    0.857778    2200        26.064421   0.604776    0.189702    133.749833 
[37m[36mINFO[0m[0m 01/27 02:34:48 | 0.779511    0.779876    0.894113    0.857778    0.400909    0.988516    1.000000    0.557176    0.542373    0.792841    0.797256    0.894113    0.857778    2400        28.433913   0.585277    0.176847    130.672921 
[37m[36mINFO[0m[0m 01/27 02:37:35 | 0.783672    0.776660    0.897816    0.851852    0.395807    0.986749    0.985866    0.568000    0.549906    0.796268    0.794207    0.897816    0.851852    2600        30.803406   0.592118    0.173572    132.862325 
[37m[36mINFO[0m[0m 01/27 02:40:22 | 0.786600    0.783762    0.895964    0.854815    0.392614    0.988516    1.000000    0.578824    0.555556    0.792460    0.795732    0.895964    0.854815    2800        33.172899   0.583333    0.183269    129.946301 
[37m[36mINFO[0m[0m 01/27 02:43:10 | 0.786967    0.781999    0.900777    0.857778    0.386747    0.991166    1.000000    0.572706    0.551789    0.797030    0.794207    0.900777    0.857778    3000        35.542392   0.564533    0.193663    129.728996 
[37m[36mINFO[0m[0m 01/27 02:45:58 | 0.790196    0.784869    0.899297    0.854815    0.383952    0.989399    1.000000    0.586824    0.564972    0.794364    0.789634    0.899297    0.854815    3200        37.911884   0.556104    0.177604    131.971699 
[37m[36mINFO[0m[0m 01/27 02:48:45 | 0.786973    0.781251    0.901148    0.866667    0.378542    0.994700    1.000000    0.572235    0.548023    0.793983    0.795732    0.901148    0.866667    3400        40.281377   0.538297    0.180600    130.748956 
[37m[36mINFO[0m[0m 01/27 02:51:30 | 0.788593    0.783882    0.908182    0.860741    0.372781    0.990283    1.000000    0.584941    0.557439    0.790556    0.794207    0.908182    0.860741    3600        42.650870   0.540384    0.182119    128.987097 
[37m[36mINFO[0m[0m 01/27 02:54:17 | 0.787492    0.782866    0.902258    0.860741    0.374980    0.989399    1.000000    0.580235    0.557439    0.792841    0.791159    0.902258    0.860741    3800        45.020363   0.545483    0.180314    131.055655 
[37m[36mINFO[0m[0m 01/27 02:57:05 | 0.788661    0.784898    0.908182    0.860741    0.374829    0.994700    1.000000    0.578824    0.557439    0.792460    0.797256    0.908182    0.860741    4000        47.389856   0.507364    0.175288    132.410925 
[37m[36mINFO[0m[0m 01/27 02:59:50 | 0.791939    0.788635    0.905591    0.862222    0.368509    0.992049    1.000000    0.594353    0.576271    0.789414    0.789634    0.905591    0.862222    4200        49.759348   0.499336    0.175254    129.935570 
[37m[36mINFO[0m[0m 01/27 03:02:35 | 0.791332    0.785406    0.909293    0.862222    0.368966    0.993816    1.000000    0.583529    0.557439    0.796649    0.798780    0.909293    0.862222    4400        52.128841   0.516420    0.177881    129.001261 
[37m[36mINFO[0m[0m 01/27 03:05:18 | 0.785531    0.782387    0.910033    0.857778    0.366910    0.994700    1.000000    0.567529    0.549906    0.794364    0.797256    0.910033    0.857778    4600        54.498334   0.492588    0.175185    127.912340 
[37m[36mINFO[0m[0m 01/27 03:07:58 | 0.786313    0.778041    0.912255    0.859259    0.366745    0.992049    0.996466    0.571765    0.548023    0.795126    0.789634    0.912255    0.859259    4800        56.867827   0.476886    0.177988    124.221564 
[37m[36mINFO[0m[0m 01/27 03:10:42 | 0.790416    0.786273    0.910774    0.860741    0.362485    0.994700    1.000000    0.584471    0.563089    0.792079    0.795732    0.910774    0.860741    5000        59.237320   0.478317    0.173300    129.388399 
[37m[36mINFO[0m[0m 01/27 03:10:42 | Cumulative gradient change saved at train_output/VLCS/SAM/[0, 1, 2]/250127_01-59-17_resnet50_sgd/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 01/27 03:10:43 | ---
[37m[36mINFO[0m[0m 01/27 03:10:43 | test-domain validation(oracle) = 79.194%
[37m[36mINFO[0m[0m 01/27 03:10:43 | training-domain validation(iid) = 78.697%
[37m[36mINFO[0m[0m 01/27 03:10:43 | last = 79.042%
[37m[36mINFO[0m[0m 01/27 03:10:43 | last (inD) = 86.074%
[37m[36mINFO[0m[0m 01/27 03:10:43 | training-domain validation (iid, inD) = 86.667%
[37m[36mINFO[0m[0m 01/27 03:10:43 | === Summary ===
[37m[36mINFO[0m[0m 01/27 03:10:43 | Command: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm SAM --test_envs 0 1 2 --dataset VLCS
[37m[36mINFO[0m[0m 01/27 03:10:43 | Unique name: 250127_01-59-17_resnet50_sgd
[37m[36mINFO[0m[0m 01/27 03:10:43 | Out path: train_output/VLCS/SAM/[0, 1, 2]/250127_01-59-17_resnet50_sgd
[37m[36mINFO[0m[0m 01/27 03:10:43 | Algorithm: SAM
[37m[36mINFO[0m[0m 01/27 03:10:43 | Dataset: VLCS
