[37m[36mINFO[0m[0m 01/27 00:42:37 | Command :: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm SAM --test_envs 0 1 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: SAM
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/VLCS/SAM/[0, 1, 3]/250127_00-42-37_resnet50_sgd
	out_root: train_output/VLCS/SAM/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 0
	unique_name: 250127_00-42-37_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	rho: 0.05
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 01/27 00:42:37 | n_steps = 5001
[37m[36mINFO[0m[0m 01/27 00:42:37 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 01/27 00:42:37 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 01/27 00:42:37 | 
[37m[36mINFO[0m[0m 01/27 00:42:37 | Testenv name escaping te_C_L_V -> te_C_L_V
[37m[36mINFO[0m[0m 01/27 00:42:37 | Test envs = [0, 1, 3], name = te_C_L_V
[37m[36mINFO[0m[0m 01/27 00:42:37 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 01/27 00:42:37 | Batch sizes for each domain: [0, 0, 32, 0] (total=32)
[37m[36mINFO[0m[0m 01/27 00:42:37 | steps-per-epoch for each domain: 82.06 -> min = 82.06
[37m[36mINFO[0m[0m 01/27 00:42:39 | # of params = 23518277
[37m[36mINFO[0m[0m 01/27 00:44:47 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 01/27 00:44:47 | 0.146604    0.127503    0.156512    0.175305    1.691621    0.093640    0.084806    0.201412    0.173258    0.156512    0.175305    0.144761    0.124444    0           0.000000    2.514216    1.064427    126.850885 
[37m[36mINFO[0m[0m 01/27 00:47:47 | 0.466158    0.469951    0.756283    0.756098    0.717495    0.357774    0.353357    0.506824    0.500942    0.756283    0.756098    0.533876    0.555556    200         2.437167    1.291870    0.248708    130.001662 
[37m[36mINFO[0m[0m 01/27 00:50:46 | 0.579705    0.579361    0.785225    0.789634    0.609094    0.628092    0.639576    0.522353    0.510358    0.785225    0.789634    0.588671    0.588148    400         4.874334    0.998190    0.242266    130.674701 
[37m[36mINFO[0m[0m 01/27 00:53:46 | 0.583520    0.587720    0.803503    0.800305    0.578216    0.613958    0.643110    0.524235    0.514124    0.803503    0.800305    0.612366    0.605926    600         7.311500    0.920039    0.241286    131.418627 
[37m[36mINFO[0m[0m 01/27 00:56:52 | 0.611699    0.615624    0.805027    0.814024    0.549772    0.647527    0.671378    0.567059    0.551789    0.805027    0.814024    0.620511    0.623704    800         9.748667    0.870374    0.254461    134.212881 
[37m[36mINFO[0m[0m 01/27 00:59:48 | 0.627632    0.630246    0.811881    0.817073    0.531233    0.689929    0.710247    0.557647    0.540490    0.811881    0.817073    0.635320    0.640000    1000        12.185834   0.817335    0.247305    127.112209 
[37m[36mINFO[0m[0m 01/27 01:02:48 | 0.637082    0.638900    0.822925    0.823171    0.519907    0.696996    0.710247    0.574118    0.564972    0.822925    0.823171    0.640133    0.641481    1200        14.623001   0.793867    0.242287    131.412908 
[37m[36mINFO[0m[0m 01/27 01:05:48 | 0.613650    0.616006    0.820259    0.817073    0.511278    0.640459    0.650177    0.570353    0.559322    0.820259    0.817073    0.630137    0.638519    1400        17.060168   0.760640    0.252682    128.360778 
[37m[36mINFO[0m[0m 01/27 01:08:44 | 0.657978    0.659631    0.824829    0.829268    0.508779    0.733216    0.738516    0.590588    0.578154    0.824829    0.829268    0.650130    0.662222    1600        19.497334   0.761595    0.240674    127.655596 
[37m[36mINFO[0m[0m 01/27 01:11:40 | 0.637740    0.640058    0.831683    0.817073    0.505934    0.695230    0.706714    0.570824    0.563089    0.831683    0.817073    0.647168    0.650370    1800        21.934501   0.746158    0.247503    127.059632 
[37m[36mINFO[0m[0m 01/27 01:14:36 | 0.667325    0.660696    0.832064    0.821646    0.499964    0.745583    0.734982    0.598118    0.581921    0.832064    0.821646    0.658275    0.665185    2000        24.371668   0.729605    0.240847    127.440453 
[37m[36mINFO[0m[0m 01/27 01:17:35 | 0.662857    0.656930    0.837014    0.821646    0.498451    0.742933    0.734982    0.585882    0.570621    0.837014    0.821646    0.659756    0.665185    2200        26.808835   0.716952    0.261483    126.931806 
[37m[36mINFO[0m[0m 01/27 01:20:33 | 0.654016    0.653241    0.833206    0.818598    0.500283    0.727032    0.731449    0.576000    0.563089    0.833206    0.818598    0.659015    0.665185    2400        29.246002   0.709858    0.250141    127.361197 
[37m[36mINFO[0m[0m 01/27 01:23:27 | 0.650158    0.658623    0.840823    0.817073    0.493633    0.711131    0.731449    0.585882    0.576271    0.840823    0.817073    0.653462    0.668148    2600        31.683168   0.681298    0.243249    125.766741 
[37m[36mINFO[0m[0m 01/27 01:26:24 | 0.669130    0.668737    0.848439    0.823171    0.488754    0.745583    0.738516    0.600941    0.595104    0.848439    0.823171    0.660866    0.672593    2800        34.120335   0.687863    0.246097    128.087931 
[37m[36mINFO[0m[0m 01/27 01:29:21 | 0.657030    0.656189    0.841965    0.810976    0.489088    0.730565    0.727915    0.584471    0.572505    0.841965    0.810976    0.656053    0.668148    3000        36.557502   0.684139    0.241260    127.671674 
[37m[36mINFO[0m[0m 01/27 01:32:22 | 0.651393    0.653396    0.846535    0.814024    0.491352    0.718198    0.724382    0.584000    0.570621    0.846535    0.814024    0.651981    0.665185    3200        38.994669   0.671353    0.261350    128.973230 
[37m[36mINFO[0m[0m 01/27 01:35:21 | 0.664926    0.662121    0.846535    0.820122    0.483789    0.738516    0.731449    0.595765    0.583804    0.846535    0.820122    0.660496    0.671111    3400        41.431835   0.659486    0.241148    131.017802 
[37m[36mINFO[0m[0m 01/27 01:38:23 | 0.665117    0.662544    0.845392    0.823171    0.481414    0.723498    0.724382    0.615059    0.595104    0.845392    0.823171    0.656794    0.668148    3600        43.869002   0.665092    0.251050    131.307151 
[37m[36mINFO[0m[0m 01/27 01:41:21 | 0.670224    0.671665    0.853770    0.827744    0.481572    0.741166    0.742049    0.607529    0.598870    0.853770    0.827744    0.661977    0.674074    3800        46.306169   0.642937    0.248636    128.878993 
[37m[36mINFO[0m[0m 01/27 01:44:20 | 0.669010    0.665443    0.851485    0.815549    0.485080    0.752650    0.745583    0.593882    0.578154    0.851485    0.815549    0.660496    0.672593    4000        48.743336   0.649518    0.242955    129.933656 
[37m[36mINFO[0m[0m 01/27 01:47:17 | 0.667897    0.666156    0.859101    0.821646    0.482508    0.723498    0.731449    0.621176    0.598870    0.859101    0.821646    0.659015    0.668148    4200        51.180503   0.654946    0.235875    130.045574 
[37m[36mINFO[0m[0m 01/27 01:50:16 | 0.666341    0.664689    0.855293    0.823171    0.480932    0.742933    0.734982    0.596706    0.589454    0.855293    0.823171    0.659385    0.669630    4400        53.617669   0.653418    0.244595    129.500496 
[37m[36mINFO[0m[0m 01/27 01:53:12 | 0.662302    0.661768    0.858340    0.824695    0.474948    0.713781    0.720848    0.619294    0.600753    0.858340    0.824695    0.653832    0.663704    4600        56.054836   0.647743    0.234302    130.002470 
[37m[36mINFO[0m[0m 01/27 01:56:11 | 0.670939    0.665204    0.856055    0.820122    0.473028    0.737633    0.727915    0.615059    0.595104    0.856055    0.820122    0.660126    0.672593    4800        58.492003   0.630548    0.238029    130.818548 
[37m[36mINFO[0m[0m 01/27 01:59:10 | 0.667682    0.661994    0.861005    0.826220    0.475362    0.726148    0.720848    0.617882    0.596987    0.861005    0.826220    0.659015    0.668148    5000        60.929170   0.629362    0.245921    129.942933 
[37m[36mINFO[0m[0m 01/27 01:59:10 | Cumulative gradient change saved at train_output/VLCS/SAM/[0, 1, 3]/250127_00-42-37_resnet50_sgd/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 01/27 01:59:12 | ---
[37m[36mINFO[0m[0m 01/27 01:59:12 | test-domain validation(oracle) = 67.022%
[37m[36mINFO[0m[0m 01/27 01:59:12 | training-domain validation(iid) = 65.798%
[37m[36mINFO[0m[0m 01/27 01:59:12 | last = 66.768%
[37m[36mINFO[0m[0m 01/27 01:59:12 | last (inD) = 82.622%
[37m[36mINFO[0m[0m 01/27 01:59:12 | training-domain validation (iid, inD) = 82.927%
[37m[36mINFO[0m[0m 01/27 01:59:12 | === Summary ===
[37m[36mINFO[0m[0m 01/27 01:59:12 | Command: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm SAM --test_envs 0 1 3 --dataset VLCS
[37m[36mINFO[0m[0m 01/27 01:59:12 | Unique name: 250127_00-42-37_resnet50_sgd
[37m[36mINFO[0m[0m 01/27 01:59:12 | Out path: train_output/VLCS/SAM/[0, 1, 3]/250127_00-42-37_resnet50_sgd
[37m[36mINFO[0m[0m 01/27 01:59:12 | Algorithm: SAM
[37m[36mINFO[0m[0m 01/27 01:59:12 | Dataset: VLCS
