[37m[36mINFO[0m[0m 01/27 04:00:17 | Command :: /jsm0707/Large-scale/train_all.py resnet50_adam config/resnet50_adam.yaml --algorithm CORAL --test_envs 1 2 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_adam
	out_dir: train_output/VLCS/CORAL/[1, 2, 3]/250127_04-00-17_resnet50_adam
	out_root: train_output/VLCS/CORAL/[1, 2, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1, 2, 3]
	trial_seed: 0
	unique_name: 250127_04-00-17_resnet50_adam
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	mmd_gamma: 1.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 01/27 04:00:17 | n_steps = 5001
[37m[36mINFO[0m[0m 01/27 04:00:17 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 01/27 04:00:17 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 01/27 04:00:17 | 
[37m[36mINFO[0m[0m 01/27 04:00:17 | Testenv name escaping te_L_S_V -> te_L_S_V
[37m[36mINFO[0m[0m 01/27 04:00:17 | Test envs = [1, 2, 3], name = te_L_S_V
[37m[36mINFO[0m[0m 01/27 04:00:17 | Train environments: [0], Test environments: [1, 2, 3]
[37m[36mINFO[0m[0m 01/27 04:00:17 | Batch sizes for each domain: [32, 0, 0, 0] (total=32)
[37m[36mINFO[0m[0m 01/27 04:00:17 | steps-per-epoch for each domain: 35.38 -> min = 35.38
[37m[36mINFO[0m[0m 01/27 04:00:18 | # of params = 23518277
[37m[36mINFO[0m[0m 01/27 04:02:19 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 01/27 04:02:19 | 0.400874    0.416575    0.694346    0.696113    1.319062    0.694346    0.696113    0.450353    0.487759    0.384996    0.385671    0.367271    0.376296    0           0.000000    1.820007    0.000000    0.860967    120.568093 
[37m[36mINFO[0m[0m 01/27 04:04:47 | 0.396405    0.389510    1.000000    0.992933    0.021109    1.000000    0.992933    0.346824    0.348399    0.404037    0.399390    0.438356    0.420741    200         5.653710    0.048336    0.000000    0.142216    119.497369 
[37m[36mINFO[0m[0m 01/27 04:07:19 | 0.288530    0.285356    0.979682    0.957597    0.107677    0.979682    0.957597    0.265412    0.271186    0.201066    0.199695    0.399111    0.385185    400         11.307420   0.014810    0.000000    0.134504    124.791135 
[37m[36mINFO[0m[0m 01/27 04:09:52 | 0.337202    0.347071    1.000000    1.000000    0.001779    1.000000    1.000000    0.216941    0.242938    0.351866    0.359756    0.442799    0.438519    600         16.961131   0.008096    0.000000    0.134500    125.155502 
[37m[36mINFO[0m[0m 01/27 04:12:24 | 0.312680    0.312603    1.000000    1.000000    0.000943    1.000000    1.000000    0.183529    0.197740    0.345773    0.344512    0.408738    0.395556    800         22.614841   0.000044    0.000000    0.137602    124.839813 
[37m[36mINFO[0m[0m 01/27 04:14:58 | 0.309239    0.310478    1.000000    1.000000    0.000049    1.000000    1.000000    0.184471    0.203390    0.340061    0.338415    0.403184    0.389630    1000        28.268551   0.000014    0.000000    0.135793    126.857495 
[37m[36mINFO[0m[0m 01/27 04:17:29 | 0.310430    0.309490    1.000000    1.000000    0.000045    1.000000    1.000000    0.185412    0.203390    0.341584    0.338415    0.404295    0.386667    1200        33.922261   0.000009    0.000000    0.134700    124.046332 
[37m[36mINFO[0m[0m 01/27 04:20:01 | 0.337630    0.343629    1.000000    1.000000    0.000036    1.000000    1.000000    0.217412    0.235405    0.354532    0.365854    0.440948    0.429630    1400        39.575972   0.000010    0.000000    0.132635    125.108835 
[37m[36mINFO[0m[0m 01/27 04:22:32 | 0.332818    0.339277    1.000000    1.000000    0.000037    1.000000    1.000000    0.212706    0.229755    0.350724    0.365854    0.435024    0.422222    1600        45.229682   0.000003    0.000000    0.135839    124.496892 
[37m[36mINFO[0m[0m 01/27 04:25:06 | 0.329077    0.335283    1.000000    1.000000    0.000030    1.000000    1.000000    0.208941    0.229755    0.348819    0.361280    0.429471    0.414815    1800        50.883392   0.000003    0.000000    0.146028    124.317112 
[37m[36mINFO[0m[0m 01/27 04:27:37 | 0.337271    0.343002    1.000000    1.000000    0.000023    1.000000    1.000000    0.217412    0.233522    0.355674    0.365854    0.438726    0.429630    2000        56.537102   0.000003    0.000000    0.132897    124.880302 
[37m[36mINFO[0m[0m 01/27 04:30:07 | 0.351904    0.352263    1.000000    1.000000    0.001569    1.000000    1.000000    0.231059    0.246704    0.351866    0.358232    0.472788    0.451852    2200        62.190813   0.000087    0.000000    0.133115    122.886825 
[37m[36mINFO[0m[0m 01/27 04:32:37 | 0.326320    0.330430    1.000000    0.996466    0.009283    1.000000    0.996466    0.184941    0.210923    0.355293    0.355183    0.438726    0.425185    2400        67.844523   0.000024    0.000000    0.131526    123.715831 
[37m[36mINFO[0m[0m 01/27 04:35:08 | 0.175083    0.178705    0.834806    0.819788    0.470534    0.834806    0.819788    0.047059    0.047081    0.261234    0.268293    0.216957    0.220741    2600        73.498233   0.294504    0.000000    0.143322    122.093678 
[37m[36mINFO[0m[0m 01/27 04:37:40 | 0.196436    0.188269    0.954064    0.932862    0.202226    0.954064    0.932862    0.105882    0.090395    0.223153    0.222561    0.260274    0.251852    2800        79.151943   0.125132    0.000000    0.134870    125.895827 
[37m[36mINFO[0m[0m 01/27 04:40:13 | 0.205282    0.199847    0.994700    0.982332    0.049066    0.994700    0.982332    0.105412    0.092279    0.235720    0.231707    0.274713    0.275556    3000        84.805654   0.027773    0.000000    0.139171    124.343266 
[37m[36mINFO[0m[0m 01/27 04:42:46 | 0.244936    0.247411    0.997350    0.989399    0.034626    0.997350    0.989399    0.129882    0.131827    0.318736    0.317073    0.286190    0.293333    3200        90.459364   0.021627    0.000000    0.144620    124.560101 
[37m[36mINFO[0m[0m 01/27 04:45:17 | 0.216621    0.208527    1.000000    0.985866    0.032013    1.000000    0.985866    0.110118    0.099812    0.264661    0.257622    0.275083    0.268148    3400        96.113074   0.007025    0.000000    0.136822    123.544606 
[37m[36mINFO[0m[0m 01/27 04:47:48 | 0.225514    0.218154    1.000000    0.992933    0.026375    1.000000    0.992933    0.105412    0.092279    0.288271    0.288110    0.282858    0.274074    3600        101.766784  0.000467    0.000000    0.133035    124.277686 
[37m[36mINFO[0m[0m 01/27 04:50:21 | 0.209927    0.205422    1.000000    0.985866    0.048178    1.000000    0.985866    0.107765    0.092279    0.252856    0.246951    0.269160    0.277037    3800        107.420495  0.002459    0.000000    0.139887    125.324927 
[37m[36mINFO[0m[0m 01/27 04:52:53 | 0.243535    0.234923    0.998233    0.992933    0.021178    0.998233    0.992933    0.121882    0.105461    0.295506    0.291159    0.313217    0.308148    4000        113.074205  0.013737    0.000000    0.133794    124.873803 
[37m[36mINFO[0m[0m 01/27 04:55:25 | 0.233774    0.226062    1.000000    0.992933    0.019660    1.000000    0.992933    0.114824    0.105461    0.295126    0.294207    0.291374    0.278519    4200        118.727915  0.000308    0.000000    0.138207    124.498405 
[37m[36mINFO[0m[0m 01/27 04:57:55 | 0.232336    0.229400    1.000000    0.992933    0.020513    1.000000    0.992933    0.112000    0.103578    0.294745    0.295732    0.290263    0.288889    4400        124.381625  0.000094    0.000000    0.133049    123.780220 
[37m[36mINFO[0m[0m 01/27 05:00:30 | 0.230589    0.219126    1.000000    0.992933    0.023893    1.000000    0.992933    0.112471    0.099812    0.289033    0.282012    0.290263    0.275556    4600        130.035336  0.000090    0.000000    0.145419    125.786785 
[37m[36mINFO[0m[0m 01/27 05:03:01 | 0.231945    0.220651    1.000000    0.992933    0.024999    1.000000    0.992933    0.110118    0.099812    0.293602    0.286585    0.292114    0.275556    4800        135.689046  0.000019    0.000000    0.138667    122.997145 
[37m[36mINFO[0m[0m 01/27 05:05:33 | 0.232206    0.225866    0.977032    0.968198    0.112592    0.977032    0.968198    0.109176    0.101695    0.302361    0.301829    0.285080    0.274074    5000        141.342756  0.000276    0.000000    0.138215    124.792976 
[37m[36mINFO[0m[0m 01/27 05:05:34 | Cumulative gradient change saved at train_output/VLCS/CORAL/[1, 2, 3]/250127_04-00-17_resnet50_adam/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 01/27 05:05:35 | ---
[37m[36mINFO[0m[0m 01/27 05:05:35 | test-domain validation(oracle) = 40.087%
[37m[36mINFO[0m[0m 01/27 05:05:35 | training-domain validation(iid) = 33.720%
[37m[36mINFO[0m[0m 01/27 05:05:35 | last = 23.221%
[37m[36mINFO[0m[0m 01/27 05:05:35 | last (inD) = 96.820%
[37m[36mINFO[0m[0m 01/27 05:05:35 | training-domain validation (iid, inD) = 100.000%
[37m[36mINFO[0m[0m 01/27 05:05:35 | === Summary ===
[37m[36mINFO[0m[0m 01/27 05:05:35 | Command: /jsm0707/Large-scale/train_all.py resnet50_adam config/resnet50_adam.yaml --algorithm CORAL --test_envs 1 2 3 --dataset VLCS
[37m[36mINFO[0m[0m 01/27 05:05:35 | Unique name: 250127_04-00-17_resnet50_adam
[37m[36mINFO[0m[0m 01/27 05:05:35 | Out path: train_output/VLCS/CORAL/[1, 2, 3]/250127_04-00-17_resnet50_adam
[37m[36mINFO[0m[0m 01/27 05:05:35 | Algorithm: CORAL
[37m[36mINFO[0m[0m 01/27 05:05:35 | Dataset: VLCS
