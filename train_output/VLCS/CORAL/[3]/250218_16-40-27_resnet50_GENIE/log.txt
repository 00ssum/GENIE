[37m[36mINFO[0m[0m 02/18 16:40:28 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 3 --dataset VLCS --trial_seed 2 --hparams_seed 9
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 9
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[3]/250218_16-40-27_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 2
	unique_name: 250218_16-40-27_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 2.716671579524612e-05
	batch_size: 29
	weight_decay: 1.71368232883332e-06
	mmd_gamma: 1.254370045611671
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/18 16:40:28 | n_steps = 5001
[37m[36mINFO[0m[0m 02/18 16:40:28 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/18 16:40:28 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/18 16:40:28 | 
[37m[36mINFO[0m[0m 02/18 16:40:28 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 02/18 16:40:28 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 02/18 16:40:28 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 02/18 16:40:28 | Batch sizes for each domain: [29, 29, 29, 0] (total=87)
[37m[36mINFO[0m[0m 02/18 16:40:28 | steps-per-epoch for each domain: 39.03, 73.28, 90.55 -> min = 39.03
[37m[36mINFO[0m[0m 02/18 16:40:29 | # of params = 23518277
[37m[36mINFO[0m[0m 02/18 16:43:03 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/18 16:43:03 | 0.440207    0.459259    0.493095    0.470461    1.251196    0.620141    0.593640    0.467294    0.459510    0.391851    0.358232    0.440207    0.459259    0           0.000000    1.786449    0.063059    2.175079    152.003770 
[37m[36mINFO[0m[0m 02/18 16:49:27 | 0.777490    0.782222    0.870468    0.857167    0.389987    1.000000    0.996466    0.783529    0.777778    0.827875    0.797256    0.777490    0.782222    200         5.123675    0.476309    0.027016    1.148446    154.424758 
[37m[36mINFO[0m[0m 02/18 16:55:43 | 0.779711    0.774815    0.880646    0.851535    0.401984    1.000000    0.992933    0.785882    0.779661    0.856055    0.782012    0.779711    0.774815    400         10.247350   0.334260    0.018387    1.142524    147.232220 
[37m[36mINFO[0m[0m 02/18 17:02:02 | 0.732692    0.740741    0.886910    0.851822    0.401690    0.999117    0.989399    0.805176    0.762712    0.856436    0.803354    0.732692    0.740741    600         15.371025   0.302315    0.016331    1.139615    150.877243 
[37m[36mINFO[0m[0m 02/18 17:08:17 | 0.786375    0.777778    0.919721    0.858888    0.413507    1.000000    0.992933    0.857412    0.783427    0.901752    0.800305    0.786375    0.777778    800         20.494700   0.262973    0.014742    1.127468    149.699854 
[37m[36mINFO[0m[0m 02/18 17:14:39 | 0.738615    0.721481    0.915339    0.849371    0.460301    1.000000    0.989399    0.866353    0.772128    0.879665    0.786585    0.738615    0.721481    1000        25.618375   0.227079    0.015282    1.140903    153.540960 
[37m[36mINFO[0m[0m 02/18 17:20:56 | 0.758608    0.742222    0.943703    0.854106    0.438359    1.000000    0.992933    0.901176    0.772128    0.929931    0.797256    0.758608    0.742222    1200        30.742049   0.195691    0.014418    1.135794    149.819585 
[37m[36mINFO[0m[0m 02/18 17:27:16 | 0.761940    0.757037    0.958964    0.854410    0.473853    0.999117    0.985866    0.924235    0.774011    0.953542    0.803354    0.761940    0.757037    1400        35.865724   0.167751    0.013518    1.159080    148.640109 
[37m[36mINFO[0m[0m 02/18 17:33:36 | 0.750093    0.746667    0.962709    0.848576    0.506365    1.000000    0.992933    0.926588    0.757062    0.961538    0.795732    0.750093    0.746667    1600        40.989399   0.135467    0.013983    1.150446    150.018895 
[37m[36mINFO[0m[0m 02/18 17:39:46 | 0.756016    0.765926    0.960369    0.844553    0.591120    0.999117    0.996466    0.930353    0.755179    0.951637    0.782012    0.756016    0.765926    1800        46.113074   0.110184    0.013853    1.150575    139.913312 
[37m[36mINFO[0m[0m 02/18 17:46:23 | 0.717512    0.736296    0.972142    0.841043    0.547890    1.000000    0.992933    0.949176    0.734463    0.967251    0.795732    0.717512    0.736296    2000        51.236749   0.096892    0.012825    1.180919    160.853011 
[37m[36mINFO[0m[0m 02/18 17:52:42 | 0.748612    0.757037    0.982132    0.848486    0.572627    0.999117    0.992933    0.965176    0.747646    0.982102    0.804878    0.748612    0.757037    2200        56.360424   0.090010    0.012791    1.144140    149.951883 
[37m[36mINFO[0m[0m 02/18 17:59:17 | 0.755646    0.751111    0.986131    0.850578    0.600637    1.000000    0.992933    0.975529    0.764595    0.982864    0.794207    0.755646    0.751111    2400        61.484099   0.063940    0.012615    1.181327    158.996982 
[37m[36mINFO[0m[0m 02/18 18:05:29 | 0.758608    0.742222    0.989186    0.837324    0.619717    1.000000    0.989399    0.981647    0.734463    0.985910    0.788110    0.758608    0.742222    2600        66.607774   0.066910    0.012169    1.147410    141.907263 
[37m[36mINFO[0m[0m 02/18 18:11:47 | 0.763421    0.752593    0.983988    0.842495    0.719722    1.000000    0.989399    0.971765    0.743879    0.980198    0.794207    0.763421    0.752593    2800        71.731449   0.043255    0.011678    1.140241    150.291173 
[37m[36mINFO[0m[0m 02/18 18:18:12 | 0.745280    0.740741    0.993943    0.853986    0.664263    1.000000    0.992933    0.990588    0.770245    0.991241    0.798780    0.745280    0.740741    3000        76.855124   0.037141    0.011288    1.118995    161.278008 
[37m[36mINFO[0m[0m 02/18 18:24:31 | 0.757867    0.761481    0.985862    0.855947    0.658267    1.000000    0.989399    0.971294    0.779661    0.986291    0.798780    0.757867    0.761481    3200        81.978799   0.041329    0.010680    1.161522    146.195697 
[37m[36mINFO[0m[0m 02/18 18:30:44 | 0.769345    0.771852    0.992367    0.846125    0.700671    1.000000    0.992933    0.987765    0.766478    0.989337    0.778963    0.769345    0.771852    3400        87.102473   0.033236    0.010347    1.156600    142.419889 
[37m[36mINFO[0m[0m 02/18 18:37:05 | 0.741207    0.745185    0.990619    0.844169    0.728822    1.000000    0.989399    0.980235    0.738230    0.991622    0.804878    0.741207    0.745185    3600        92.226148   0.020482    0.010012    1.176496    145.436390 
[37m[36mINFO[0m[0m 02/18 18:43:32 | 0.734543    0.754074    0.995235    0.847177    0.810095    1.000000    0.985866    0.992941    0.772128    0.992765    0.783537    0.734543    0.754074    3800        97.349823   0.025472    0.009519    1.166749    153.653159 
[37m[36mINFO[0m[0m 02/18 18:49:43 | 0.761570    0.765926    0.996632    0.855271    0.683375    1.000000    0.992933    0.992941    0.766478    0.996954    0.806402    0.761570    0.765926    4000        102.473498  0.024097    0.009074    1.121050    146.602843 
[37m[36mINFO[0m[0m 02/18 18:56:08 | 0.740466    0.736296    0.991978    0.840851    0.716317    1.000000    0.989399    0.981647    0.741996    0.994288    0.791159    0.740466    0.736296    4200        107.597173  0.018364    0.008911    1.162316    152.390944 
[37m[36mINFO[0m[0m 02/18 19:02:21 | 0.744909    0.757037    0.996714    0.842747    0.749973    1.000000    0.992933    0.996235    0.753296    0.993907    0.782012    0.744909    0.757037    4400        112.720848  0.016143    0.008486    1.118906    149.478661 
[37m[36mINFO[0m[0m 02/18 19:08:39 | 0.760089    0.776296    0.995743    0.849610    0.779495    1.000000    0.989399    0.992941    0.775895    0.994288    0.783537    0.760089    0.776296    4600        117.844523  0.025031    0.007984    1.152079    147.328193 
[37m[36mINFO[0m[0m 02/18 19:14:45 | 0.760459    0.770370    0.995116    0.836488    0.895768    1.000000    0.989399    0.991059    0.753296    0.994288    0.766768    0.760459    0.770370    4800        122.968198  0.016466    0.007761    1.082671    149.820743 
[37m[36mINFO[0m[0m 02/18 19:20:49 | 0.728249    0.737778    0.998297    0.849275    0.700189    1.000000    0.996466    0.997176    0.749529    0.997715    0.801829    0.728249    0.737778    5000        128.091873  0.020828    0.007538    1.153874    133.319892 
[37m[36mINFO[0m[0m 02/18 19:20:49 | Cumulative gradient change saved at train_output/VLCS/CORAL/[3]/250218_16-40-27_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/18 19:20:51 | ---
[37m[36mINFO[0m[0m 02/18 19:20:51 | test-domain validation(oracle) = 77.749%
[37m[36mINFO[0m[0m 02/18 19:20:51 | training-domain validation(iid) = 78.638%
[37m[36mINFO[0m[0m 02/18 19:20:51 | last = 72.825%
[37m[36mINFO[0m[0m 02/18 19:20:51 | last (inD) = 84.927%
[37m[36mINFO[0m[0m 02/18 19:20:51 | training-domain validation (iid, inD) = 85.889%
[37m[36mINFO[0m[0m 02/18 19:20:51 | === Summary ===
[37m[36mINFO[0m[0m 02/18 19:20:51 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 3 --dataset VLCS --trial_seed 2 --hparams_seed 9
[37m[36mINFO[0m[0m 02/18 19:20:51 | Unique name: 250218_16-40-27_resnet50_GENIE
[37m[36mINFO[0m[0m 02/18 19:20:51 | Out path: train_output/VLCS/CORAL/[3]/250218_16-40-27_resnet50_GENIE
[37m[36mINFO[0m[0m 02/18 19:20:51 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/18 19:20:51 | Dataset: VLCS
