[37m[36mINFO[0m[0m 02/11 06:11:45 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 3 --dataset VLCS --trial_seed 0 --hparams_seed 2
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 2
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[3]/250211_06-11-45_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 0
	unique_name: 250211_06-11-45_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.9041073434446342e-05
	batch_size: 9
	weight_decay: 0.0006566989842279891
	mmd_gamma: 1.5832433896458313
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/11 06:11:45 | n_steps = 5001
[37m[36mINFO[0m[0m 02/11 06:11:45 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/11 06:11:45 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/11 06:11:45 | 
[37m[36mINFO[0m[0m 02/11 06:11:45 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 02/11 06:11:45 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 02/11 06:11:45 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 02/11 06:11:45 | Batch sizes for each domain: [9, 9, 9, 0] (total=27)
[37m[36mINFO[0m[0m 02/11 06:11:45 | steps-per-epoch for each domain: 125.78, 236.11, 291.78 -> min = 125.78
[37m[36mINFO[0m[0m 02/11 06:11:46 | # of params = 23518277
[37m[36mINFO[0m[0m 02/11 06:14:13 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/11 06:14:13 | 0.448352    0.474074    0.535612    0.532973    1.264365    0.682862    0.674912    0.533647    0.529190    0.390327    0.394817    0.448352    0.474074    0           0.000000    1.812850    0.092801    1.497875    145.507354 
[37m[36mINFO[0m[0m 02/11 06:17:45 | 0.728619    0.740741    0.838469    0.836667    0.451970    0.988516    0.989399    0.731765    0.740113    0.795126    0.780488    0.728619    0.740741    200         1.590106    0.573368    0.029201    0.335878    144.815745 
[37m[36mINFO[0m[0m 02/11 06:21:17 | 0.756387    0.785185    0.854428    0.834449    0.422334    0.996466    0.996466    0.753412    0.740113    0.813404    0.766768    0.756387    0.785185    400         3.180212    0.421561    0.023103    0.330488    146.081297 
[37m[36mINFO[0m[0m 02/11 06:24:45 | 0.718623    0.733333    0.844711    0.829338    0.439995    1.000000    0.996466    0.747765    0.747646    0.786367    0.743902    0.718623    0.733333    600         4.770318    0.375977    0.018470    0.345294    139.272595 
[37m[36mINFO[0m[0m 02/11 06:28:21 | 0.772677    0.791111    0.867537    0.844343    0.393435    1.000000    0.996466    0.753412    0.743879    0.849200    0.792683    0.772677    0.791111    800         6.360424    0.357807    0.017073    0.385027    138.314136 
[37m[36mINFO[0m[0m 02/11 06:31:58 | 0.773047    0.780741    0.876242    0.850818    0.393279    0.998233    0.992933    0.787765    0.768362    0.842727    0.791159    0.773047    0.780741    1000        7.950530    0.346938    0.015675    0.353541    146.043267 
[37m[36mINFO[0m[0m 02/11 06:35:34 | 0.778601    0.802963    0.890165    0.857627    0.386798    0.999117    1.000000    0.800471    0.766478    0.870906    0.806402    0.778601    0.802963    1200        9.540636    0.327331    0.015302    0.352628    145.729800 
[37m[36mINFO[0m[0m 02/11 06:39:03 | 0.768974    0.789630    0.863372    0.830604    0.454001    0.999117    1.000000    0.728471    0.693032    0.862529    0.798780    0.768974    0.789630    1400        11.130742   0.312455    0.013011    0.330296    142.693881 
[37m[36mINFO[0m[0m 02/11 06:42:30 | 0.795261    0.789630    0.893774    0.845042    0.405593    1.000000    1.000000    0.796706    0.736347    0.884615    0.798780    0.795261    0.789630    1600        12.720848   0.313562    0.012836    0.345107    138.146746 
[37m[36mINFO[0m[0m 02/11 06:46:06 | 0.723436    0.752593    0.883150    0.846927    0.407762    1.000000    1.000000    0.811294    0.774011    0.838157    0.766768    0.723436    0.752593    1800        14.310954   0.312200    0.012582    0.385184    139.443233 
[37m[36mINFO[0m[0m 02/11 06:49:39 | 0.786375    0.786667    0.903790    0.846507    0.397916    1.000000    1.000000    0.824471    0.751412    0.886900    0.788110    0.786375    0.786667    2000        15.901060   0.281846    0.012776    0.349589    142.888747 
[37m[36mINFO[0m[0m 02/11 06:53:09 | 0.770826    0.779259    0.917265    0.859588    0.412141    1.000000    0.996466    0.860706    0.775895    0.891089    0.806402    0.770826    0.779259    2200        17.491166   0.265517    0.011719    0.365257    136.913856 
[37m[36mINFO[0m[0m 02/11 06:56:47 | 0.778601    0.783704    0.922478    0.843489    0.424450    0.998233    1.000000    0.862118    0.775895    0.907083    0.754573    0.778601    0.783704    2400        19.081272   0.261835    0.011517    0.377374    142.180240 
[37m[36mINFO[0m[0m 02/11 07:00:22 | 0.723436    0.749630    0.916331    0.842084    0.435422    1.000000    1.000000    0.856000    0.777778    0.892993    0.748476    0.723436    0.749630    2600        20.671378   0.231406    0.012724    0.393253    136.317692 
[37m[36mINFO[0m[0m 02/11 07:03:52 | 0.773047    0.777778    0.914844    0.845910    0.475796    1.000000    1.000000    0.846588    0.774011    0.897944    0.763720    0.773047    0.777778    2800        22.261484   0.231094    0.010996    0.358966    138.511192 
[37m[36mINFO[0m[0m 02/11 07:07:21 | 0.692336    0.690370    0.901155    0.817458    0.478257    0.999117    0.992933    0.797647    0.691149    0.906702    0.768293    0.692336    0.690370    3000        23.851590   0.226691    0.011453    0.353393    138.315809 
[37m[36mINFO[0m[0m 02/11 07:10:53 | 0.767864    0.771852    0.933621    0.842102    0.429324    0.998233    0.996466    0.885647    0.764595    0.916984    0.765244    0.767864    0.771852    3200        25.441696   0.228203    0.010837    0.362457    139.670541 
[37m[36mINFO[0m[0m 02/11 07:14:30 | 0.787116    0.776296    0.924597    0.836321    0.461461    0.992933    0.992933    0.883294    0.772128    0.897563    0.743902    0.787116    0.776296    3400        27.031802   0.226209    0.010439    0.394941    137.619293 
[37m[36mINFO[0m[0m 02/11 07:18:07 | 0.765272    0.768889    0.932199    0.843028    0.439943    1.000000    0.996466    0.864000    0.755179    0.932597    0.777439    0.765272    0.768889    3600        28.621908   0.221372    0.011038    0.385078    140.374691 
[37m[36mINFO[0m[0m 02/11 07:21:39 | 0.762680    0.771852    0.916585    0.823151    0.480845    1.000000    0.996466    0.856000    0.738230    0.893755    0.734756    0.762680    0.771852    3800        30.212014   0.201825    0.011095    0.368573    138.130541 
[37m[36mINFO[0m[0m 02/11 07:25:06 | 0.713439    0.694815    0.935267    0.833989    0.496398    0.999117    0.992933    0.880941    0.751412    0.925743    0.757622    0.713439    0.694815    4000        31.802120   0.164198    0.011224    0.336815    140.056619 
[37m[36mINFO[0m[0m 02/11 07:28:40 | 0.741947    0.736296    0.931124    0.846586    0.478152    0.999117    0.996466    0.893647    0.787194    0.900609    0.756098    0.741947    0.736296    4200        33.392226   0.176809    0.011454    0.348218    143.395426 
[37m[36mINFO[0m[0m 02/11 07:32:10 | 0.753425    0.760000    0.914388    0.816365    0.587654    1.000000    0.996466    0.841412    0.719397    0.901752    0.733232    0.753425    0.760000    4400        34.982332   0.152287    0.010809    0.335951    143.266706 
[37m[36mINFO[0m[0m 02/11 07:35:42 | 0.764161    0.760000    0.959259    0.857645    0.482181    1.000000    0.996466    0.924235    0.785311    0.953542    0.791159    0.764161    0.760000    4600        36.572438   0.168722    0.010127    0.352534    141.894367 
[37m[36mINFO[0m[0m 02/11 07:39:16 | 0.742688    0.733333    0.963927    0.850902    0.498944    1.000000    1.000000    0.931765    0.764595    0.960015    0.788110    0.742688    0.733333    4800        38.162544   0.134532    0.010677    0.363893    140.868891 
[37m[36mINFO[0m[0m 02/11 07:42:50 | 0.742688    0.734815    0.953551    0.845031    0.548019    0.999117    0.996466    0.920941    0.762712    0.940594    0.775915    0.742688    0.734815    5000        39.752650   0.142224    0.010167    0.358551    142.670505 
[37m[36mINFO[0m[0m 02/11 07:42:51 | Cumulative gradient change saved at train_output/VLCS/CORAL/[3]/250211_06-11-45_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/11 07:42:52 | ---
[37m[36mINFO[0m[0m 02/11 07:42:52 | test-domain validation(oracle) = 77.860%
[37m[36mINFO[0m[0m 02/11 07:42:52 | training-domain validation(iid) = 77.083%
[37m[36mINFO[0m[0m 02/11 07:42:52 | last = 74.269%
[37m[36mINFO[0m[0m 02/11 07:42:52 | last (inD) = 84.503%
[37m[36mINFO[0m[0m 02/11 07:42:52 | training-domain validation (iid, inD) = 85.959%
[37m[36mINFO[0m[0m 02/11 07:42:52 | === Summary ===
[37m[36mINFO[0m[0m 02/11 07:42:52 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 3 --dataset VLCS --trial_seed 0 --hparams_seed 2
[37m[36mINFO[0m[0m 02/11 07:42:52 | Unique name: 250211_06-11-45_resnet50_GENIE
[37m[36mINFO[0m[0m 02/11 07:42:52 | Out path: train_output/VLCS/CORAL/[3]/250211_06-11-45_resnet50_GENIE
[37m[36mINFO[0m[0m 02/11 07:42:52 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/11 07:42:52 | Dataset: VLCS
