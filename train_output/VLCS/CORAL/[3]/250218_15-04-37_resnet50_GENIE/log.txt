[37m[36mINFO[0m[0m 02/18 15:04:37 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 3 --dataset VLCS --trial_seed 2 --hparams_seed 8
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 8
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[3]/250218_15-04-37_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 2
	unique_name: 250218_15-04-37_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 3.019123985218396e-05
	batch_size: 9
	weight_decay: 0.0020603523571546007
	mmd_gamma: 7.360960263393403
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/18 15:04:37 | n_steps = 5001
[37m[36mINFO[0m[0m 02/18 15:04:37 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/18 15:04:37 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/18 15:04:37 | 
[37m[36mINFO[0m[0m 02/18 15:04:37 | Testenv name escaping te_V -> te_V
[37m[36mINFO[0m[0m 02/18 15:04:37 | Test envs = [3], name = te_V
[37m[36mINFO[0m[0m 02/18 15:04:37 | Train environments: [0, 1, 2], Test environments: [3]
[37m[36mINFO[0m[0m 02/18 15:04:37 | Batch sizes for each domain: [9, 9, 9, 0] (total=27)
[37m[36mINFO[0m[0m 02/18 15:04:37 | steps-per-epoch for each domain: 125.78, 236.11, 291.78 -> min = 125.78
[37m[36mINFO[0m[0m 02/18 15:04:39 | # of params = 23518277
[37m[36mINFO[0m[0m 02/18 15:07:03 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/18 15:07:03 | 0.492040    0.521481    0.534315    0.525651    1.298169    0.627208    0.614841    0.461647    0.465160    0.514090    0.496951    0.492040    0.521481    0           0.000000    1.694293    0.075088    1.284783    142.874993 
[37m[36mINFO[0m[0m 02/18 15:10:56 | 0.700481    0.696296    0.792737    0.781253    0.565807    0.976148    0.964664    0.642353    0.653484    0.759711    0.725610    0.700481    0.696296    200         1.590106    0.675133    0.010169    0.396328    154.064071 
[37m[36mINFO[0m[0m 02/18 15:14:46 | 0.710478    0.715556    0.848980    0.848558    0.424227    1.000000    0.996466    0.735059    0.770245    0.811881    0.778963    0.710478    0.715556    400         3.180212    0.475627    0.008439    0.381122    153.996474 
[37m[36mINFO[0m[0m 02/18 15:18:28 | 0.699371    0.691852    0.836250    0.829815    0.438529    0.995583    0.996466    0.691765    0.691149    0.821401    0.801829    0.699371    0.691852    600         4.770318    0.423541    0.007085    0.359603    149.795623 
[37m[36mINFO[0m[0m 02/18 15:22:09 | 0.754906    0.758519    0.868533    0.847410    0.407647    1.000000    0.992933    0.775059    0.762712    0.830541    0.786585    0.754906    0.758519    800         6.360424    0.399224    0.006389    0.353426    150.703840 
[37m[36mINFO[0m[0m 02/18 15:25:51 | 0.677527    0.672593    0.837851    0.825080    0.443066    1.000000    0.992933    0.707765    0.691149    0.805788    0.791159    0.677527    0.672593    1000        7.950530    0.393739    0.005880    0.355690    150.236666 
[37m[36mINFO[0m[0m 02/18 15:29:36 | 0.714920    0.722963    0.830065    0.788559    0.498979    0.998233    0.985866    0.714353    0.657250    0.777609    0.722561    0.714920    0.722963    1200        9.540636    0.361184    0.005339    0.367220    151.486262 
[37m[36mINFO[0m[0m 02/18 15:33:18 | 0.755646    0.749630    0.888375    0.851607    0.391895    1.000000    0.996466    0.797647    0.770245    0.867479    0.788110    0.755646    0.749630    1400        11.130742   0.339338    0.005227    0.348778    152.039215 
[37m[36mINFO[0m[0m 02/18 15:36:54 | 0.750833    0.757037    0.876302    0.830993    0.445594    0.998233    0.982332    0.796706    0.743879    0.833968    0.766768    0.750833    0.757037    1600        12.720848   0.340221    0.005173    0.357007    144.927594 
[37m[36mINFO[0m[0m 02/18 15:40:36 | 0.768234    0.758519    0.882624    0.844002    0.407788    1.000000    0.992933    0.783059    0.757062    0.864813    0.782012    0.768234    0.758519    1800        14.310954   0.296440    0.004956    0.376190    147.254686 
[37m[36mINFO[0m[0m 02/18 15:44:22 | 0.704554    0.703704    0.890159    0.841491    0.428573    1.000000    0.992933    0.787765    0.749529    0.882711    0.782012    0.704554    0.703704    2000        15.901060   0.320744    0.004671    0.406833    144.345631 
[37m[36mINFO[0m[0m 02/18 15:48:07 | 0.734913    0.720000    0.886309    0.839345    0.441467    0.999117    0.985866    0.779765    0.760829    0.880046    0.771341    0.734913    0.720000    2200        17.491166   0.277817    0.004406    0.386250    147.514379 
[37m[36mINFO[0m[0m 02/18 15:51:56 | 0.751944    0.762963    0.894827    0.831837    0.473263    0.999117    0.992933    0.825882    0.749529    0.859482    0.753049    0.751944    0.762963    2400        19.081272   0.275060    0.004349    0.380300    153.448200 
[37m[36mINFO[0m[0m 02/18 15:55:43 | 0.731211    0.725926    0.912224    0.844576    0.430139    0.998233    0.985866    0.846588    0.755179    0.891851    0.792683    0.731211    0.725926    2600        20.671378   0.289775    0.004131    0.338231    159.303628 
[37m[36mINFO[0m[0m 02/18 15:59:22 | 0.759348    0.757037    0.923230    0.852115    0.428801    1.000000    0.996466    0.852706    0.770245    0.916984    0.789634    0.759348    0.757037    2800        22.261484   0.250797    0.004248    0.337243    151.408259 
[37m[36mINFO[0m[0m 02/18 16:03:12 | 0.773787    0.768889    0.924310    0.843841    0.421799    0.999117    0.989399    0.858353    0.757062    0.915461    0.785061    0.773787    0.768889    3000        23.851590   0.257575    0.004110    0.351083    159.521362 
[37m[36mINFO[0m[0m 02/18 16:06:39 | 0.757497    0.762963    0.925663    0.847440    0.465860    1.000000    0.992933    0.847059    0.755179    0.929931    0.794207    0.757497    0.762963    3200        25.441696   0.219245    0.004209    0.337221    139.923593 
[37m[36mINFO[0m[0m 02/18 16:10:25 | 0.740837    0.755556    0.908536    0.828123    0.486402    0.999117    0.982332    0.825882    0.730697    0.900609    0.771341    0.740837    0.755556    3400        27.031802   0.232933    0.004089    0.348974    156.065075 
[37m[36mINFO[0m[0m 02/18 16:14:11 | 0.746390    0.745185    0.907668    0.831311    0.481237    1.000000    0.996466    0.847529    0.730697    0.875476    0.766768    0.746390    0.745185    3600        28.621908   0.248631    0.003864    0.337960    158.274907 
[37m[36mINFO[0m[0m 02/18 16:17:51 | 0.710107    0.737778    0.924221    0.840511    0.499752    0.994700    0.985866    0.870118    0.755179    0.907845    0.780488    0.710107    0.737778    3800        30.212014   0.234981    0.003777    0.323486    155.033406 
[37m[36mINFO[0m[0m 02/18 16:21:33 | 0.760829    0.755556    0.909897    0.827730    0.540386    0.999117    0.989399    0.843294    0.751412    0.887281    0.742378    0.760829    0.755556    4000        31.802120   0.223135    0.003786    0.351360    151.835460 
[37m[36mINFO[0m[0m 02/18 16:25:17 | 0.728619    0.731852    0.926614    0.843542    0.491526    1.000000    0.989399    0.864000    0.768362    0.915842    0.772866    0.728619    0.731852    4200        33.392226   0.228100    0.003726    0.381602    147.536747 
[37m[36mINFO[0m[0m 02/18 16:29:01 | 0.778601    0.782222    0.917350    0.831389    0.545577    0.998233    0.992933    0.875294    0.766478    0.878522    0.734756    0.778601    0.782222    4400        34.982332   0.187889    0.003770    0.415345    141.460719 
[37m[36mINFO[0m[0m 02/18 16:32:51 | 0.654202    0.642963    0.921189    0.833666    0.477673    0.998233    0.985866    0.883765    0.751412    0.881569    0.763720    0.654202    0.642963    4600        36.572438   0.212589    0.003362    0.410483    147.234050 
[37m[36mINFO[0m[0m 02/18 16:36:39 | 0.720104    0.714074    0.926118    0.830162    0.470487    0.999117    0.992933    0.866824    0.723164    0.912414    0.774390    0.720104    0.714074    4800        38.162544   0.200451    0.003653    0.390547    149.953597 
[37m[36mINFO[0m[0m 02/18 16:40:21 | 0.715661    0.725926    0.910986    0.828010    0.493487    1.000000    0.992933    0.809882    0.721281    0.923077    0.769817    0.715661    0.725926    5000        39.752650   0.191792    0.003587    0.402214    141.755123 
[37m[36mINFO[0m[0m 02/18 16:40:21 | Cumulative gradient change saved at train_output/VLCS/CORAL/[3]/250218_15-04-37_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/18 16:40:23 | ---
[37m[36mINFO[0m[0m 02/18 16:40:23 | test-domain validation(oracle) = 77.860%
[37m[36mINFO[0m[0m 02/18 16:40:23 | training-domain validation(iid) = 75.935%
[37m[36mINFO[0m[0m 02/18 16:40:23 | last = 71.566%
[37m[36mINFO[0m[0m 02/18 16:40:23 | last (inD) = 82.801%
[37m[36mINFO[0m[0m 02/18 16:40:23 | training-domain validation (iid, inD) = 85.212%
[37m[36mINFO[0m[0m 02/18 16:40:23 | === Summary ===
[37m[36mINFO[0m[0m 02/18 16:40:23 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 3 --dataset VLCS --trial_seed 2 --hparams_seed 8
[37m[36mINFO[0m[0m 02/18 16:40:23 | Unique name: 250218_15-04-37_resnet50_GENIE
[37m[36mINFO[0m[0m 02/18 16:40:23 | Out path: train_output/VLCS/CORAL/[3]/250218_15-04-37_resnet50_GENIE
[37m[36mINFO[0m[0m 02/18 16:40:23 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/18 16:40:23 | Dataset: VLCS
