[37m[36mINFO[0m[0m 02/10 22:57:35 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 --dataset VLCS --trial_seed 1 --hparams_seed 12
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 12
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[0]/250210_22-57-35_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 1
	unique_name: 250210_22-57-35_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 3.0065625733392338e-05
	batch_size: 34
	weight_decay: 4.89485556145273e-06
	mmd_gamma: 0.8969215075436004
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/10 22:57:35 | n_steps = 5001
[37m[36mINFO[0m[0m 02/10 22:57:35 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/10 22:57:35 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/10 22:57:35 | 
[37m[36mINFO[0m[0m 02/10 22:57:35 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 02/10 22:57:35 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 02/10 22:57:35 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 02/10 22:57:35 | Batch sizes for each domain: [0, 34, 34, 34] (total=102)
[37m[36mINFO[0m[0m 02/10 22:57:35 | steps-per-epoch for each domain: 62.50, 77.24, 79.44 -> min = 62.50
[37m[36mINFO[0m[0m 02/10 22:57:36 | # of params = 23518277
[37m[36mINFO[0m[0m 02/10 23:00:05 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/10 23:00:05 | 0.613958    0.618375    0.432326    0.428842    1.405183    0.613958    0.618375    0.466824    0.461394    0.376618    0.419207    0.453536    0.405926    0           0.000000    1.790269    0.040822    1.448756    147.604140 
[37m[36mINFO[0m[0m 02/10 23:06:39 | 0.982332    0.975265    0.799792    0.784419    0.573516    0.982332    0.975265    0.768941    0.753296    0.797411    0.788110    0.833025    0.811852    200         3.200000    0.639837    0.029146    1.243710    144.887003 
[37m[36mINFO[0m[0m 02/10 23:13:16 | 0.990283    0.989399    0.855993    0.823953    0.494887    0.990283    0.989399    0.813176    0.766478    0.849581    0.849085    0.905220    0.856296    400         6.400000    0.458151    0.023726    1.264187    144.624317 
[37m[36mINFO[0m[0m 02/10 23:19:49 | 0.986749    0.985866    0.865530    0.824793    0.505529    0.986749    0.985866    0.817882    0.764595    0.870526    0.847561    0.908182    0.862222    600         9.600000    0.402148    0.020895    1.271712    138.438895 
[37m[36mINFO[0m[0m 02/10 23:26:26 | 0.985866    0.975265    0.878675    0.818106    0.529089    0.985866    0.975265    0.817882    0.753296    0.890708    0.852134    0.927434    0.848889    800         12.800000   0.343261    0.019833    1.247563    147.459926 
[37m[36mINFO[0m[0m 02/10 23:33:06 | 0.984099    0.968198    0.900473    0.816918    0.559163    0.984099    0.968198    0.856471    0.753296    0.909368    0.830793    0.935579    0.866667    1000        16.000000   0.293022    0.019374    1.284463    143.071326 
[37m[36mINFO[0m[0m 02/10 23:39:41 | 0.985866    0.968198    0.926168    0.809989    0.580991    0.985866    0.968198    0.895529    0.760829    0.932216    0.824695    0.950759    0.844444    1200        19.200000   0.237102    0.019881    1.262565    142.494537 
[37m[36mINFO[0m[0m 02/10 23:46:17 | 0.985866    0.968198    0.902455    0.787273    0.687444    0.985866    0.968198    0.826824    0.715631    0.924600    0.798780    0.955942    0.847407    1400        22.400000   0.205517    0.019595    1.277863    140.240585 
[37m[36mINFO[0m[0m 02/10 23:53:02 | 0.979682    0.964664    0.906307    0.780746    0.767463    0.979682    0.964664    0.886118    0.728814    0.898705    0.792683    0.934098    0.820741    1600        25.600000   0.166430    0.019955    1.292272    146.344465 
[37m[36mINFO[0m[0m 02/10 23:59:36 | 0.991166    0.982332    0.960102    0.795814    0.728452    0.991166    0.982332    0.953412    0.758945    0.967251    0.801829    0.959645    0.826667    1800        28.800000   0.145291    0.019537    1.263947    140.993166 
[37m[36mINFO[0m[0m 02/11 00:06:15 | 0.984099    0.968198    0.966200    0.800694    0.745297    0.984099    0.968198    0.937412    0.726930    0.975628    0.827744    0.985561    0.847407    2000        32.000000   0.114491    0.019050    1.287451    141.731699 
[37m[36mINFO[0m[0m 02/11 00:12:55 | 0.980565    0.971731    0.972997    0.790885    0.812734    0.980565    0.971731    0.952471    0.730697    0.980960    0.806402    0.985561    0.835556    2200        35.200000   0.082891    0.019279    1.260629    148.281862 
[37m[36mINFO[0m[0m 02/11 00:19:35 | 0.983216    0.975265    0.977000    0.801918    0.836697    0.983216    0.975265    0.966118    0.753296    0.975248    0.810976    0.989633    0.841481    2400        38.400000   0.088992    0.017566    1.292508    141.117093 
[37m[36mINFO[0m[0m 02/11 00:26:07 | 0.990283    0.982332    0.971751    0.787482    0.883305    0.990283    0.982332    0.947294    0.726930    0.979436    0.788110    0.988523    0.847407    2600        41.600000   0.076280    0.016565    1.268150    138.452528 
[37m[36mINFO[0m[0m 02/11 00:32:55 | 0.987633    0.982332    0.986548    0.781701    0.898609    0.987633    0.982332    0.980706    0.719397    0.988195    0.807927    0.990744    0.817778    2800        44.800000   0.063808    0.015830    1.286031    150.377062 
[37m[36mINFO[0m[0m 02/11 00:39:39 | 0.984099    0.978799    0.990407    0.802945    0.847349    0.984099    0.978799    0.989647    0.775895    0.989718    0.801829    0.991855    0.831111    3000        48.000000   0.052902    0.016029    1.301308    144.103876 
[37m[36mINFO[0m[0m 02/11 00:46:24 | 0.977915    0.971731    0.987957    0.798934    0.921496    0.977915    0.971731    0.984941    0.758945    0.987814    0.818598    0.991114    0.819259    3200        51.200000   0.050255    0.015490    1.299835    145.042907 
[37m[36mINFO[0m[0m 02/11 00:53:05 | 0.978799    0.968198    0.988149    0.798096    0.938000    0.978799    0.968198    0.982588    0.753296    0.986672    0.824695    0.995187    0.816296    3400        54.400000   0.043938    0.014663    1.271723    146.437016 
[37m[36mINFO[0m[0m 02/11 00:59:52 | 0.977032    0.978799    0.995276    0.804515    0.914810    0.977032    0.978799    0.995294    0.753296    0.992384    0.824695    0.998149    0.835556    3600        57.600000   0.039399    0.014064    1.310469    145.076328 
[37m[36mINFO[0m[0m 02/11 01:06:35 | 0.980565    0.978799    0.992529    0.787441    0.966800    0.980565    0.978799    0.989647    0.738230    0.992384    0.803354    0.995557    0.820741    3800        60.800000   0.022098    0.013828    1.298989    143.225210 
[37m[36mINFO[0m[0m 02/11 01:13:25 | 0.979682    0.975265    0.993030    0.793494    0.939944    0.979682    0.975265    0.993412    0.753296    0.990861    0.807927    0.994817    0.819259    4000        64.000000   0.038742    0.013781    1.318229    146.183984 
[37m[36mINFO[0m[0m 02/11 01:20:05 | 0.980565    0.978799    0.993453    0.786454    0.941956    0.980565    0.978799    0.990588    0.738230    0.991622    0.803354    0.998149    0.817778    4200        67.200000   0.026049    0.012466    1.263450    147.808209 
[37m[36mINFO[0m[0m 02/11 01:26:48 | 0.980565    0.964664    0.984200    0.801016    1.022210    0.980565    0.964664    0.976000    0.753296    0.984006    0.820122    0.992595    0.829630    4400        70.400000   0.043774    0.011861    1.301599    142.872766 
[37m[36mINFO[0m[0m 02/11 01:33:33 | 0.982332    0.978799    0.993037    0.800749    1.030769    0.982332    0.978799    0.995294    0.741996    0.990480    0.824695    0.993336    0.835556    4600        73.600000   0.020821    0.012189    1.317737    140.902237 
[37m[36mINFO[0m[0m 02/11 01:40:18 | 0.984099    0.978799    0.994299    0.803203    0.954116    0.984099    0.978799    0.989647    0.749529    0.996954    0.818598    0.996298    0.841481    4800        76.800000   0.020251    0.011368    1.306573    143.729916 
[37m[36mINFO[0m[0m 02/11 01:47:01 | 0.980565    0.968198    0.997400    0.802067    0.952213    0.980565    0.968198    0.994824    0.747646    0.998858    0.817073    0.998519    0.841481    5000        80.000000   0.012313    0.011010    1.292023    144.403682 
[37m[36mINFO[0m[0m 02/11 01:47:01 | Cumulative gradient change saved at train_output/VLCS/CORAL/[0]/250210_22-57-35_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/11 01:47:03 | ---
[37m[36mINFO[0m[0m 02/11 01:47:03 | test-domain validation(oracle) = 99.028%
[37m[36mINFO[0m[0m 02/11 01:47:03 | training-domain validation(iid) = 98.675%
[37m[36mINFO[0m[0m 02/11 01:47:03 | last = 98.057%
[37m[36mINFO[0m[0m 02/11 01:47:03 | last (inD) = 80.207%
[37m[36mINFO[0m[0m 02/11 01:47:03 | training-domain validation (iid, inD) = 82.479%
[37m[36mINFO[0m[0m 02/11 01:47:03 | === Summary ===
[37m[36mINFO[0m[0m 02/11 01:47:03 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 --dataset VLCS --trial_seed 1 --hparams_seed 12
[37m[36mINFO[0m[0m 02/11 01:47:03 | Unique name: 250210_22-57-35_resnet50_GENIE
[37m[36mINFO[0m[0m 02/11 01:47:03 | Out path: train_output/VLCS/CORAL/[0]/250210_22-57-35_resnet50_GENIE
[37m[36mINFO[0m[0m 02/11 01:47:03 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/11 01:47:03 | Dataset: VLCS
