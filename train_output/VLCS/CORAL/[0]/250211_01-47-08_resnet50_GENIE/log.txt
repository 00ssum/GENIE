[37m[36mINFO[0m[0m 02/11 01:47:08 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 --dataset VLCS --trial_seed 1 --hparams_seed 18
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 18
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[0]/250211_01-47-08_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 1
	unique_name: 250211_01-47-08_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.0002805925701721472
	batch_size: 39
	weight_decay: 1.5909976529017505e-06
	mmd_gamma: 0.7378941383723668
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/11 01:47:08 | n_steps = 5001
[37m[36mINFO[0m[0m 02/11 01:47:08 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/11 01:47:08 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/11 01:47:08 | 
[37m[36mINFO[0m[0m 02/11 01:47:08 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 02/11 01:47:08 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 02/11 01:47:08 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 02/11 01:47:08 | Batch sizes for each domain: [0, 39, 39, 39] (total=117)
[37m[36mINFO[0m[0m 02/11 01:47:08 | steps-per-epoch for each domain: 54.49, 67.33, 69.26 -> min = 54.49
[37m[36mINFO[0m[0m 02/11 01:47:10 | # of params = 23518277
[37m[36mINFO[0m[0m 02/11 01:49:34 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/11 01:49:34 | 0.613958    0.618375    0.432326    0.428842    1.423536    0.613958    0.618375    0.466824    0.461394    0.376618    0.419207    0.453536    0.405926    0           0.000000    1.795745    0.039490    1.654556    141.915863 
[37m[36mINFO[0m[0m 02/11 01:56:54 | 0.989399    0.971731    0.822628    0.805220    0.519922    0.989399    0.971731    0.769412    0.757062    0.821021    0.818598    0.877453    0.840000    200         3.670588    0.643658    0.031006    1.459717    148.255052 
[37m[36mINFO[0m[0m 02/11 02:04:05 | 0.985866    0.975265    0.851560    0.817527    0.493881    0.985866    0.975265    0.798588    0.753296    0.856055    0.842988    0.900037    0.856296    400         7.341176    0.455372    0.026136    1.444710    142.017872 
[37m[36mINFO[0m[0m 02/11 02:11:19 | 0.988516    0.985866    0.861697    0.810750    0.512734    0.988516    0.985866    0.821647    0.772128    0.854151    0.820122    0.909293    0.840000    600         11.011765   0.390548    0.023176    1.448829    143.944201 
[37m[36mINFO[0m[0m 02/11 02:18:37 | 0.990283    0.985866    0.894429    0.809891    0.516479    0.990283    0.985866    0.840941    0.745763    0.908987    0.823171    0.933358    0.860741    800         14.682353   0.322394    0.022576    1.450313    148.133983 
[37m[36mINFO[0m[0m 02/11 02:25:48 | 0.991166    0.975265    0.910228    0.822322    0.576633    0.991166    0.975265    0.888000    0.779661    0.907845    0.838415    0.934839    0.848889    1000        18.352941   0.277686    0.022423    1.437710    143.020018 
[37m[36mINFO[0m[0m 02/11 02:33:07 | 0.983216    0.975265    0.916645    0.806505    0.641448    0.983216    0.975265    0.888000    0.753296    0.907845    0.826220    0.954091    0.840000    1200        22.023529   0.224507    0.022789    1.454640    148.802667 
[37m[36mINFO[0m[0m 02/11 02:40:16 | 0.984982    0.975265    0.945572    0.796533    0.680457    0.984982    0.975265    0.934118    0.762712    0.945545    0.797256    0.957053    0.829630    1400        25.694118   0.182116    0.021832    1.444213    139.482130 
[37m[36mINFO[0m[0m 02/11 02:47:25 | 0.975265    0.978799    0.950778    0.801153    0.716214    0.975265    0.978799    0.936471    0.772128    0.943260    0.797256    0.972603    0.834074    1600        29.364706   0.153625    0.022038    1.429083    143.948722 
[37m[36mINFO[0m[0m 02/11 02:54:37 | 0.986749    0.982332    0.962254    0.807119    0.715701    0.986749    0.982332    0.944000    0.755179    0.965347    0.824695    0.977416    0.841481    1800        33.035294   0.122458    0.021534    1.450509    141.052161 
[37m[36mINFO[0m[0m 02/11 03:01:53 | 0.984099    0.975265    0.975746    0.799180    0.805177    0.984099    0.975265    0.967529    0.757062    0.975628    0.806402    0.984080    0.834074    2000        36.705882   0.092578    0.021617    1.448279    146.364611 
[37m[36mINFO[0m[0m 02/11 03:08:58 | 0.974382    0.971731    0.982644    0.792470    0.861487    0.974382    0.971731    0.978353    0.747646    0.984387    0.794207    0.985191    0.835556    2200        40.376471   0.075448    0.021287    1.429638    139.793392 
[37m[36mINFO[0m[0m 02/11 03:16:11 | 0.968198    0.971731    0.977429    0.780788    0.841179    0.968198    0.971731    0.973647    0.736347    0.977152    0.792683    0.981488    0.813333    2400        44.047059   0.059473    0.020279    1.450735    142.216402 
[37m[36mINFO[0m[0m 02/11 03:23:27 | 0.981449    0.968198    0.980440    0.803497    0.951989    0.981449    0.968198    0.977412    0.768362    0.980198    0.812500    0.983710    0.829630    2600        47.717647   0.070497    0.018169    1.449540    146.548147 
[37m[36mINFO[0m[0m 02/11 03:30:31 | 0.975265    0.971731    0.982694    0.798434    0.974523    0.975265    0.971731    0.968000    0.738230    0.989337    0.817073    0.990744    0.840000    2800        51.388235   0.049011    0.018537    1.423626    139.332380 
[37m[36mINFO[0m[0m 02/11 03:37:38 | 0.978799    0.971731    0.991891    0.792973    0.917448    0.978799    0.971731    0.989647    0.745763    0.990099    0.809451    0.995927    0.823704    3000        55.058824   0.038730    0.017369    1.426693    141.504409 
[37m[36mINFO[0m[0m 02/11 03:44:46 | 0.979682    0.978799    0.989579    0.797608    0.950567    0.979682    0.978799    0.981176    0.747646    0.992003    0.815549    0.995557    0.829630    3200        58.729412   0.045114    0.016455    1.419862    143.903985 
[37m[36mINFO[0m[0m 02/11 03:51:57 | 0.990283    0.978799    0.993724    0.803187    0.991942    0.990283    0.978799    0.994353    0.757062    0.992003    0.812500    0.994817    0.840000    3400        62.400000   0.030900    0.016772    1.455814    139.521009 
[37m[36mINFO[0m[0m 02/11 03:59:10 | 0.978799    0.975265    0.990412    0.800167    1.020765    0.978799    0.975265    0.990118    0.757062    0.986672    0.806402    0.994447    0.837037    3600        66.070588   0.026030    0.015481    1.453060    142.399639 
[37m[36mINFO[0m[0m 02/11 04:06:26 | 0.965548    0.957597    0.992750    0.788578    0.942383    0.965548    0.957597    0.989176    0.732580    0.993145    0.809451    0.995927    0.823704    3800        69.741176   0.026747    0.014477    1.478786    140.659590 
[37m[36mINFO[0m[0m 02/11 04:13:38 | 0.978799    0.978799    0.995271    0.795519    1.014653    0.978799    0.978799    0.992941    0.740113    0.996573    0.807927    0.996298    0.838519    4000        73.411765   0.021649    0.014126    1.455135    140.852494 
[37m[36mINFO[0m[0m 02/11 04:20:55 | 0.984982    0.975265    0.995817    0.800731    1.049244    0.984982    0.975265    0.993882    0.764595    0.995050    0.809451    0.998519    0.828148    4200        77.082353   0.016123    0.013497    1.459329    145.217668 
[37m[36mINFO[0m[0m 02/11 04:28:11 | 0.990283    0.989399    0.992572    0.805553    1.124334    0.990283    0.989399    0.986824    0.740113    0.992003    0.824695    0.998889    0.851852    4400        80.752941   0.017619    0.012806    1.460418    143.768156 
[37m[36mINFO[0m[0m 02/11 04:35:25 | 0.986749    0.982332    0.995216    0.803220    0.999285    0.986749    0.982332    0.994353    0.766478    0.993145    0.797256    0.998149    0.845926    4600        84.423529   0.018699    0.012443    1.441419    145.366225 
[37m[36mINFO[0m[0m 02/11 04:42:44 | 0.988516    0.985866    0.996422    0.808210    1.060256    0.988516    0.985866    0.995294    0.764595    0.996192    0.817073    0.997779    0.842963    4800        88.094118   0.013876    0.011988    1.457146    148.130971 
[37m[36mINFO[0m[0m 02/11 04:50:00 | 0.989399    0.985866    0.999006    0.799210    1.002276    0.989399    0.985866    1.000000    0.741996    0.999238    0.818598    0.997779    0.837037    5000        91.764706   0.012387    0.011649    1.449403    145.687417 
[37m[36mINFO[0m[0m 02/11 04:50:00 | Cumulative gradient change saved at train_output/VLCS/CORAL/[0]/250211_01-47-08_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/11 04:50:01 | ---
[37m[36mINFO[0m[0m 02/11 04:50:01 | test-domain validation(oracle) = 99.028%
[37m[36mINFO[0m[0m 02/11 04:50:01 | training-domain validation(iid) = 99.117%
[37m[36mINFO[0m[0m 02/11 04:50:01 | last = 98.940%
[37m[36mINFO[0m[0m 02/11 04:50:01 | last (inD) = 79.921%
[37m[36mINFO[0m[0m 02/11 04:50:01 | training-domain validation (iid, inD) = 82.232%
[37m[36mINFO[0m[0m 02/11 04:50:01 | === Summary ===
[37m[36mINFO[0m[0m 02/11 04:50:01 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 --dataset VLCS --trial_seed 1 --hparams_seed 18
[37m[36mINFO[0m[0m 02/11 04:50:01 | Unique name: 250211_01-47-08_resnet50_GENIE
[37m[36mINFO[0m[0m 02/11 04:50:01 | Out path: train_output/VLCS/CORAL/[0]/250211_01-47-08_resnet50_GENIE
[37m[36mINFO[0m[0m 02/11 04:50:01 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/11 04:50:01 | Dataset: VLCS
