[37m[36mINFO[0m[0m 02/10 16:44:11 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 --dataset VLCS --trial_seed 1 --hparams_seed 2
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 2
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[0]/250210_16-44-11_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[0]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0]
	trial_seed: 1
	unique_name: 250210_16-44-11_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.0001126313085293539
	batch_size: 38
	weight_decay: 0.006639128805224463
	mmd_gamma: 0.28205739923739065
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/10 16:44:11 | n_steps = 5001
[37m[36mINFO[0m[0m 02/10 16:44:11 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/10 16:44:11 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/10 16:44:11 | 
[37m[36mINFO[0m[0m 02/10 16:44:11 | Testenv name escaping te_C -> te_C
[37m[36mINFO[0m[0m 02/10 16:44:11 | Test envs = [0], name = te_C
[37m[36mINFO[0m[0m 02/10 16:44:11 | Train environments: [1, 2, 3], Test environments: [0]
[37m[36mINFO[0m[0m 02/10 16:44:11 | Batch sizes for each domain: [0, 38, 38, 38] (total=114)
[37m[36mINFO[0m[0m 02/10 16:44:11 | steps-per-epoch for each domain: 55.92, 69.11, 71.08 -> min = 55.92
[37m[36mINFO[0m[0m 02/10 16:44:13 | # of params = 23518277
[37m[36mINFO[0m[0m 02/10 16:46:47 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/10 16:46:47 | 0.613958    0.618375    0.432326    0.428842    1.428737    0.613958    0.618375    0.466824    0.461394    0.376618    0.419207    0.453536    0.405926    0           0.000000    1.817462    0.076645    1.499119    152.428097 
[37m[36mINFO[0m[0m 02/10 16:54:12 | 0.981449    0.982332    0.807764    0.799145    0.547159    0.981449    0.982332    0.769882    0.762712    0.798172    0.812500    0.855239    0.822222    200         3.576471    0.682530    0.047386    1.473927    151.049255 
[37m[36mINFO[0m[0m 02/10 17:01:36 | 0.980565    0.961131    0.828366    0.806426    0.535289    0.980565    0.961131    0.801412    0.766478    0.808454    0.823171    0.875231    0.829630    400         7.152941    0.479258    0.043769    1.452797    152.983017 
[37m[36mINFO[0m[0m 02/10 17:08:49 | 0.982332    0.985866    0.865037    0.825864    0.496946    0.982332    0.985866    0.808471    0.779661    0.876238    0.847561    0.910404    0.850370    600         10.729412   0.409183    0.039877    1.435821    145.751107 
[37m[36mINFO[0m[0m 02/10 17:16:02 | 0.990283    0.985866    0.879482    0.820743    0.497558    0.990283    0.985866    0.818824    0.757062    0.890708    0.841463    0.928915    0.863704    800         14.305882   0.363427    0.036424    1.445690    144.367752 
[37m[36mINFO[0m[0m 02/10 17:23:18 | 0.977915    0.978799    0.881733    0.800120    0.551803    0.977915    0.978799    0.824000    0.747646    0.894136    0.820122    0.927064    0.832593    1000        17.882353   0.348529    0.033211    1.421022    151.482155 
[37m[36mINFO[0m[0m 02/10 17:30:42 | 0.984099    0.985866    0.893697    0.808402    0.551437    0.984099    0.985866    0.859765    0.766478    0.898705    0.823171    0.922621    0.835556    1200        21.458824   0.303514    0.033062    1.477917    148.215929 
[37m[36mINFO[0m[0m 02/10 17:38:12 | 0.984099    0.982332    0.893180    0.792290    0.583393    0.984099    0.982332    0.827294    0.728814    0.918888    0.812500    0.933358    0.835556    1400        25.035294   0.258442    0.033325    1.509415    147.918953 
[37m[36mINFO[0m[0m 02/10 17:45:46 | 0.989399    0.982332    0.923009    0.818631    0.596306    0.989399    0.982332    0.898353    0.777778    0.915842    0.827744    0.954832    0.850370    1600        28.611765   0.247117    0.030967    1.528995    148.023937 
[37m[36mINFO[0m[0m 02/10 17:53:10 | 0.978799    0.978799    0.934904    0.783640    0.646402    0.978799    0.978799    0.908706    0.732580    0.948210    0.809451    0.947797    0.808889    1800        32.188235   0.217149    0.031360    1.494384    145.221828 
[37m[36mINFO[0m[0m 02/10 18:00:31 | 0.979682    0.968198    0.932907    0.789660    0.689265    0.979682    0.968198    0.925647    0.738230    0.935644    0.829268    0.937431    0.801481    2000        35.764706   0.196471    0.029224    1.465309    148.121787 
[37m[36mINFO[0m[0m 02/10 18:07:50 | 0.980565    0.982332    0.961780    0.796170    0.681496    0.980565    0.982332    0.938824    0.747646    0.967251    0.820122    0.979267    0.820741    2200        39.341176   0.184450    0.028111    1.473802    144.280138 
[37m[36mINFO[0m[0m 02/10 18:15:06 | 0.981449    0.978799    0.906839    0.769101    0.809444    0.981449    0.978799    0.832000    0.696798    0.931835    0.794207    0.956683    0.816296    2400        42.917647   0.161742    0.027958    1.434241    149.598049 
[37m[36mINFO[0m[0m 02/10 18:22:26 | 0.984099    0.982332    0.964445    0.795126    0.786758    0.984099    0.982332    0.944471    0.740113    0.971820    0.818598    0.977046    0.826667    2600        46.494118   0.162614    0.025405    1.449588    149.435218 
[37m[36mINFO[0m[0m 02/10 18:29:45 | 0.975265    0.971731    0.951327    0.775154    0.788986    0.975265    0.971731    0.942118    0.730697    0.959254    0.814024    0.952610    0.780741    2800        50.070588   0.128271    0.027225    1.474492    144.716511 
[37m[36mINFO[0m[0m 02/10 18:37:12 | 0.963781    0.968198    0.967748    0.777623    0.902195    0.963781    0.968198    0.961412    0.730697    0.971820    0.814024    0.970011    0.788148    3000        53.647059   0.142866    0.024056    1.458959    154.607844 
[37m[36mINFO[0m[0m 02/10 18:44:29 | 0.988516    0.982332    0.973401    0.791457    0.789448    0.988516    0.982332    0.968471    0.743879    0.968393    0.820122    0.983340    0.810370    3200        57.223529   0.126771    0.024058    1.463328    144.862929 
[37m[36mINFO[0m[0m 02/10 18:51:56 | 0.977915    0.964664    0.963355    0.780691    0.922186    0.977915    0.964664    0.958588    0.732580    0.958873    0.810976    0.972603    0.798519    3400        60.800000   0.136556    0.021768    1.486956    149.128359 
[37m[36mINFO[0m[0m 02/10 18:59:24 | 0.983216    0.978799    0.972297    0.779422    0.861170    0.983216    0.978799    0.958588    0.728814    0.978294    0.809451    0.980007    0.800000    3600        64.376471   0.122470    0.022834    1.508196    146.083893 
[37m[36mINFO[0m[0m 02/10 19:06:41 | 0.972615    0.957597    0.970158    0.769555    0.837983    0.972615    0.957597    0.960941    0.732580    0.969155    0.782012    0.980378    0.794074    3800        67.952941   0.129942    0.021375    1.443214    148.884783 
[37m[36mINFO[0m[0m 02/10 19:14:01 | 0.980565    0.975265    0.978931    0.788691    0.758694    0.980565    0.975265    0.977412    0.747646    0.977152    0.812500    0.982229    0.805926    4000        71.529412   0.148863    0.019048    1.474565    144.904639 
[37m[36mINFO[0m[0m 02/10 19:21:19 | 0.964664    0.978799    0.909146    0.740554    0.997345    0.964664    0.978799    0.865882    0.681733    0.920792    0.760671    0.940763    0.779259    4200        75.105882   0.110494    0.020461    1.461882    146.156381 
[37m[36mINFO[0m[0m 02/10 19:28:43 | 0.918728    0.950530    0.900926    0.704031    0.990305    0.918728    0.950530    0.911059    0.696798    0.885758    0.701220    0.905961    0.714074    4400        78.682353   0.131963    0.018309    1.504972    142.543911 
[37m[36mINFO[0m[0m 02/10 19:36:17 | 0.969081    0.964664    0.960536    0.771235    0.943869    0.969081    0.964664    0.951529    0.721281    0.961919    0.783537    0.968160    0.808889    4600        82.258824   0.150096    0.017029    1.471553    159.279842 
[37m[36mINFO[0m[0m 02/10 19:43:45 | 0.964664    0.968198    0.939811    0.756591    0.975333    0.964664    0.968198    0.922824    0.702448    0.943260    0.786585    0.953351    0.780741    4800        85.835294   0.110891    0.018431    1.492787    150.302870 
[37m[36mINFO[0m[0m 02/10 19:51:13 | 0.983216    0.971731    0.970636    0.765995    0.920799    0.983216    0.971731    0.954824    0.715631    0.974486    0.794207    0.982599    0.788148    5000        89.411765   0.111359    0.017977    1.517053    143.964645 
[37m[36mINFO[0m[0m 02/10 19:51:13 | Cumulative gradient change saved at train_output/VLCS/CORAL/[0]/250210_16-44-11_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/10 19:51:14 | ---
[37m[36mINFO[0m[0m 02/10 19:51:14 | test-domain validation(oracle) = 98.233%
[37m[36mINFO[0m[0m 02/10 19:51:14 | training-domain validation(iid) = 98.233%
[37m[36mINFO[0m[0m 02/10 19:51:14 | last = 98.322%
[37m[36mINFO[0m[0m 02/10 19:51:14 | last (inD) = 76.600%
[37m[36mINFO[0m[0m 02/10 19:51:14 | training-domain validation (iid, inD) = 82.586%
[37m[36mINFO[0m[0m 02/10 19:51:14 | === Summary ===
[37m[36mINFO[0m[0m 02/10 19:51:14 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 --dataset VLCS --trial_seed 1 --hparams_seed 2
[37m[36mINFO[0m[0m 02/10 19:51:14 | Unique name: 250210_16-44-11_resnet50_GENIE
[37m[36mINFO[0m[0m 02/10 19:51:14 | Out path: train_output/VLCS/CORAL/[0]/250210_16-44-11_resnet50_GENIE
[37m[36mINFO[0m[0m 02/10 19:51:14 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/10 19:51:14 | Dataset: VLCS
