[37m[36mINFO[0m[0m 02/20 16:08:58 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset VLCS --trial_seed 2 --hparams_seed 14
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 14
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[0, 1, 3]/250220_16-08-58_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 2
	unique_name: 250220_16-08-58_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 4.223376545632674e-05
	batch_size: 27
	weight_decay: 1.4009530778122364e-05
	mmd_gamma: 5.905916742185723
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/20 16:08:58 | n_steps = 5001
[37m[36mINFO[0m[0m 02/20 16:08:58 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/20 16:08:58 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/20 16:08:58 | 
[37m[36mINFO[0m[0m 02/20 16:08:58 | Testenv name escaping te_C_L_V -> te_C_L_V
[37m[36mINFO[0m[0m 02/20 16:08:58 | Test envs = [0, 1, 3], name = te_C_L_V
[37m[36mINFO[0m[0m 02/20 16:08:58 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/20 16:08:58 | Batch sizes for each domain: [0, 0, 27, 0] (total=27)
[37m[36mINFO[0m[0m 02/20 16:08:58 | steps-per-epoch for each domain: 97.26 -> min = 97.26
[37m[36mINFO[0m[0m 02/20 16:09:00 | # of params = 23518277
[37m[36mINFO[0m[0m 02/20 16:11:34 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/20 16:11:34 | 0.413261    0.418627    0.465727    0.500000    1.076181    0.327739    0.314488    0.467765    0.461394    0.465727    0.500000    0.444280    0.480000    0           0.000000    1.751786    0.000000    1.210525    152.969594 
[37m[36mINFO[0m[0m 02/20 16:14:41 | 0.635960    0.647266    0.832064    0.780488    0.541614    0.703180    0.703180    0.567529    0.583804    0.832064    0.780488    0.637171    0.654815    200         2.056359    0.577888    0.000000    0.204498    145.941065 
[37m[36mINFO[0m[0m 02/20 16:17:47 | 0.626150    0.634089    0.855674    0.814024    0.498895    0.651943    0.657244    0.621176    0.634652    0.855674    0.814024    0.605331    0.610370    400         4.112719    0.455492    0.000000    0.241260    138.033368 
[37m[36mINFO[0m[0m 02/20 16:20:55 | 0.667320    0.671981    0.866717    0.801829    0.503555    0.742049    0.731449    0.609412    0.625235    0.866717    0.801829    0.650500    0.659259    600         6.169078    0.393449    0.000000    0.246263    139.149257 
[37m[36mINFO[0m[0m 02/20 16:24:10 | 0.661797    0.666169    0.893374    0.791159    0.563722    0.724382    0.710247    0.619765    0.629002    0.893374    0.791159    0.641244    0.659259    800         8.225438    0.335843    0.000000    0.251487    144.348474 
[37m[36mINFO[0m[0m 02/20 16:27:23 | 0.620703    0.637251    0.939832    0.826220    0.499686    0.610424    0.607774    0.621176    0.661017    0.939832    0.826220    0.630507    0.642963    1000        10.281797   0.268078    0.000000    0.220758    148.535081 
[37m[36mINFO[0m[0m 02/20 16:30:41 | 0.516767    0.530232    0.901371    0.768293    0.687444    0.406360    0.416961    0.548235    0.557439    0.901371    0.768293    0.595705    0.616296    1200        12.338157   0.225168    0.000000    0.222920    154.123652 
[37m[36mINFO[0m[0m 02/20 16:33:59 | 0.542677    0.557901    0.912414    0.797256    0.700408    0.449647    0.484099    0.565647    0.576271    0.912414    0.797256    0.612736    0.613333    1400        14.394516   0.205845    0.000000    0.224126    152.362454 
[37m[36mINFO[0m[0m 02/20 16:37:19 | 0.486537    0.505519    0.932216    0.792683    0.855968    0.385159    0.438163    0.538353    0.548023    0.932216    0.792683    0.536098    0.530370    1600        16.450876   0.186139    0.000000    0.236746    152.671149 
[37m[36mINFO[0m[0m 02/20 16:40:35 | 0.607662    0.618130    0.966489    0.814024    0.690679    0.583922    0.575972    0.619294    0.638418    0.966489    0.814024    0.619770    0.640000    1800        18.507235   0.158133    0.000000    0.261916    144.567267 
[37m[36mINFO[0m[0m 02/20 16:43:52 | 0.619162    0.631201    0.976390    0.810976    0.695762    0.591873    0.593640    0.624000    0.642185    0.976390    0.810976    0.641614    0.657778    2000        20.563595   0.122196    0.000000    0.284950    139.378705 
[37m[36mINFO[0m[0m 02/20 16:47:14 | 0.600400    0.619531    0.959254    0.810976    0.795811    0.483216    0.494700    0.650824    0.677966    0.959254    0.810976    0.667160    0.685926    2200        22.619954   0.134810    0.000000    0.248202    152.478688 
[37m[36mINFO[0m[0m 02/20 16:50:40 | 0.649280    0.657862    0.961919    0.803354    0.789838    0.669611    0.678445    0.603294    0.619586    0.961919    0.803354    0.674935    0.675556    2400        24.676314   0.114103    0.000000    0.219738    161.849528 
[37m[36mINFO[0m[0m 02/20 16:54:09 | 0.588002    0.595821    0.941356    0.795732    0.775666    0.617491    0.618375    0.586353    0.612053    0.941356    0.795732    0.560163    0.557037    2600        26.732673   0.097320    0.000000    0.206130    167.617156 
[37m[36mINFO[0m[0m 02/20 16:57:30 | 0.568781    0.587909    0.975628    0.806402    0.802923    0.487633    0.494700    0.586353    0.608286    0.975628    0.806402    0.632358    0.660741    2800        28.789033   0.085128    0.000000    0.198896    161.590722 
[37m[36mINFO[0m[0m 02/20 17:00:48 | 0.536024    0.562786    0.985529    0.798780    0.761365    0.370141    0.399293    0.612235    0.632768    0.985529    0.798780    0.625694    0.656296    3000        30.845392   0.072278    0.000000    0.213495    155.022148 
[37m[36mINFO[0m[0m 02/20 17:04:01 | 0.580138    0.587150    0.963442    0.794207    0.801973    0.604240    0.593640    0.543059    0.557439    0.963442    0.794207    0.593114    0.610370    3200        32.901752   0.053951    0.000000    0.282290    137.104092 
[37m[36mINFO[0m[0m 02/20 17:07:34 | 0.666862    0.676100    0.982864    0.814024    0.909793    0.734099    0.717314    0.626353    0.659134    0.982864    0.814024    0.640133    0.651852    3400        34.958111   0.068992    0.000000    0.301811    152.149596 
[37m[36mINFO[0m[0m 02/20 17:10:56 | 0.630750    0.634541    0.976390    0.804878    0.857247    0.685512    0.657244    0.606588    0.627119    0.976390    0.804878    0.600148    0.619259    3600        37.014471   0.049915    0.000000    0.293004    143.629851 
[37m[36mINFO[0m[0m 02/20 17:14:21 | 0.595838    0.607653    0.970297    0.757622    0.863272    0.620141    0.611307    0.593882    0.610169    0.970297    0.757622    0.573491    0.601481    3800        39.070830   0.079329    0.000000    0.280980    148.469454 
[37m[36mINFO[0m[0m 02/20 17:17:50 | 0.655458    0.665809    0.980960    0.806402    1.107362    0.697880    0.685512    0.635765    0.670433    0.980960    0.806402    0.632729    0.641481    4000        41.127190   0.075670    0.000000    0.263171    156.794581 
[37m[36mINFO[0m[0m 02/20 17:21:12 | 0.564643    0.597185    0.992003    0.817073    0.976784    0.428445    0.452297    0.621647    0.666667    0.992003    0.817073    0.643836    0.672593    4200        43.183549   0.058629    0.000000    0.256782    150.670455 
[37m[36mINFO[0m[0m 02/20 17:24:42 | 0.541655    0.574678    0.990480    0.820122    1.189994    0.386042    0.427562    0.599529    0.632768    0.990480    0.820122    0.639393    0.663704    4400        45.239909   0.031012    0.000000    0.232802    163.235995 
[37m[36mINFO[0m[0m 02/20 17:28:10 | 0.587082    0.589775    0.990480    0.803354    0.980104    0.500000    0.494700    0.645176    0.655367    0.990480    0.803354    0.616068    0.619259    4600        47.296268   0.055903    0.000000    0.205714    167.275616 
[37m[36mINFO[0m[0m 02/20 17:31:25 | 0.590518    0.609719    0.974867    0.807927    0.953640    0.463781    0.459364    0.660235    0.698682    0.974867    0.807927    0.647538    0.671111    4800        49.352628   0.039785    0.000000    0.205072    153.109112 
[37m[36mINFO[0m[0m 02/20 17:34:27 | 0.493895    0.505228    0.990099    0.789634    0.924517    0.318905    0.332155    0.579294    0.589454    0.990099    0.789634    0.583488    0.594074    5000        51.408987   0.046059    0.000000    0.208870    140.929693 
[37m[36mINFO[0m[0m 02/20 17:34:27 | Cumulative gradient change saved at train_output/VLCS/CORAL/[0, 1, 3]/250220_16-08-58_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/20 17:34:29 | ---
[37m[36mINFO[0m[0m 02/20 17:34:29 | test-domain validation(oracle) = 66.686%
[37m[36mINFO[0m[0m 02/20 17:34:29 | training-domain validation(iid) = 62.070%
[37m[36mINFO[0m[0m 02/20 17:34:29 | last = 49.390%
[37m[36mINFO[0m[0m 02/20 17:34:29 | last (inD) = 78.963%
[37m[36mINFO[0m[0m 02/20 17:34:29 | training-domain validation (iid, inD) = 82.622%
[37m[36mINFO[0m[0m 02/20 17:34:29 | === Summary ===
[37m[36mINFO[0m[0m 02/20 17:34:29 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset VLCS --trial_seed 2 --hparams_seed 14
[37m[36mINFO[0m[0m 02/20 17:34:29 | Unique name: 250220_16-08-58_resnet50_GENIE
[37m[36mINFO[0m[0m 02/20 17:34:29 | Out path: train_output/VLCS/CORAL/[0, 1, 3]/250220_16-08-58_resnet50_GENIE
[37m[36mINFO[0m[0m 02/20 17:34:29 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/20 17:34:29 | Dataset: VLCS
