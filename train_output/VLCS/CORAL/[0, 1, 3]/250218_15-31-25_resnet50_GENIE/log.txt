[37m[36mINFO[0m[0m 02/18 15:31:25 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset VLCS --trial_seed 0 --hparams_seed 18
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 18
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[0, 1, 3]/250218_15-31-25_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 0
	unique_name: 250218_15-31-25_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 0.00021894640154701087
	batch_size: 13
	weight_decay: 7.1565884139944e-05
	mmd_gamma: 0.48667511803994756
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/18 15:31:25 | n_steps = 5001
[37m[36mINFO[0m[0m 02/18 15:31:25 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/18 15:31:25 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/18 15:31:25 | 
[37m[36mINFO[0m[0m 02/18 15:31:25 | Testenv name escaping te_C_L_V -> te_C_L_V
[37m[36mINFO[0m[0m 02/18 15:31:25 | Test envs = [0, 1, 3], name = te_C_L_V
[37m[36mINFO[0m[0m 02/18 15:31:25 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/18 15:31:25 | Batch sizes for each domain: [0, 0, 13, 0] (total=13)
[37m[36mINFO[0m[0m 02/18 15:31:25 | steps-per-epoch for each domain: 202.00 -> min = 202.00
[37m[36mINFO[0m[0m 02/18 15:31:26 | # of params = 23518277
[37m[36mINFO[0m[0m 02/18 15:33:43 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/18 15:33:43 | 0.290355    0.276479    0.416603    0.414634    1.394761    0.113074    0.084806    0.471059    0.455744    0.416603    0.414634    0.286931    0.288889    0           0.000000    1.758802    0.000000    1.107405    136.236562 
[37m[36mINFO[0m[0m 02/18 15:36:28 | 0.663062    0.669341    0.805027    0.774390    0.556825    0.748233    0.749117    0.616000    0.623352    0.805027    0.774390    0.624954    0.635556    200         0.990099    0.712113    0.000000    0.128104    139.297560 
[37m[36mINFO[0m[0m 02/18 15:39:18 | 0.677354    0.667325    0.816451    0.769817    0.613958    0.740283    0.734982    0.639059    0.619586    0.816451    0.769817    0.652721    0.647407    400         1.980198    0.547345    0.000000    0.129223    143.727196 
[37m[36mINFO[0m[0m 02/18 15:42:15 | 0.501871    0.499173    0.763138    0.750000    0.624740    0.427562    0.427562    0.537882    0.508475    0.763138    0.750000    0.540170    0.561481    600         2.970297    0.511993    0.000000    0.151753    147.033343 
[37m[36mINFO[0m[0m 02/18 15:44:54 | 0.541020    0.526570    0.802361    0.737805    0.827993    0.528269    0.526502    0.581647    0.548023    0.802361    0.737805    0.513143    0.505185    800         3.960396    0.430398    0.000000    0.129357    133.321479 
[37m[36mINFO[0m[0m 02/18 15:47:52 | 0.659245    0.662473    0.861386    0.785061    0.551273    0.737633    0.731449    0.598118    0.602637    0.861386    0.785061    0.641984    0.653333    1000        4.950495    0.429666    0.000000    0.163826    144.368412 
[37m[36mINFO[0m[0m 02/18 15:50:44 | 0.666740    0.674490    0.844631    0.785061    0.562347    0.742933    0.734982    0.635294    0.664783    0.844631    0.785061    0.621992    0.623704    1200        5.940594    0.429372    0.000000    0.125249    147.222538 
[37m[36mINFO[0m[0m 02/18 15:53:37 | 0.665104    0.668206    0.886139    0.775915    0.643057    0.742933    0.731449    0.629647    0.634652    0.886139    0.775915    0.622732    0.638519    1400        6.930693    0.373808    0.000000    0.141886    144.997013 
[37m[36mINFO[0m[0m 02/18 15:56:16 | 0.667887    0.684430    0.860625    0.766768    0.621839    0.722615    0.727915    0.661647    0.677966    0.860625    0.766768    0.619400    0.647407    1600        7.920792    0.308094    0.000000    0.136681    131.535696 
[37m[36mINFO[0m[0m 02/18 15:59:09 | 0.617679    0.617229    0.914318    0.780488    0.609509    0.649293    0.660777    0.604706    0.610169    0.914318    0.780488    0.599037    0.580741    1800        8.910891    0.313038    0.000000    0.122547    148.577473 
[37m[36mINFO[0m[0m 02/18 16:01:56 | 0.562860    0.563815    0.874714    0.760671    0.742651    0.531802    0.515901    0.613647    0.621469    0.874714    0.760671    0.543132    0.554074    2000        9.900990    0.309935    0.000000    0.130544    140.237193 
[37m[36mINFO[0m[0m 02/18 16:04:40 | 0.670015    0.685067    0.931455    0.801829    0.620929    0.699647    0.713781    0.634353    0.662900    0.931455    0.801829    0.676046    0.678519    2200        10.891089   0.264502    0.000000    0.152022    134.389695 
[37m[36mINFO[0m[0m 02/18 16:07:16 | 0.648145    0.650398    0.888043    0.756098    0.947692    0.721731    0.713781    0.570353    0.578154    0.888043    0.756098    0.652351    0.659259    2400        11.881188   0.263263    0.000000    0.125409    130.124946 
[37m[36mINFO[0m[0m 02/18 16:10:07 | 0.658488    0.666106    0.933740    0.775915    0.699888    0.699647    0.717314    0.619765    0.615819    0.933740    0.775915    0.656053    0.665185    2600        12.871287   0.233548    0.000000    0.146366    141.900285 
[37m[36mINFO[0m[0m 02/18 16:12:58 | 0.628833    0.636761    0.843107    0.722561    0.792709    0.681979    0.692580    0.629176    0.617702    0.843107    0.722561    0.575342    0.600000    2800        13.861386   0.232246    0.000000    0.138199    143.236829 
[37m[36mINFO[0m[0m 02/18 16:15:40 | 0.693919    0.698052    0.890327    0.794207    0.829126    0.770318    0.759717    0.635765    0.644068    0.890327    0.794207    0.675676    0.690370    3000        14.851485   0.236765    0.000000    0.133692    135.787117 
[37m[36mINFO[0m[0m 02/18 16:18:32 | 0.655941    0.669408    0.945925    0.778963    0.866837    0.707597    0.724382    0.600471    0.608286    0.945925    0.778963    0.659756    0.675556    3200        15.841584   0.193599    0.000000    0.156912    140.585844 
[37m[36mINFO[0m[0m 02/18 16:21:19 | 0.543995    0.559114    0.914699    0.760671    0.880821    0.422261    0.459364    0.613647    0.612053    0.914699    0.760671    0.596076    0.605926    3400        16.831683   0.195034    0.000000    0.136093    139.103713 
[37m[36mINFO[0m[0m 02/18 16:24:09 | 0.653693    0.668888    0.902513    0.769817    0.896563    0.716431    0.738516    0.648941    0.666667    0.902513    0.769817    0.595705    0.601481    3600        17.821782   0.182933    0.000000    0.192641    131.449923 
[37m[36mINFO[0m[0m 02/18 16:27:04 | 0.522544    0.547862    0.925743    0.745427    0.906221    0.459364    0.515901    0.545882    0.549906    0.925743    0.745427    0.562384    0.577778    3800        18.811881   0.174466    0.000000    0.129701    149.236194 
[37m[36mINFO[0m[0m 02/18 16:29:39 | 0.614838    0.630997    0.939832    0.771341    0.801456    0.583039    0.611307    0.597647    0.612053    0.939832    0.771341    0.663828    0.669630    4000        19.801980   0.226837    0.000000    0.131784    129.088490 
[37m[36mINFO[0m[0m 02/18 16:32:31 | 0.645363    0.651597    0.915080    0.745427    0.921193    0.713781    0.724382    0.563294    0.580038    0.915080    0.745427    0.659015    0.650370    4200        20.792079   0.161145    0.000000    0.178364    136.290366 
[37m[36mINFO[0m[0m 02/18 16:35:28 | 0.652125    0.662855    0.932597    0.778963    0.900088    0.664311    0.671378    0.628235    0.653484    0.932597    0.778963    0.663828    0.663704    4400        21.782178   0.138226    0.000000    0.144319    148.159748 
[37m[36mINFO[0m[0m 02/18 16:38:16 | 0.553077    0.591102    0.945164    0.782012    0.674706    0.434629    0.501767    0.612235    0.647834    0.945164    0.782012    0.612366    0.623704    4600        22.772277   0.152786    0.000000    0.129426    141.975589 
[37m[36mINFO[0m[0m 02/18 16:41:03 | 0.534066    0.538955    0.939452    0.762195    0.880259    0.454947    0.466431    0.563765    0.559322    0.939452    0.762195    0.583488    0.591111    4800        23.762376   0.177464    0.000000    0.143917    138.408897 
[37m[36mINFO[0m[0m 02/18 16:43:53 | 0.512165    0.527137    0.955446    0.778963    0.948427    0.303004    0.325088    0.634824    0.645951    0.955446    0.778963    0.598667    0.610370    5000        24.752475   0.128137    0.000000    0.171736    135.902971 
[37m[36mINFO[0m[0m 02/18 16:43:54 | Cumulative gradient change saved at train_output/VLCS/CORAL/[0, 1, 3]/250218_15-31-25_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/18 16:43:55 | ---
[37m[36mINFO[0m[0m 02/18 16:43:55 | test-domain validation(oracle) = 69.392%
[37m[36mINFO[0m[0m 02/18 16:43:55 | training-domain validation(iid) = 67.002%
[37m[36mINFO[0m[0m 02/18 16:43:55 | last = 51.216%
[37m[36mINFO[0m[0m 02/18 16:43:55 | last (inD) = 77.896%
[37m[36mINFO[0m[0m 02/18 16:43:55 | training-domain validation (iid, inD) = 80.183%
[37m[36mINFO[0m[0m 02/18 16:43:55 | === Summary ===
[37m[36mINFO[0m[0m 02/18 16:43:55 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset VLCS --trial_seed 0 --hparams_seed 18
[37m[36mINFO[0m[0m 02/18 16:43:55 | Unique name: 250218_15-31-25_resnet50_GENIE
[37m[36mINFO[0m[0m 02/18 16:43:55 | Out path: train_output/VLCS/CORAL/[0, 1, 3]/250218_15-31-25_resnet50_GENIE
[37m[36mINFO[0m[0m 02/18 16:43:55 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/18 16:43:55 | Dataset: VLCS
