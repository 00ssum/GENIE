[37m[36mINFO[0m[0m 02/18 19:26:55 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset VLCS --trial_seed 1 --hparams_seed 0
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[0, 1, 3]/250218_19-26-55_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 1
	unique_name: 250218_19-26-55_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	mmd_gamma: 1.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/18 19:26:55 | n_steps = 5001
[37m[36mINFO[0m[0m 02/18 19:26:55 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/18 19:26:55 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/18 19:26:55 | 
[37m[36mINFO[0m[0m 02/18 19:26:55 | Testenv name escaping te_C_L_V -> te_C_L_V
[37m[36mINFO[0m[0m 02/18 19:26:55 | Test envs = [0, 1, 3], name = te_C_L_V
[37m[36mINFO[0m[0m 02/18 19:26:55 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/18 19:26:55 | Batch sizes for each domain: [0, 0, 32, 0] (total=32)
[37m[36mINFO[0m[0m 02/18 19:26:55 | steps-per-epoch for each domain: 82.06 -> min = 82.06
[37m[36mINFO[0m[0m 02/18 19:26:57 | # of params = 23518277
[37m[36mINFO[0m[0m 02/18 19:29:07 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/18 19:29:07 | 0.304457    0.307509    0.552932    0.548780    1.066286    0.170495    0.159011    0.447059    0.433145    0.552932    0.548780    0.295816    0.330370    0           0.000000    1.573447    0.000000    1.335571    128.449381 
[37m[36mINFO[0m[0m 02/18 19:32:24 | 0.485371    0.481908    0.832064    0.812500    0.493523    0.278269    0.282686    0.616941    0.604520    0.832064    0.812500    0.560903    0.558519    200         2.437167    0.517413    0.000000    0.292997    138.370182 
[37m[36mINFO[0m[0m 02/18 19:35:27 | 0.568873    0.552985    0.875095    0.842988    0.420996    0.477915    0.462898    0.614118    0.585687    0.875095    0.842988    0.614587    0.610370    400         4.874334    0.451679    0.000000    0.231758    136.862868 
[37m[36mINFO[0m[0m 02/18 19:38:31 | 0.677007    0.662112    0.865194    0.830793    0.495713    0.711131    0.706714    0.659765    0.644068    0.865194    0.830793    0.660126    0.635556    600         7.311500    0.336010    0.000000    0.250171    134.398471 
[37m[36mINFO[0m[0m 02/18 19:41:43 | 0.628524    0.612799    0.921554    0.853659    0.480265    0.564488    0.561837    0.648000    0.615819    0.921554    0.853659    0.673084    0.660741    800         9.748667    0.241839    0.000000    0.253734    141.301637 
[37m[36mINFO[0m[0m 02/18 19:45:06 | 0.546065    0.548787    0.884235    0.759146    0.821097    0.434629    0.431095    0.610824    0.627119    0.884235    0.759146    0.592743    0.588148    1000        12.185834   0.193406    0.000000    0.274132    147.311836 
[37m[36mINFO[0m[0m 02/18 19:48:22 | 0.655426    0.645532    0.935263    0.841463    0.616857    0.691696    0.692580    0.612235    0.574388    0.935263    0.841463    0.662347    0.669630    1200        14.623001   0.147320    0.000000    0.270129    142.409438 
