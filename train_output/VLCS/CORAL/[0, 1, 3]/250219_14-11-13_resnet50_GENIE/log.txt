[37m[36mINFO[0m[0m 02/19 14:11:13 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 3 --dataset VLCS --trial_seed 1 --hparams_seed 16
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 16
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[0, 1, 3]/250219_14-11-13_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 1
	unique_name: 250219_14-11-13_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 1.3198033474735314e-05
	batch_size: 10
	weight_decay: 0.001844990356992375
	mmd_gamma: 1.2838669915289476
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/19 14:11:13 | n_steps = 5001
[37m[36mINFO[0m[0m 02/19 14:11:13 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/19 14:11:13 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/19 14:11:13 | 
[37m[36mINFO[0m[0m 02/19 14:11:13 | Testenv name escaping te_C_L_V -> te_C_L_V
[37m[36mINFO[0m[0m 02/19 14:11:13 | Test envs = [0, 1, 3], name = te_C_L_V
[37m[36mINFO[0m[0m 02/19 14:11:13 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 02/19 14:11:13 | Batch sizes for each domain: [0, 0, 10, 0] (total=10)
[37m[36mINFO[0m[0m 02/19 14:11:13 | steps-per-epoch for each domain: 262.60 -> min = 262.60
[37m[36mINFO[0m[0m 02/19 14:11:14 | # of params = 23518277
[37m[36mINFO[0m[0m 02/19 14:13:23 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/19 14:13:23 | 0.174391    0.194438    0.413938    0.419207    1.053424    0.113074    0.109541    0.244235    0.248588    0.413938    0.419207    0.165864    0.225185    0           0.000000    1.495080    0.000000    1.172309    127.830907 
[37m[36mINFO[0m[0m 02/19 14:15:52 | 0.650000    0.629128    0.718964    0.745427    0.672206    0.668728    0.653710    0.669647    0.664783    0.718964    0.745427    0.611625    0.568889    200         0.761615    0.778654    0.000000    0.126741    123.888160 
[37m[36mINFO[0m[0m 02/19 14:18:25 | 0.552323    0.556693    0.763519    0.763720    0.605042    0.509717    0.526502    0.563765    0.548023    0.763519    0.763720    0.583488    0.595556    400         1.523229    0.634474    0.000000    0.114076    129.981591 
[37m[36mINFO[0m[0m 02/19 14:21:03 | 0.467242    0.472241    0.762757    0.740854    0.633348    0.340106    0.346290    0.568471    0.559322    0.762757    0.740854    0.493151    0.511111    600         2.284844    0.580632    0.000000    0.118206    133.835067 
[37m[36mINFO[0m[0m 02/19 14:23:39 | 0.459695    0.451569    0.746382    0.733232    0.633522    0.292403    0.279152    0.578353    0.555556    0.746382    0.733232    0.508330    0.520000    800         3.046458    0.532150    0.000000    0.110894    134.523482 
[37m[36mINFO[0m[0m 02/19 14:26:31 | 0.609301    0.608866    0.822925    0.795732    0.529577    0.613074    0.611307    0.620235    0.606403    0.822925    0.795732    0.594595    0.608889    1000        3.808073    0.541360    0.000000    0.134069    145.117160 
[37m[36mINFO[0m[0m 02/19 14:29:50 | 0.651764    0.643187    0.846535    0.797256    0.525626    0.729682    0.720848    0.611765    0.589454    0.846535    0.797256    0.613847    0.619259    1200        4.569688    0.551950    0.000000    0.133343    172.160829 
[37m[36mINFO[0m[0m 02/19 14:32:58 | 0.629473    0.623656    0.848819    0.815549    0.547071    0.661661    0.650177    0.636235    0.625235    0.848819    0.815549    0.590522    0.595556    1400        5.331302    0.457069    0.000000    0.106323    166.998406 
[37m[36mINFO[0m[0m 02/19 14:36:01 | 0.621911    0.619313    0.866717    0.820122    0.507187    0.635159    0.636042    0.626353    0.602637    0.866717    0.820122    0.604221    0.619259    1600        6.092917    0.401495    0.000000    0.120925    158.574636 
[37m[36mINFO[0m[0m 02/19 14:39:07 | 0.639684    0.623426    0.870526    0.842988    0.452948    0.648410    0.643110    0.614588    0.585687    0.870526    0.842988    0.656053    0.641481    1800        6.854532    0.401993    0.000000    0.136374    158.613276 
[37m[36mINFO[0m[0m 02/19 14:42:12 | 0.642216    0.629845    0.880807    0.809451    0.522022    0.640459    0.618375    0.642353    0.625235    0.880807    0.809451    0.643836    0.645926    2000        7.616146    0.377460    0.000000    0.176251    149.644043 
