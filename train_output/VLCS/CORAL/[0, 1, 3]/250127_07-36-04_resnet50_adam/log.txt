[37m[36mINFO[0m[0m 01/27 07:36:04 | Command :: /jsm0707/Large-scale/train_all.py resnet50_adam config/resnet50_adam.yaml --algorithm CORAL --test_envs 0 1 3 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_adam
	out_dir: train_output/VLCS/CORAL/[0, 1, 3]/250127_07-36-04_resnet50_adam
	out_root: train_output/VLCS/CORAL/[0, 1, 3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 3]
	trial_seed: 0
	unique_name: 250127_07-36-04_resnet50_adam
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	mmd_gamma: 1.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 01/27 07:36:04 | n_steps = 5001
[37m[36mINFO[0m[0m 01/27 07:36:04 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 01/27 07:36:04 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 01/27 07:36:04 | 
[37m[36mINFO[0m[0m 01/27 07:36:04 | Testenv name escaping te_C_L_V -> te_C_L_V
[37m[36mINFO[0m[0m 01/27 07:36:04 | Test envs = [0, 1, 3], name = te_C_L_V
[37m[36mINFO[0m[0m 01/27 07:36:04 | Train environments: [2], Test environments: [0, 1, 3]
[37m[36mINFO[0m[0m 01/27 07:36:04 | Batch sizes for each domain: [0, 0, 32, 0] (total=32)
[37m[36mINFO[0m[0m 01/27 07:36:04 | steps-per-epoch for each domain: 82.06 -> min = 82.06
[37m[36mINFO[0m[0m 01/27 07:36:05 | # of params = 23518277
[37m[36mINFO[0m[0m 01/27 07:38:09 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 01/27 07:38:09 | 0.266543    0.253872    0.454684    0.408537    1.331692    0.085689    0.060071    0.465882    0.448211    0.454684    0.408537    0.248056    0.253333    0           0.000000    1.675835    0.000000    0.879189    123.682842 
[37m[36mINFO[0m[0m 01/27 07:41:05 | 0.653218    0.649473    0.825590    0.762195    0.630098    0.734982    0.720848    0.583059    0.587571    0.825590    0.762195    0.641614    0.640000    200         2.437167    0.527980    0.000000    0.246959    126.013062 
[37m[36mINFO[0m[0m 01/27 07:44:02 | 0.682947    0.678176    0.903656    0.798780    0.535429    0.751767    0.742049    0.606588    0.593220    0.903656    0.798780    0.690485    0.699259    400         4.874334    0.334116    0.000000    0.240486    128.238432 
[37m[36mINFO[0m[0m 01/27 07:46:56 | 0.648595    0.645599    0.897182    0.769817    0.876572    0.740283    0.731449    0.599059    0.587571    0.897182    0.769817    0.606442    0.617778    600         7.311500    0.221643    0.000000    0.242369    125.136312 
[37m[36mINFO[0m[0m 01/27 07:49:51 | 0.587941    0.609619    0.924219    0.772866    0.632231    0.505300    0.586572    0.564706    0.559322    0.924219    0.772866    0.693817    0.682963    800         9.748667    0.186083    0.000000    0.246255    126.551403 
[37m[36mINFO[0m[0m 01/27 07:52:42 | 0.700239    0.714698    0.955446    0.775915    1.020678    0.806537    0.826855    0.605176    0.612053    0.955446    0.775915    0.689004    0.705185    1000        12.185834   0.128700    0.000000    0.240414    122.624349 
[37m[36mINFO[0m[0m 01/27 07:55:38 | 0.623195    0.630141    0.969916    0.803354    1.053882    0.674912    0.678445    0.565647    0.563089    0.969916    0.803354    0.629026    0.648889    1200        14.623001   0.094472    0.000000    0.240554    127.096222 
[37m[36mINFO[0m[0m 01/27 07:58:35 | 0.691556    0.684065    0.976009    0.786585    0.824915    0.733216    0.734982    0.642824    0.632768    0.976009    0.786585    0.698630    0.684444    1400        17.060168   0.095780    0.000000    0.247464    128.033413 
[37m[36mINFO[0m[0m 01/27 08:01:33 | 0.696925    0.697438    0.946687    0.778963    0.975652    0.813604    0.802120    0.589647    0.589454    0.946687    0.778963    0.687523    0.700741    1600        19.497334   0.084840    0.000000    0.244206    128.451334 
[37m[36mINFO[0m[0m 01/27 08:04:32 | 0.548745    0.566175    0.942498    0.750000    1.232895    0.498233    0.554770    0.530824    0.514124    0.942498    0.750000    0.617179    0.629630    1800        21.934501   0.070946    0.000000    0.244912    130.023717 
[37m[36mINFO[0m[0m 01/27 08:07:31 | 0.642061    0.646449    0.980198    0.785061    1.053148    0.640459    0.660777    0.636706    0.625235    0.980198    0.785061    0.649019    0.653333    2000        24.371668   0.069489    0.000000    0.239939    130.927129 
[37m[36mINFO[0m[0m 01/27 08:10:28 | 0.549755    0.558249    0.981721    0.785061    1.082916    0.409894    0.399293    0.615529    0.638418    0.981721    0.785061    0.623843    0.637037    2200        26.808835   0.043459    0.000000    0.243252    129.060657 
[37m[36mINFO[0m[0m 01/27 08:13:26 | 0.545813    0.569866    0.985910    0.795732    1.176478    0.431979    0.477032    0.579765    0.576271    0.985910    0.795732    0.625694    0.656296    2400        29.246002   0.047436    0.000000    0.236751    130.229782 
[37m[36mINFO[0m[0m 01/27 08:16:21 | 0.631315    0.629175    0.989718    0.798780    1.563180    0.635159    0.632509    0.619765    0.612053    0.989718    0.798780    0.639023    0.642963    2600        31.683168   0.040931    0.000000    0.237387    127.711128 
[37m[36mINFO[0m[0m 01/27 08:19:16 | 0.667935    0.667989    0.959634    0.759146    1.315575    0.756184    0.756184    0.588235    0.578154    0.959634    0.759146    0.659385    0.669630    2800        34.120335   0.044091    0.000000    0.240484    126.481910 
[37m[36mINFO[0m[0m 01/27 08:22:14 | 0.698144    0.712884    0.994669    0.783537    1.266793    0.827739    0.823322    0.622118    0.630885    0.994669    0.783537    0.644576    0.684444    3000        36.557502   0.055050    0.000000    0.246078    128.962378 
[37m[36mINFO[0m[0m 01/27 08:25:12 | 0.558546    0.563988    0.977913    0.771341    0.827880    0.467314    0.480565    0.601882    0.595104    0.977913    0.771341    0.606442    0.616296    3200        38.994669   0.076935    0.000000    0.242360    129.670523 
[37m[36mINFO[0m[0m 01/27 08:28:10 | 0.585346    0.604906    0.993526    0.777439    1.040965    0.497350    0.537102    0.619294    0.634652    0.993526    0.777439    0.639393    0.642963    3400        41.431835   0.049793    0.000000    0.236296    130.825060 
[37m[36mINFO[0m[0m 01/27 08:31:08 | 0.588282    0.601188    0.994288    0.797256    0.879133    0.541519    0.565371    0.608000    0.602637    0.994288    0.797256    0.615328    0.635556    3600        43.869002   0.030110    0.000000    0.247948    128.496884 
[37m[36mINFO[0m[0m 01/27 08:34:07 | 0.633557    0.653645    0.995811    0.794207    1.198497    0.636926    0.664311    0.611765    0.619586    0.995811    0.794207    0.651981    0.677037    3800        46.306169   0.029993    0.000000    0.246846    129.419053 
[37m[36mINFO[0m[0m 01/27 08:37:03 | 0.581630    0.605574    0.992765    0.791159    1.209856    0.536219    0.590106    0.594824    0.596987    0.992765    0.791159    0.613847    0.629630    4000        48.743336   0.030159    0.000000    0.240116    127.595666 
[37m[36mINFO[0m[0m 01/27 08:39:58 | 0.615023    0.645878    0.988957    0.789634    1.131245    0.583039    0.625442    0.636706    0.664783    0.988957    0.789634    0.625324    0.647407    4200        51.180503   0.036199    0.000000    0.241270    127.400431 
[37m[36mINFO[0m[0m 01/27 08:42:54 | 0.589650    0.603928    0.990099    0.778963    1.257634    0.485866    0.508834    0.622588    0.621469    0.990099    0.778963    0.660496    0.681481    4400        53.617669   0.013456    0.000000    0.247242    125.865932 
[37m[36mINFO[0m[0m 01/27 08:45:51 | 0.647389    0.658657    0.993145    0.785061    1.157787    0.727915    0.756184    0.574118    0.564972    0.993145    0.785061    0.640133    0.654815    4600        56.054836   0.057364    0.000000    0.244025    128.749506 
[37m[36mINFO[0m[0m 01/27 08:48:47 | 0.566061    0.581316    0.992003    0.795732    1.207886    0.416078    0.452297    0.624941    0.610169    0.992003    0.795732    0.657164    0.681481    4800        58.492003   0.031752    0.000000    0.240635    127.549900 
[37m[36mINFO[0m[0m 01/27 08:51:47 | 0.674090    0.686358    0.995050    0.782012    1.183580    0.746466    0.773852    0.579765    0.580038    0.995050    0.782012    0.696039    0.705185    5000        60.929170   0.028602    0.000000    0.252862    129.223915 
[37m[36mINFO[0m[0m 01/27 08:51:47 | Cumulative gradient change saved at train_output/VLCS/CORAL/[0, 1, 3]/250127_07-36-04_resnet50_adam/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 01/27 08:51:48 | ---
[37m[36mINFO[0m[0m 01/27 08:51:48 | test-domain validation(oracle) = 70.024%
[37m[36mINFO[0m[0m 01/27 08:51:48 | training-domain validation(iid) = 62.320%
[37m[36mINFO[0m[0m 01/27 08:51:48 | last = 67.409%
[37m[36mINFO[0m[0m 01/27 08:51:48 | last (inD) = 78.201%
[37m[36mINFO[0m[0m 01/27 08:51:48 | training-domain validation (iid, inD) = 80.335%
[37m[36mINFO[0m[0m 01/27 08:51:48 | === Summary ===
[37m[36mINFO[0m[0m 01/27 08:51:48 | Command: /jsm0707/Large-scale/train_all.py resnet50_adam config/resnet50_adam.yaml --algorithm CORAL --test_envs 0 1 3 --dataset VLCS
[37m[36mINFO[0m[0m 01/27 08:51:48 | Unique name: 250127_07-36-04_resnet50_adam
[37m[36mINFO[0m[0m 01/27 08:51:48 | Out path: train_output/VLCS/CORAL/[0, 1, 3]/250127_07-36-04_resnet50_adam
[37m[36mINFO[0m[0m 01/27 08:51:48 | Algorithm: CORAL
[37m[36mINFO[0m[0m 01/27 08:51:48 | Dataset: VLCS
