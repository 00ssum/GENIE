[37m[36mINFO[0m[0m 02/23 12:05:57 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 2 --dataset VLCS --trial_seed 2 --hparams_seed 9
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 9
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[0, 1, 2]/250223_12-05-57_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[0, 1, 2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 2]
	trial_seed: 2
	unique_name: 250223_12-05-57_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 2.716671579524612e-05
	batch_size: 29
	weight_decay: 1.71368232883332e-06
	mmd_gamma: 1.254370045611671
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/23 12:05:57 | n_steps = 5001
[37m[36mINFO[0m[0m 02/23 12:05:57 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/23 12:05:57 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/23 12:05:57 | 
[37m[36mINFO[0m[0m 02/23 12:05:57 | Testenv name escaping te_C_L_S -> te_C_L_S
[37m[36mINFO[0m[0m 02/23 12:05:57 | Test envs = [0, 1, 2], name = te_C_L_S
[37m[36mINFO[0m[0m 02/23 12:05:57 | Train environments: [3], Test environments: [0, 1, 2]
[37m[36mINFO[0m[0m 02/23 12:05:57 | Batch sizes for each domain: [0, 0, 0, 29] (total=29)
[37m[36mINFO[0m[0m 02/23 12:05:57 | steps-per-epoch for each domain: 93.14 -> min = 93.14
[37m[36mINFO[0m[0m 02/23 12:05:58 | # of params = 23518277
[37m[36mINFO[0m[0m 02/23 12:08:35 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/23 12:08:35 | 0.493095    0.470461    0.440207    0.459259    1.682047    0.620141    0.593640    0.467294    0.459510    0.391851    0.358232    0.440207    0.459259    0           0.000000    1.896542    0.000000    1.303348    155.711731 
[37m[36mINFO[0m[0m 02/23 12:11:44 | 0.735297    0.741004    0.861163    0.838519    0.432956    0.938163    0.925795    0.527059    0.551789    0.740670    0.745427    0.861163    0.838519    200         2.147353    0.539149    0.000000    0.162501    155.560880 
[37m[36mINFO[0m[0m 02/23 12:14:47 | 0.781577    0.783100    0.906701    0.845926    0.412200    0.992933    0.989399    0.584471    0.600753    0.767327    0.759146    0.906701    0.845926    400         4.294706    0.319927    0.000000    0.163382    150.329618 
[37m[36mINFO[0m[0m 02/23 12:18:01 | 0.764487    0.772505    0.931507    0.860741    0.429782    0.977915    0.968198    0.566118    0.593220    0.749429    0.756098    0.931507    0.860741    600         6.442058    0.250852    0.000000    0.174748    159.080599 
[37m[36mINFO[0m[0m 02/23 12:21:07 | 0.774098    0.769248    0.917068    0.869630    0.456590    0.969081    0.968198    0.593882    0.613936    0.759330    0.725610    0.917068    0.869630    800         8.589411    0.229079    0.000000    0.194627    147.185041 
[37m[36mINFO[0m[0m 02/23 12:24:18 | 0.790502    0.795373    0.953351    0.857778    0.519350    0.988516    0.985866    0.612235    0.636535    0.770754    0.763720    0.953351    0.857778    1000        10.736764   0.173371    0.000000    0.160729    159.232101 
[37m[36mINFO[0m[0m 02/23 12:27:34 | 0.792365    0.797154    0.969641    0.874074    0.477452    0.975265    0.964664    0.621176    0.647834    0.780655    0.778963    0.969641    0.874074    1200        12.884117   0.139708    0.000000    0.163406    163.121578 
[37m[36mINFO[0m[0m 02/23 12:30:37 | 0.762505    0.784122    0.959274    0.844444    0.481368    0.974382    0.982332    0.585412    0.613936    0.727723    0.756098    0.959274    0.844444    1400        15.031470   0.125749    0.000000    0.183286    146.089641 
[37m[36mINFO[0m[0m 02/23 12:33:46 | 0.779304    0.773635    0.961496    0.856296    0.601764    0.971731    0.957597    0.592000    0.602637    0.774181    0.760671    0.961496    0.856296    1600        17.178823   0.108672    0.000000    0.209026    147.334654 
[37m[36mINFO[0m[0m 02/23 12:37:01 | 0.783407    0.771830    0.975935    0.871111    0.561473    0.969965    0.971731    0.609882    0.612053    0.770373    0.731707    0.975935    0.871111    1800        19.326175   0.086163    0.000000    0.184779    158.682763 
[37m[36mINFO[0m[0m 02/23 12:40:23 | 0.784841    0.784038    0.982969    0.869630    0.525967    0.984099    0.975265    0.612235    0.617702    0.758187    0.759146    0.982969    0.869630    2000        21.473528   0.073189    0.000000    0.209332    159.217211 
[37m[36mINFO[0m[0m 02/23 12:43:29 | 0.732039    0.738488    0.963347    0.847407    0.603164    0.957597    0.950530    0.525647    0.542373    0.712871    0.722561    0.963347    0.847407    2200        23.620881   0.066581    0.000000    0.184739    149.411461 
[37m[36mINFO[0m[0m 02/23 12:46:52 | 0.793502    0.791923    0.991485    0.872593    0.603812    0.975265    0.964664    0.631059    0.653484    0.774181    0.757622    0.991485    0.872593    2400        25.768234   0.077215    0.000000    0.212302    160.455093 
[37m[36mINFO[0m[0m 02/23 12:50:12 | 0.790923    0.786292    0.982969    0.845926    0.758892    0.978799    0.961131    0.625882    0.655367    0.768088    0.742378    0.982969    0.845926    2600        27.915587   0.052534    0.000000    0.185226    162.871425 
[37m[36mINFO[0m[0m 02/23 12:53:23 | 0.789669    0.799044    0.978526    0.851852    0.613136    0.977032    0.975265    0.617412    0.645951    0.774562    0.775915    0.978526    0.851852    2800        30.062940   0.050703    0.000000    0.194537    151.998080 
[37m[36mINFO[0m[0m 02/23 12:56:32 | 0.779631    0.790053    0.987782    0.837037    0.704364    0.980565    0.985866    0.612706    0.632768    0.745621    0.751524    0.987782    0.837037    3000        32.210292   0.043899    0.000000    0.242862    140.709594 
[37m[36mINFO[0m[0m 02/23 12:59:48 | 0.771176    0.766497    0.948908    0.840000    0.880041    0.963781    0.950530    0.613647    0.623352    0.736101    0.725610    0.948908    0.840000    3200        34.357645   0.028836    0.000000    0.224362    151.358950 
[37m[36mINFO[0m[0m 02/23 13:03:07 | 0.784875    0.787763    0.977046    0.826667    0.787243    0.980565    0.971731    0.635294    0.662900    0.738766    0.728659    0.977046    0.826667    3400        36.504998   0.054995    0.000000    0.225812    153.133451 
[37m[36mINFO[0m[0m 02/23 13:06:09 | 0.726060    0.741763    0.981118    0.804444    0.995624    0.910777    0.911661    0.555294    0.581921    0.712110    0.731707    0.981118    0.804444    3600        38.652351   0.037732    0.000000    0.212866    140.030748 
[37m[36mINFO[0m[0m 02/23 13:09:26 | 0.774565    0.792104    0.992966    0.850370    0.732103    0.979682    0.982332    0.615529    0.651601    0.728484    0.742378    0.992966    0.850370    3800        40.799704   0.045881    0.000000    0.223552    152.089816 
[37m[36mINFO[0m[0m 02/23 13:12:56 | 0.772411    0.785323    0.997408    0.850370    0.729777    0.969081    0.957597    0.603294    0.634652    0.744859    0.763720    0.997408    0.850370    4000        42.947057   0.028608    0.000000    0.199864    169.526925 
[37m[36mINFO[0m[0m 02/23 13:16:17 | 0.782820    0.787846    0.989633    0.857778    0.579495    0.978799    0.978799    0.612235    0.627119    0.757426    0.757622    0.989633    0.857778    4200        45.094409   0.041305    0.000000    0.195498    162.098129 
[37m[36mINFO[0m[0m 02/23 13:19:35 | 0.773598    0.777659    0.988893    0.847407    0.606759    0.969081    0.971731    0.600000    0.615819    0.751714    0.745427    0.988893    0.847407    4400        47.241762   0.030147    0.000000    0.254286    146.949827 
[37m[36mINFO[0m[0m 02/23 13:22:48 | 0.769402    0.774693    0.994447    0.854815    0.705607    0.963781    0.961131    0.579765    0.600753    0.764661    0.762195    0.994447    0.854815    4600        49.389115   0.027929    0.000000    0.208719    151.347881 
[37m[36mINFO[0m[0m 02/23 13:26:16 | 0.779586    0.792772    0.998519    0.854815    0.752566    0.978799    0.985866    0.603294    0.619586    0.756664    0.772866    0.998519    0.854815    4800        51.536468   0.012120    0.000000    0.238296    160.261305 
[37m[36mINFO[0m[0m 02/23 13:29:34 | 0.790445    0.796209    0.997408    0.865185    0.927133    0.964664    0.968198    0.622588    0.638418    0.784082    0.782012    0.997408    0.865185    5000        53.683821   0.007552    0.000000    0.201898    157.659700 
[37m[36mINFO[0m[0m 02/23 13:29:34 | Cumulative gradient change saved at train_output/VLCS/CORAL/[0, 1, 2]/250223_12-05-57_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/23 13:29:35 | ---
[37m[36mINFO[0m[0m 02/23 13:29:35 | test-domain validation(oracle) = 78.967%
[37m[36mINFO[0m[0m 02/23 13:29:35 | training-domain validation(iid) = 79.237%
[37m[36mINFO[0m[0m 02/23 13:29:35 | last = 79.044%
[37m[36mINFO[0m[0m 02/23 13:29:35 | last (inD) = 86.519%
[37m[36mINFO[0m[0m 02/23 13:29:35 | training-domain validation (iid, inD) = 87.407%
[37m[36mINFO[0m[0m 02/23 13:29:35 | === Summary ===
[37m[36mINFO[0m[0m 02/23 13:29:35 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 2 --dataset VLCS --trial_seed 2 --hparams_seed 9
[37m[36mINFO[0m[0m 02/23 13:29:35 | Unique name: 250223_12-05-57_resnet50_GENIE
[37m[36mINFO[0m[0m 02/23 13:29:35 | Out path: train_output/VLCS/CORAL/[0, 1, 2]/250223_12-05-57_resnet50_GENIE
[37m[36mINFO[0m[0m 02/23 13:29:35 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/23 13:29:35 | Dataset: VLCS
