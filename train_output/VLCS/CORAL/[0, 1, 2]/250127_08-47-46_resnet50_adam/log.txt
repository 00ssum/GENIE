[37m[36mINFO[0m[0m 01/27 08:47:46 | Command :: /jsm0707/Large-scale/train_all.py resnet50_adam config/resnet50_adam.yaml --algorithm CORAL --test_envs 0 1 2 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_adam.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_adam
	out_dir: train_output/VLCS/CORAL/[0, 1, 2]/250127_08-47-46_resnet50_adam
	out_root: train_output/VLCS/CORAL/[0, 1, 2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 2]
	trial_seed: 0
	unique_name: 250127_08-47-46_resnet50_adam
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: adam
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	mmd_gamma: 1.0
	swad: False
	swad_kwargs: 
	  n_converge: 3
	  n_tolerance: 6
	  tolerance_ratio: 0.3
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	ld: 0.1
	lr_mult: 10.0
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 01/27 08:47:46 | n_steps = 5001
[37m[36mINFO[0m[0m 01/27 08:47:46 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 01/27 08:47:46 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 01/27 08:47:46 | 
[37m[36mINFO[0m[0m 01/27 08:47:46 | Testenv name escaping te_C_L_S -> te_C_L_S
[37m[36mINFO[0m[0m 01/27 08:47:46 | Test envs = [0, 1, 2], name = te_C_L_S
[37m[36mINFO[0m[0m 01/27 08:47:46 | Train environments: [3], Test environments: [0, 1, 2]
[37m[36mINFO[0m[0m 01/27 08:47:46 | Batch sizes for each domain: [0, 0, 0, 32] (total=32)
[37m[36mINFO[0m[0m 01/27 08:47:46 | steps-per-epoch for each domain: 84.41 -> min = 84.41
[37m[36mINFO[0m[0m 01/27 08:47:48 | # of params = 23518277
[37m[36mINFO[0m[0m 01/27 08:49:55 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 01/27 08:49:55 | 0.468687    0.482627    0.456868    0.463704    1.488309    0.497350    0.498233    0.453647    0.487759    0.455065    0.461890    0.456868    0.463704    0           0.000000    1.766220    0.000000    1.299640    126.044480 
[37m[36mINFO[0m[0m 01/27 08:52:37 | 0.750708    0.731548    0.875231    0.840000    0.492234    0.979682    0.975265    0.547765    0.527307    0.724676    0.692073    0.875231    0.840000    200         2.369493    0.479769    0.000000    0.170629    127.240783 
[37m[36mINFO[0m[0m 01/27 08:55:19 | 0.772156    0.759139    0.917808    0.838519    0.568139    0.977032    0.975265    0.591529    0.585687    0.747906    0.716463    0.917808    0.838519    400         4.738986    0.276341    0.000000    0.172573    127.437692 
[37m[36mINFO[0m[0m 01/27 08:58:00 | 0.758537    0.753195    0.931877    0.837037    0.499011    0.962014    0.968198    0.561882    0.561205    0.751714    0.730183    0.931877    0.837037    600         7.108478    0.201972    0.000000    0.170408    126.404226 
[37m[36mINFO[0m[0m 01/27 09:00:41 | 0.782471    0.762049    0.950759    0.832593    0.604815    0.977915    0.961131    0.593412    0.585687    0.776085    0.739329    0.950759    0.832593    800         9.477971    0.136771    0.000000    0.179955    125.582784 
[37m[36mINFO[0m[0m 01/27 09:03:25 | 0.748111    0.721959    0.960385    0.819259    0.676734    0.957597    0.950530    0.554824    0.546139    0.731912    0.669207    0.960385    0.819259    1000        11.847464   0.134279    0.000000    0.177455    127.773639 
[37m[36mINFO[0m[0m 01/27 09:06:06 | 0.732664    0.719634    0.962606    0.823704    0.663308    0.981449    0.978799    0.524235    0.497175    0.692308    0.682927    0.962606    0.823704    1200        14.216957   0.094815    0.000000    0.183652    124.802930 
[37m[36mINFO[0m[0m 01/27 09:08:46 | 0.755988    0.737047    0.984820    0.837037    0.694913    0.938163    0.922261    0.584941    0.580038    0.744859    0.708841    0.984820    0.837037    1400        16.586449   0.092092    0.000000    0.169828    126.090432 
[37m[36mINFO[0m[0m 01/27 09:11:25 | 0.728603    0.716089    0.972233    0.794074    0.919503    0.961131    0.964664    0.536941    0.523540    0.687738    0.660061    0.972233    0.794074    1600        18.955942   0.052137    0.000000    0.169161    125.394178 
[37m[36mINFO[0m[0m 01/27 09:14:05 | 0.738010    0.710718    0.984450    0.826667    0.559861    0.927562    0.897527    0.561412    0.559322    0.725057    0.675305    0.984450    0.826667    1800        21.325435   0.066319    0.000000    0.168799    126.018600 
[37m[36mINFO[0m[0m 01/27 09:16:48 | 0.739026    0.723996    0.976305    0.795556    0.954010    0.933746    0.925795    0.560941    0.548023    0.722391    0.698171    0.976305    0.795556    2000        23.694928   0.074463    0.000000    0.174372    127.599521 
[37m[36mINFO[0m[0m 01/27 09:19:30 | 0.764256    0.762026    0.990744    0.835556    0.806664    0.967314    0.971731    0.582118    0.585687    0.743336    0.728659    0.990744    0.835556    2200        26.064421   0.053562    0.000000    0.172756    127.695852 
[37m[36mINFO[0m[0m 01/27 09:22:11 | 0.768470    0.750444    0.979267    0.810370    0.797623    0.922261    0.915194    0.615059    0.612053    0.768088    0.724085    0.979267    0.810370    2400        28.433913   0.057249    0.000000    0.178757    125.319018 
[37m[36mINFO[0m[0m 01/27 09:24:52 | 0.752136    0.761697    0.963717    0.811852    0.824506    0.923145    0.936396    0.623059    0.645951    0.710206    0.702744    0.963717    0.811852    2600        30.803406   0.040511    0.000000    0.170854    127.230006 
[37m[36mINFO[0m[0m 01/27 09:27:32 | 0.737722    0.730896    0.977786    0.811852    0.816145    0.954947    0.950530    0.551059    0.534840    0.707159    0.707317    0.977786    0.811852    2800        33.172899   0.036443    0.000000    0.170968    125.355972 
[37m[36mINFO[0m[0m 01/27 09:30:14 | 0.781191    0.772608    0.990744    0.841481    0.761320    0.972615    0.971731    0.607059    0.608286    0.763899    0.737805    0.990744    0.841481    3000        35.542392   0.044367    0.000000    0.176560    126.162458 
[37m[36mINFO[0m[0m 01/27 09:32:55 | 0.765889    0.740079    0.988153    0.819259    0.769900    0.958481    0.943463    0.588235    0.572505    0.750952    0.704268    0.988153    0.819259    3200        37.911884   0.041184    0.000000    0.175100    125.695389 
[37m[36mINFO[0m[0m 01/27 09:35:38 | 0.766440    0.748532    0.995557    0.835556    0.794562    0.954064    0.950530    0.582118    0.572505    0.763138    0.722561    0.995557    0.835556    3400        40.281377   0.041401    0.000000    0.173384    128.824802 
[37m[36mINFO[0m[0m 01/27 09:38:23 | 0.747395    0.720347    0.971862    0.819259    0.769829    0.895760    0.837456    0.596235    0.610169    0.750190    0.713415    0.971862    0.819259    3600        42.650870   0.056379    0.000000    0.178141    128.688807 
[37m[36mINFO[0m[0m 01/27 09:41:05 | 0.758922    0.747845    0.992225    0.823704    0.742432    0.950530    0.950530    0.580235    0.585687    0.746002    0.707317    0.992225    0.823704    3800        45.020363   0.053571    0.000000    0.168029    128.931856 
[37m[36mINFO[0m[0m 01/27 09:43:49 | 0.752719    0.739636    0.994817    0.816296    0.958828    0.951413    0.936396    0.569882    0.570621    0.736862    0.711890    0.994817    0.816296    4000        47.389856   0.023926    0.000000    0.176246    129.095260 
[37m[36mINFO[0m[0m 01/27 09:46:34 | 0.754390    0.740683    0.991114    0.848889    0.992672    0.969081    0.971731    0.569412    0.553672    0.724676    0.696646    0.991114    0.848889    4200        49.759348   0.022461    0.000000    0.174529    129.587996 
[37m[36mINFO[0m[0m 01/27 09:49:16 | 0.742155    0.740450    0.995187    0.813333    0.920830    0.960247    0.964664    0.559059    0.563089    0.707159    0.693598    0.995187    0.813333    4400        52.128841   0.046254    0.000000    0.175830    127.381399 
[37m[36mINFO[0m[0m 01/27 09:51:59 | 0.755723    0.745376    0.996298    0.829630    0.829494    0.968198    0.954064    0.567059    0.576271    0.731912    0.705793    0.996298    0.829630    4600        54.498334   0.027021    0.000000    0.176991    127.530223 
[37m[36mINFO[0m[0m 01/27 09:54:40 | 0.774803    0.764131    0.997779    0.838519    1.088668    0.954947    0.957597    0.621176    0.629002    0.748286    0.705793    0.997779    0.838519    4800        56.867827   0.014694    0.000000    0.174135    126.148681 
[37m[36mINFO[0m[0m 01/27 09:57:24 | 0.739610    0.734603    0.996298    0.819259    0.986216    0.958481    0.968198    0.556235    0.540490    0.704113    0.695122    0.996298    0.819259    5000        59.237320   0.037214    0.000000    0.170574    129.949694 
[37m[36mINFO[0m[0m 01/27 09:57:25 | Cumulative gradient change saved at train_output/VLCS/CORAL/[0, 1, 2]/250127_08-47-46_resnet50_adam/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 01/27 09:57:26 | ---
[37m[36mINFO[0m[0m 01/27 09:57:26 | test-domain validation(oracle) = 78.119%
[37m[36mINFO[0m[0m 01/27 09:57:26 | training-domain validation(iid) = 75.439%
[37m[36mINFO[0m[0m 01/27 09:57:26 | last = 73.961%
[37m[36mINFO[0m[0m 01/27 09:57:26 | last (inD) = 81.926%
[37m[36mINFO[0m[0m 01/27 09:57:26 | training-domain validation (iid, inD) = 84.889%
[37m[36mINFO[0m[0m 01/27 09:57:26 | === Summary ===
[37m[36mINFO[0m[0m 01/27 09:57:26 | Command: /jsm0707/Large-scale/train_all.py resnet50_adam config/resnet50_adam.yaml --algorithm CORAL --test_envs 0 1 2 --dataset VLCS
[37m[36mINFO[0m[0m 01/27 09:57:26 | Unique name: 250127_08-47-46_resnet50_adam
[37m[36mINFO[0m[0m 01/27 09:57:26 | Out path: train_output/VLCS/CORAL/[0, 1, 2]/250127_08-47-46_resnet50_adam
[37m[36mINFO[0m[0m 01/27 09:57:26 | Algorithm: CORAL
[37m[36mINFO[0m[0m 01/27 09:57:26 | Dataset: VLCS
