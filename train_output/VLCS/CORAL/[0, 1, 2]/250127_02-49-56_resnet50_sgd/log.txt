[37m[36mINFO[0m[0m 01/27 02:49:56 | Command :: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm CORAL --test_envs 0 1 2 --dataset VLCS
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 0
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/VLCS/CORAL/[0, 1, 2]/250127_02-49-56_resnet50_sgd
	out_root: train_output/VLCS/CORAL/[0, 1, 2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 2]
	trial_seed: 0
	unique_name: 250127_02-49-56_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 5e-05
	batch_size: 32
	weight_decay: 0.0
	mmd_gamma: 1.0
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 01/27 02:49:56 | n_steps = 5001
[37m[36mINFO[0m[0m 01/27 02:49:56 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 01/27 02:49:56 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 01/27 02:49:56 | 
[37m[36mINFO[0m[0m 01/27 02:49:56 | Testenv name escaping te_C_L_S -> te_C_L_S
[37m[36mINFO[0m[0m 01/27 02:49:56 | Test envs = [0, 1, 2], name = te_C_L_S
[37m[36mINFO[0m[0m 01/27 02:49:56 | Train environments: [3], Test environments: [0, 1, 2]
[37m[36mINFO[0m[0m 01/27 02:49:56 | Batch sizes for each domain: [0, 0, 0, 32] (total=32)
[37m[36mINFO[0m[0m 01/27 02:49:56 | steps-per-epoch for each domain: 84.41 -> min = 84.41
[37m[36mINFO[0m[0m 01/27 02:49:58 | # of params = 23518277
[37m[36mINFO[0m[0m 01/27 02:52:05 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 01/27 02:52:05 | 0.155478    0.132159    0.131803    0.120000    1.834981    0.091873    0.081272    0.186824    0.158192    0.187738    0.157012    0.131803    0.120000    0           0.000000    1.766220    0.000000    1.558586    125.711880 
[37m[36mINFO[0m[0m 01/27 02:54:46 | 0.475933    0.495335    0.442058    0.454815    1.388400    0.558304    0.568905    0.469647    0.500942    0.399848    0.416159    0.442058    0.454815    200         2.369493    1.504311    0.000000    0.176002    125.077289 
[37m[36mINFO[0m[0m 01/27 02:57:33 | 0.512877    0.533096    0.466494    0.482963    1.273588    0.609541    0.632509    0.494588    0.521657    0.434501    0.445122    0.466494    0.482963    400         4.738986    1.319294    0.000000    0.169367    132.867368 
[37m[36mINFO[0m[0m 01/27 03:00:16 | 0.583685    0.594129    0.542392    0.577778    1.165521    0.647527    0.657244    0.574588    0.600753    0.528941    0.524390    0.542392    0.577778    600         7.108478    1.191186    0.000000    0.173106    128.031370 
[37m[36mINFO[0m[0m 01/27 03:02:57 | 0.666860    0.657924    0.623473    0.651852    1.043238    0.712898    0.703180    0.629647    0.644068    0.658035    0.626524    0.623473    0.651852    800         9.477971    1.071617    0.000000    0.169539    126.426231 
[37m[36mINFO[0m[0m 01/27 03:05:37 | 0.707021    0.687032    0.700852    0.702222    0.916879    0.760601    0.727915    0.633882    0.636535    0.726580    0.696646    0.700852    0.702222    1000        11.847464   0.953413    0.000000    0.167393    126.120512 
[37m[36mINFO[0m[0m 01/27 03:08:22 | 0.740067    0.738227    0.762310    0.762963    0.794619    0.826855    0.812721    0.630588    0.653484    0.762757    0.748476    0.762310    0.762963    1200        14.216957   0.821469    0.000000    0.171273    130.553004 
[37m[36mINFO[0m[0m 01/27 03:11:05 | 0.762939    0.760205    0.803776    0.783704    0.680322    0.881625    0.876325    0.635294    0.649718    0.771896    0.754573    0.803776    0.783704    1400        16.586449   0.705008    0.000000    0.170441    128.336168 
[37m[36mINFO[0m[0m 01/27 03:13:47 | 0.773406    0.766059    0.827101    0.798519    0.606063    0.922261    0.918728    0.618824    0.623352    0.779132    0.756098    0.827101    0.798519    1600        18.955942   0.609805    0.000000    0.174908    126.990799 
[37m[36mINFO[0m[0m 01/27 03:16:29 | 0.778660    0.765946    0.837468    0.805926    0.546834    0.939929    0.929329    0.618824    0.613936    0.777228    0.754573    0.837468    0.805926    1800        21.325435   0.541295    0.000000    0.166065    127.967566 
[37m[36mINFO[0m[0m 01/27 03:19:10 | 0.779154    0.770030    0.846353    0.814815    0.508904    0.950530    0.943463    0.608941    0.612053    0.777989    0.754573    0.846353    0.814815    2000        23.694928   0.491804    0.000000    0.169658    127.900183 
[37m[36mINFO[0m[0m 01/27 03:21:52 | 0.773770    0.756351    0.851536    0.823704    0.486704    0.961131    0.946996    0.592471    0.578154    0.767708    0.743902    0.851536    0.823704    2200        26.064421   0.466959    0.000000    0.169475    128.075166 
[37m[36mINFO[0m[0m 01/27 03:24:32 | 0.774992    0.756273    0.854498    0.838519    0.467044    0.964664    0.950530    0.589176    0.574388    0.771135    0.743902    0.854498    0.838519    2400        28.433913   0.444575    0.000000    0.166462    126.361417 
[37m[36mINFO[0m[0m 01/27 03:27:14 | 0.775619    0.760817    0.861903    0.838519    0.447879    0.964664    0.950530    0.591059    0.581921    0.771135    0.750000    0.861903    0.838519    2600        30.803406   0.432567    0.000000    0.180597    126.322295 
[37m[36mINFO[0m[0m 01/27 03:29:55 | 0.782591    0.769241    0.866346    0.844444    0.426006    0.969965    0.957597    0.599059    0.589454    0.778751    0.760671    0.866346    0.844444    2800        33.172899   0.414801    0.000000    0.174728    125.638594 
[37m[36mINFO[0m[0m 01/27 03:32:34 | 0.777468    0.762240    0.873380    0.838519    0.427253    0.976148    0.964664    0.585882    0.578154    0.770373    0.743902    0.873380    0.838519    3000        35.542392   0.394543    0.000000    0.165130    125.178123 
[37m[36mINFO[0m[0m 01/27 03:35:16 | 0.781926    0.766891    0.875602    0.847407    0.410763    0.976148    0.961131    0.590118    0.581921    0.779513    0.757622    0.875602    0.847407    3200        37.911884   0.378348    0.000000    0.184031    125.450855 
[37m[36mINFO[0m[0m 01/27 03:37:56 | 0.778422    0.761697    0.871899    0.847407    0.402767    0.977915    0.971731    0.581647    0.572505    0.775704    0.740854    0.871899    0.847407    3400        40.281377   0.358407    0.000000    0.175931    124.834780 
[37m[36mINFO[0m[0m 01/27 03:40:38 | 0.783610    0.772027    0.873380    0.857778    0.387878    0.977032    0.968198    0.592000    0.585687    0.781797    0.762195    0.873380    0.857778    3600        42.650870   0.347705    0.000000    0.176000    126.617518 
[37m[36mINFO[0m[0m 01/27 03:43:18 | 0.779665    0.768033    0.880415    0.856296    0.389224    0.978799    0.971731    0.582588    0.576271    0.777609    0.756098    0.880415    0.856296    3800        45.020363   0.360286    0.000000    0.165365    126.910130 
[37m[36mINFO[0m[0m 01/27 03:46:00 | 0.776657    0.758211    0.881525    0.853333    0.391127    0.981449    0.975265    0.571294    0.563089    0.777228    0.736280    0.881525    0.853333    4000        47.389856   0.345716    0.000000    0.170042    127.350345 
[37m[36mINFO[0m[0m 01/27 03:48:41 | 0.785049    0.774024    0.880415    0.857778    0.374474    0.980565    0.975265    0.590118    0.580038    0.784463    0.766768    0.880415    0.857778    4200        49.759348   0.322276    0.000000    0.169698    126.997608 
[37m[36mINFO[0m[0m 01/27 03:51:19 | 0.782211    0.771244    0.892262    0.857778    0.368942    0.980565    0.975265    0.585412    0.576271    0.780655    0.762195    0.892262    0.857778    4400        52.128841   0.334658    0.000000    0.170272    124.508649 
[37m[36mINFO[0m[0m 01/27 03:54:00 | 0.772927    0.762199    0.888930    0.862222    0.371730    0.980565    0.978799    0.565176    0.559322    0.773039    0.748476    0.888930    0.862222    4600        54.498334   0.301206    0.000000    0.174149    126.150232 
[37m[36mINFO[0m[0m 01/27 03:56:44 | 0.779515    0.771722    0.893003    0.860741    0.366287    0.978799    0.975265    0.580235    0.583804    0.779513    0.756098    0.893003    0.860741    4800        56.867827   0.304482    0.000000    0.179725    127.941432 
[37m[36mINFO[0m[0m 01/27 03:59:25 | 0.778598    0.767263    0.887449    0.862222    0.363188    0.983216    0.982332    0.574588    0.572505    0.777989    0.746951    0.887449    0.862222    5000        59.237320   0.301565    0.000000    0.179046    124.875323 
[37m[36mINFO[0m[0m 01/27 03:59:25 | Cumulative gradient change saved at train_output/VLCS/CORAL/[0, 1, 2]/250127_02-49-56_resnet50_sgd/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 01/27 03:59:26 | ---
[37m[36mINFO[0m[0m 01/27 03:59:26 | test-domain validation(oracle) = 78.505%
[37m[36mINFO[0m[0m 01/27 03:59:26 | training-domain validation(iid) = 77.293%
[37m[36mINFO[0m[0m 01/27 03:59:26 | last = 77.860%
[37m[36mINFO[0m[0m 01/27 03:59:26 | last (inD) = 86.222%
[37m[36mINFO[0m[0m 01/27 03:59:26 | training-domain validation (iid, inD) = 86.222%
[37m[36mINFO[0m[0m 01/27 03:59:26 | === Summary ===
[37m[36mINFO[0m[0m 01/27 03:59:26 | Command: /jsm0707/Large-scale/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm CORAL --test_envs 0 1 2 --dataset VLCS
[37m[36mINFO[0m[0m 01/27 03:59:26 | Unique name: 250127_02-49-56_resnet50_sgd
[37m[36mINFO[0m[0m 01/27 03:59:26 | Out path: train_output/VLCS/CORAL/[0, 1, 2]/250127_02-49-56_resnet50_sgd
[37m[36mINFO[0m[0m 01/27 03:59:26 | Algorithm: CORAL
[37m[36mINFO[0m[0m 01/27 03:59:26 | Dataset: VLCS
