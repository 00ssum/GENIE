[37m[36mINFO[0m[0m 02/23 18:44:44 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 0 1 2 --dataset VLCS --trial_seed 2 --hparams_seed 14
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 14
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[0, 1, 2]/250223_18-44-44_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[0, 1, 2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [0, 1, 2]
	trial_seed: 2
	unique_name: 250223_18-44-44_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 4.223376545632674e-05
	batch_size: 27
	weight_decay: 1.4009530778122364e-05
	mmd_gamma: 5.905916742185723
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/23 18:44:44 | n_steps = 5001
[37m[36mINFO[0m[0m 02/23 18:44:44 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/23 18:44:44 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/23 18:44:44 | 
[37m[36mINFO[0m[0m 02/23 18:44:44 | Testenv name escaping te_C_L_S -> te_C_L_S
[37m[36mINFO[0m[0m 02/23 18:44:44 | Test envs = [0, 1, 2], name = te_C_L_S
[37m[36mINFO[0m[0m 02/23 18:44:44 | Train environments: [3], Test environments: [0, 1, 2]
[37m[36mINFO[0m[0m 02/23 18:44:44 | Batch sizes for each domain: [0, 0, 0, 27] (total=27)
[37m[36mINFO[0m[0m 02/23 18:44:44 | steps-per-epoch for each domain: 100.04 -> min = 100.04
[37m[36mINFO[0m[0m 02/23 18:44:46 | # of params = 23518277
[37m[36mINFO[0m[0m 02/23 18:47:11 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/23 18:47:11 | 0.493095    0.470461    0.440207    0.459259    1.801094    0.620141    0.593640    0.467294    0.459510    0.391851    0.358232    0.440207    0.459259    0           0.000000    1.933892    0.000000    1.492929    143.742393 
[37m[36mINFO[0m[0m 02/23 18:50:17 | 0.789455    0.781145    0.876712    0.865185    0.390471    0.991166    0.985866    0.585882    0.604520    0.791318    0.753049    0.876712    0.865185    200         1.999260    0.623199    0.000000    0.154142    155.019192 
[37m[36mINFO[0m[0m 02/23 18:53:09 | 0.797348    0.786149    0.907442    0.877037    0.371744    0.992049    0.989399    0.623529    0.632768    0.776466    0.736280    0.907442    0.877037    400         3.998519    0.367335    0.000000    0.149708    141.918786 
[37m[36mINFO[0m[0m 02/23 18:56:03 | 0.770952    0.764722    0.914476    0.862222    0.403361    0.977915    0.982332    0.552000    0.572505    0.782940    0.739329    0.914476    0.862222    600         5.997779    0.326801    0.000000    0.147868    144.926429 
[37m[36mINFO[0m[0m 02/23 18:58:54 | 0.770113    0.780183    0.918178    0.853333    0.488844    0.984982    0.975265    0.623529    0.661017    0.701828    0.704268    0.918178    0.853333    800         7.997038    0.246727    0.000000    0.160462    138.287940 
[37m[36mINFO[0m[0m 02/23 19:01:47 | 0.741489    0.739709    0.897445    0.819259    0.606291    0.974382    0.957597    0.546353    0.572505    0.703732    0.689024    0.897445    0.819259    1000        9.996298    0.227030    0.000000    0.152815    142.798966 
[37m[36mINFO[0m[0m 02/23 19:04:46 | 0.786208    0.781629    0.932618    0.865185    0.478578    0.984099    0.961131    0.619765    0.645951    0.754760    0.737805    0.932618    0.865185    1200        11.995557   0.190805    0.000000    0.166406    145.970025 
[37m[36mINFO[0m[0m 02/23 19:07:39 | 0.795153    0.805930    0.968160    0.866667    0.484297    0.984099    0.978799    0.620706    0.647834    0.780655    0.791159    0.968160    0.866667    1400        13.994817   0.145999    0.000000    0.148925    143.343046 
[37m[36mINFO[0m[0m 02/23 19:10:38 | 0.782293    0.796181    0.965198    0.853333    0.511285    0.970848    0.968198    0.637647    0.677966    0.738385    0.742378    0.965198    0.853333    1600        15.994076   0.130705    0.000000    0.150576    148.349392 
[37m[36mINFO[0m[0m 02/23 19:13:36 | 0.752237    0.763741    0.978156    0.857778    0.554741    0.956714    0.957597    0.560471    0.598870    0.739528    0.734756    0.978156    0.857778    1800        17.993336   0.100399    0.000000    0.154734    147.041551 
[37m[36mINFO[0m[0m 02/23 19:16:30 | 0.781104    0.771424    0.951499    0.841481    0.654814    0.968198    0.939929    0.634824    0.668550    0.740289    0.705793    0.951499    0.841481    2000        19.992595   0.126609    0.000000    0.143325    145.013328 
