[37m[36mINFO[0m[0m 02/06 17:49:11 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 1 --dataset VLCS --trial_seed 0 --hparams_seed 1
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 1
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[1]/250206_17-49-11_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 0
	unique_name: 250206_17-49-11_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 5.0781288859686544e-05
	batch_size: 44
	weight_decay: 0.00046410133598234803
	mmd_gamma: 1.1642706271054615
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/06 17:49:11 | n_steps = 5001
[37m[36mINFO[0m[0m 02/06 17:49:11 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/06 17:49:11 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/06 17:49:11 | 
[37m[36mINFO[0m[0m 02/06 17:49:11 | Testenv name escaping te_L -> te_L
[37m[36mINFO[0m[0m 02/06 17:49:11 | Test envs = [1], name = te_L
[37m[36mINFO[0m[0m 02/06 17:49:11 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 02/06 17:49:11 | Batch sizes for each domain: [44, 0, 44, 44] (total=132)
[37m[36mINFO[0m[0m 02/06 17:49:11 | steps-per-epoch for each domain: 25.73, 59.68, 61.39 -> min = 25.73
[37m[36mINFO[0m[0m 02/06 17:49:13 | # of params = 23518277
[37m[36mINFO[0m[0m 02/06 17:51:51 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/06 17:51:51 | 0.459765    0.489642    0.479821    0.487365    1.367881    0.611307    0.628975    0.459765    0.489642    0.384615    0.387195    0.443539    0.445926    0           0.000000    1.822593    0.041409    1.190207    157.601628 
[37m[36mINFO[0m[0m 02/06 17:55:57 | 0.649882    0.647834    0.906437    0.883468    0.319177    0.996466    0.996466    0.649882    0.647834    0.845392    0.810976    0.877453    0.842963    200         7.773852    0.387592    0.027728    0.409050    163.509868 
[37m[36mINFO[0m[0m 02/06 18:00:04 | 0.629176    0.625235    0.934855    0.891484    0.299020    0.999117    0.996466    0.629176    0.625235    0.886900    0.823171    0.918549    0.854815    400         15.547703   0.221720    0.018872    0.380433    171.541925 
[37m[36mINFO[0m[0m 02/06 18:04:05 | 0.584471    0.578154    0.942850    0.878824    0.326377    1.000000    0.996466    0.584471    0.578154    0.892232    0.789634    0.936320    0.850370    600         23.321555   0.175430    0.016835    0.431045    154.792367 
[37m[36mINFO[0m[0m 02/06 18:08:08 | 0.669176    0.666667    0.959387    0.892998    0.314584    1.000000    1.000000    0.669176    0.666667    0.932216    0.806402    0.945946    0.872593    800         31.095406   0.143580    0.015659    0.393805    163.572328 
[37m[36mINFO[0m[0m 02/06 18:12:13 | 0.611765    0.589454    0.964884    0.885733    0.353425    1.000000    0.992933    0.611765    0.589454    0.939452    0.809451    0.955202    0.854815    1000        38.869258   0.104174    0.014669    0.402833    164.470854 
[37m[36mINFO[0m[0m 02/06 18:16:41 | 0.637176    0.642185    0.975131    0.885434    0.370999    1.000000    1.000000    0.637176    0.642185    0.953161    0.789634    0.972233    0.866667    1200        46.643110   0.074691    0.014121    0.456008    177.092585 
[37m[36mINFO[0m[0m 02/06 18:20:51 | 0.672000    0.674200    0.984926    0.885970    0.397263    1.000000    1.000000    0.672000    0.674200    0.971439    0.794207    0.983340    0.863704    1400        54.416961   0.064446    0.013702    0.426393    165.135267 
[37m[36mINFO[0m[0m 02/06 18:24:57 | 0.624471    0.617702    0.989588    0.889874    0.409722    1.000000    0.996466    0.624471    0.617702    0.981721    0.809451    0.987042    0.863704    1600        62.190813   0.048506    0.012382    0.429390    159.660493 
[37m[36mINFO[0m[0m 02/06 18:29:06 | 0.633412    0.623352    0.994358    0.889409    0.402826    1.000000    0.996466    0.633412    0.623352    0.990480    0.812500    0.992595    0.859259    1800        69.964664   0.039774    0.012214    0.396574    170.085211 
[37m[36mINFO[0m[0m 02/06 18:33:10 | 0.648000    0.651601    0.991844    0.885491    0.478340    1.000000    1.000000    0.648000    0.651601    0.985529    0.795732    0.990004    0.860741    2000        77.738516   0.033079    0.011275    0.421530    159.494472 
[37m[36mINFO[0m[0m 02/06 18:37:16 | 0.648941    0.642185    0.994997    0.890700    0.446948    1.000000    0.992933    0.648941    0.642185    0.992765    0.812500    0.992225    0.866667    2200        85.512367   0.025715    0.010670    0.383023    168.934890 
[37m[36mINFO[0m[0m 02/06 18:41:19 | 0.614118    0.619586    0.993059    0.881393    0.478185    0.999117    0.996466    0.614118    0.619586    0.988576    0.800305    0.991485    0.847407    2400        93.286219   0.019135    0.009956    0.419927    159.578435 
[37m[36mINFO[0m[0m 02/06 18:45:33 | 0.640941    0.640301    0.995532    0.890890    0.442184    0.998233    0.996466    0.640941    0.640301    0.994288    0.812500    0.994076    0.863704    2600        101.060071  0.020295    0.009068    0.426603    168.590120 
[37m[36mINFO[0m[0m 02/06 18:49:36 | 0.609412    0.610169    0.998103    0.885487    0.473378    1.000000    0.996466    0.609412    0.610169    0.995050    0.815549    0.999260    0.844444    2800        108.833922  0.016079    0.008940    0.420692    158.772753 
[37m[36mINFO[0m[0m 02/06 18:53:56 | 0.667294    0.681733    0.996738    0.879465    0.501010    1.000000    1.000000    0.667294    0.681733    0.994288    0.785061    0.995927    0.853333    3000        116.607774  0.012921    0.008586    0.442904    171.143421 
[37m[36mINFO[0m[0m 02/06 18:57:53 | 0.627294    0.623352    0.997743    0.890876    0.473450    1.000000    0.996466    0.627294    0.623352    0.996192    0.810976    0.997038    0.865185    3200        124.381625  0.013773    0.008260    0.394471    157.909433 
[37m[36mINFO[0m[0m 02/06 19:01:55 | 0.640941    0.634652    0.997137    0.892519    0.533406    1.000000    1.000000    0.640941    0.634652    0.997334    0.807927    0.994076    0.869630    3400        132.155477  0.011163    0.007729    0.407080    161.382803 
[37m[36mINFO[0m[0m 02/06 19:06:04 | 0.626824    0.619586    0.998872    0.888507    0.494630    1.000000    0.996466    0.626824    0.619586    0.998096    0.821646    0.998519    0.847407    3600        139.929329  0.007913    0.007412    0.413594    165.356062 
[37m[36mINFO[0m[0m 02/06 19:10:01 | 0.632000    0.629002    0.998124    0.890325    0.529743    1.000000    0.996466    0.632000    0.629002    0.997334    0.804878    0.997038    0.869630    3800        147.703180  0.009583    0.007105    0.404789    156.938176 
[37m[36mINFO[0m[0m 02/06 19:14:11 | 0.625882    0.613936    0.998618    0.888435    0.507221    1.000000    0.996466    0.625882    0.613936    0.997334    0.814024    0.998519    0.854815    4000        155.477032  0.012286    0.006891    0.422320    165.219427 
[37m[36mINFO[0m[0m 02/06 19:18:13 | 0.598118    0.591337    0.995896    0.874959    0.542007    1.000000    0.996466    0.598118    0.591337    0.996573    0.798780    0.991114    0.829630    4200        163.250883  0.007948    0.006797    0.413261    159.572936 
[37m[36mINFO[0m[0m 02/06 19:22:22 | 0.635294    0.634652    0.997119    0.889642    0.539350    1.000000    1.000000    0.635294    0.634652    0.995430    0.817073    0.995927    0.851852    4400        171.024735  0.009543    0.006385    0.384538    171.562529 
[37m[36mINFO[0m[0m 02/06 19:26:22 | 0.662588    0.672316    0.995973    0.884997    0.552446    1.000000    1.000000    0.662588    0.672316    0.991622    0.795732    0.996298    0.859259    4600        178.798587  0.011317    0.006464    0.417495    156.307835 
[37m[36mINFO[0m[0m 02/06 19:30:26 | 0.618824    0.610169    0.998738    0.895553    0.506509    1.000000    1.000000    0.618824    0.610169    0.996954    0.815549    0.999260    0.871111    4800        186.572438  0.011078    0.006406    0.390007    165.984126 
