[37m[36mINFO[0m[0m 02/07 22:51:48 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 1 --dataset VLCS --trial_seed 1 --hparams_seed 19
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 19
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[1]/250207_22-51-48_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 1
	unique_name: 250207_22-51-48_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 3.7692486045276154e-05
	batch_size: 10
	weight_decay: 0.0003178970604685295
	mmd_gamma: 1.483929697730203
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/07 22:51:48 | n_steps = 5001
[37m[36mINFO[0m[0m 02/07 22:51:48 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/07 22:51:48 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/07 22:51:48 | 
[37m[36mINFO[0m[0m 02/07 22:51:48 | Testenv name escaping te_L -> te_L
[37m[36mINFO[0m[0m 02/07 22:51:48 | Test envs = [1], name = te_L
[37m[36mINFO[0m[0m 02/07 22:51:48 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 02/07 22:51:48 | Batch sizes for each domain: [10, 0, 10, 10] (total=30)
[37m[36mINFO[0m[0m 02/07 22:51:48 | steps-per-epoch for each domain: 113.20, 262.60, 270.10 -> min = 113.20
[37m[36mINFO[0m[0m 02/07 22:51:49 | # of params = 23518277
[37m[36mINFO[0m[0m 02/07 22:54:18 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/07 22:54:18 | 0.466824    0.461394    0.489829    0.492234    1.302864    0.613958    0.618375    0.466824    0.461394    0.397182    0.440549    0.458349    0.417778    0           0.000000    1.766147    0.191222    1.189950    147.191254 
[37m[36mINFO[0m[0m 02/07 22:57:21 | 0.628235    0.608286    0.879761    0.883987    0.352554    0.986749    0.992933    0.628235    0.608286    0.819878    0.833841    0.832655    0.825185    200         1.766784    0.564938    0.041076    0.185697    146.139781 
[37m[36mINFO[0m[0m 02/07 23:00:26 | 0.676706    0.672316    0.898022    0.883659    0.340155    0.997350    1.000000    0.676706    0.672316    0.824448    0.810976    0.872270    0.840000    400         3.533569    0.333666    0.032100    0.206541    143.572359 
[37m[36mINFO[0m[0m 02/07 23:03:40 | 0.656941    0.647834    0.882250    0.867009    0.381072    0.993816    0.985866    0.656941    0.647834    0.794364    0.801829    0.858571    0.813333    600         5.300353    0.295790    0.026970    0.199805    154.206997 
[37m[36mINFO[0m[0m 02/07 23:06:44 | 0.687059    0.674200    0.896087    0.886878    0.339454    0.999117    0.992933    0.687059    0.674200    0.818355    0.826220    0.870789    0.841481    800         7.067138    0.323337    0.023000    0.183971    147.430025 
[37m[36mINFO[0m[0m 02/07 23:09:53 | 0.622588    0.612053    0.909311    0.877509    0.329644    1.000000    0.985866    0.622588    0.612053    0.841965    0.815549    0.885968    0.831111    1000        8.833922    0.294305    0.022408    0.198803    149.229431 
[37m[36mINFO[0m[0m 02/07 23:13:05 | 0.592471    0.598870    0.920778    0.873943    0.326040    0.998233    0.996466    0.592471    0.598870    0.864433    0.795732    0.899667    0.829630    1200        10.600707   0.259451    0.021114    0.193371    153.290640 
[37m[36mINFO[0m[0m 02/07 23:16:12 | 0.685176    0.661017    0.931289    0.899628    0.303621    1.000000    0.996466    0.685176    0.661017    0.883092    0.849085    0.910774    0.853333    1400        12.367491   0.238676    0.019972    0.188194    148.805337 
[37m[36mINFO[0m[0m 02/07 23:19:16 | 0.684706    0.676083    0.913838    0.883350    0.356972    1.000000    0.992933    0.684706    0.676083    0.837776    0.818598    0.903739    0.838519    1600        14.134276   0.209779    0.018759    0.184436    147.015841 
[37m[36mINFO[0m[0m 02/07 23:22:28 | 0.616471    0.604520    0.930823    0.876991    0.336229    0.998233    0.996466    0.616471    0.604520    0.883092    0.804878    0.911144    0.829630    1800        15.901060   0.232497    0.018219    0.217232    148.651113 
[37m[36mINFO[0m[0m 02/07 23:25:49 | 0.610353    0.612053    0.929972    0.890554    0.354514    0.999117    0.996466    0.610353    0.612053    0.879284    0.829268    0.911514    0.845926    2000        17.667845   0.207278    0.017840    0.247490    151.472185 
[37m[36mINFO[0m[0m 02/07 23:29:07 | 0.664941    0.670433    0.948184    0.893293    0.304853    0.999117    0.989399    0.664941    0.670433    0.900228    0.846037    0.945205    0.844444    2200        19.434629   0.190092    0.017691    0.242256    149.568369 
[37m[36mINFO[0m[0m 02/07 23:32:20 | 0.584471    0.585687    0.932110    0.863154    0.376615    1.000000    1.000000    0.584471    0.585687    0.878522    0.783537    0.917808    0.805926    2400        21.201413   0.167815    0.017661    0.228592    147.347327 
[37m[36mINFO[0m[0m 02/07 23:35:41 | 0.627765    0.621469    0.951662    0.890668    0.320716    1.000000    0.996466    0.627765    0.621469    0.910891    0.841463    0.944095    0.834074    2600        22.968198   0.178463    0.016188    0.209398    158.838775 
[37m[36mINFO[0m[0m 02/07 23:38:45 | 0.681412    0.677966    0.949460    0.885919    0.366516    0.999117    0.992933    0.681412    0.677966    0.918126    0.829268    0.931137    0.835556    2800        24.734982   0.158231    0.016403    0.182230    147.939950 
[37m[36mINFO[0m[0m 02/07 23:41:56 | 0.576471    0.572505    0.947078    0.874431    0.364320    1.000000    0.985866    0.576471    0.572505    0.895659    0.803354    0.945576    0.834074    3000        26.501767   0.144629    0.015476    0.200622    150.926298 
[37m[36mINFO[0m[0m 02/07 23:45:10 | 0.639059    0.632768    0.953799    0.892758    0.370819    1.000000    0.996466    0.639059    0.632768    0.915080    0.853659    0.946316    0.828148    3200        28.268551   0.162123    0.015049    0.208756    152.477207 
[37m[36mINFO[0m[0m 02/07 23:48:22 | 0.648000    0.632768    0.941787    0.884551    0.395896    0.999117    0.985866    0.648000    0.632768    0.889185    0.829268    0.937060    0.838519    3400        30.035336   0.135116    0.015912    0.202648    150.725632 
[37m[36mINFO[0m[0m 02/07 23:51:37 | 0.681882    0.672316    0.950019    0.870789    0.411014    1.000000    0.992933    0.681882    0.672316    0.906702    0.795732    0.943354    0.823704    3600        31.802120   0.138053    0.015569    0.190055    157.113770 
[37m[36mINFO[0m[0m 02/07 23:54:47 | 0.666353    0.644068    0.970221    0.892772    0.346958    0.999117    0.996466    0.666353    0.644068    0.947829    0.855183    0.963717    0.826667    3800        33.568905   0.135647    0.014880    0.194406    151.408073 
[37m[36mINFO[0m[0m 02/07 23:58:03 | 0.653647    0.642185    0.966732    0.880629    0.385583    1.000000    0.985866    0.653647    0.642185    0.939071    0.832317    0.961126    0.823704    4000        35.335689   0.124987    0.014351    0.228092    150.591974 
[37m[36mINFO[0m[0m 02/08 00:01:20 | 0.670588    0.657250    0.966921    0.880230    0.390719    0.999117    1.000000    0.670588    0.657250    0.937928    0.814024    0.963717    0.826667    4200        37.102473   0.105341    0.014621    0.239007    148.871930 
[37m[36mINFO[0m[0m 02/08 00:04:40 | 0.671059    0.655367    0.967829    0.877870    0.425293    0.998233    0.989399    0.671059    0.655367    0.947829    0.833841    0.957423    0.810370    4400        38.869258   0.112217    0.014032    0.262765    147.584366 
[37m[36mINFO[0m[0m 02/08 00:08:02 | 0.644235    0.638418    0.977871    0.883626    0.396088    1.000000    0.996466    0.644235    0.638418    0.955826    0.827744    0.977786    0.826667    4600        40.636042   0.106858    0.013601    0.254999    150.835462 
[37m[36mINFO[0m[0m 02/08 00:11:19 | 0.639529    0.636535    0.976970    0.881108    0.400665    0.999117    0.985866    0.639529    0.636535    0.956969    0.830793    0.974824    0.826667    4800        42.402827   0.098980    0.013730    0.236137    150.027788 
[37m[36mINFO[0m[0m 02/08 00:14:40 | 0.583529    0.563089    0.958675    0.859953    0.439836    1.000000    0.985866    0.583529    0.563089    0.921935    0.786585    0.954091    0.807407    5000        44.169611   0.111091    0.014011    0.209616    158.674368 
[37m[36mINFO[0m[0m 02/08 00:14:40 | Cumulative gradient change saved at train_output/VLCS/CORAL/[1]/250207_22-51-48_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/08 00:14:41 | ---
[37m[36mINFO[0m[0m 02/08 00:14:41 | test-domain validation(oracle) = 68.141%
[37m[36mINFO[0m[0m 02/08 00:14:41 | training-domain validation(iid) = 68.518%
[37m[36mINFO[0m[0m 02/08 00:14:41 | last = 58.353%
[37m[36mINFO[0m[0m 02/08 00:14:41 | last (inD) = 85.995%
[37m[36mINFO[0m[0m 02/08 00:14:41 | training-domain validation (iid, inD) = 89.963%
[37m[36mINFO[0m[0m 02/08 00:14:41 | === Summary ===
[37m[36mINFO[0m[0m 02/08 00:14:41 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 1 --dataset VLCS --trial_seed 1 --hparams_seed 19
[37m[36mINFO[0m[0m 02/08 00:14:41 | Unique name: 250207_22-51-48_resnet50_GENIE
[37m[36mINFO[0m[0m 02/08 00:14:41 | Out path: train_output/VLCS/CORAL/[1]/250207_22-51-48_resnet50_GENIE
[37m[36mINFO[0m[0m 02/08 00:14:41 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/08 00:14:41 | Dataset: VLCS
