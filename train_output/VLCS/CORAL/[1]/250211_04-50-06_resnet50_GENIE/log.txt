[37m[36mINFO[0m[0m 02/11 04:50:06 | Command :: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 1 --dataset VLCS --trial_seed 1 --hparams_seed 4
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: CORAL
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_GENIE.yaml']
	data_dir: data
	dataset: VLCS
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 4
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_GENIE
	out_dir: train_output/VLCS/CORAL/[1]/250211_04-50-06_resnet50_GENIE
	out_root: train_output/VLCS/CORAL/[1]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [1]
	trial_seed: 1
	unique_name: 250211_04-50-06_resnet50_GENIE
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: genie
	freeze_bn: False
	pretrained: True
	lr: 7.71168362195819e-05
	batch_size: 8
	weight_decay: 0.0007466379205516084
	mmd_gamma: 0.9925162881521883
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[VLCS] #envs=4, #classes=5
	env0: C (#1415)
	env1: L (#2656)
	env2: S (#3282)
	env3: V (#3376)

[37m[36mINFO[0m[0m 02/11 04:50:06 | n_steps = 5001
[37m[36mINFO[0m[0m 02/11 04:50:06 | checkpoint_freq = 200
[37m[36mINFO[0m[0m 02/11 04:50:06 | n_steps is updated to 5001 => 5001 for checkpointing
[37m[36mINFO[0m[0m 02/11 04:50:06 | 
[37m[36mINFO[0m[0m 02/11 04:50:06 | Testenv name escaping te_L -> te_L
[37m[36mINFO[0m[0m 02/11 04:50:06 | Test envs = [1], name = te_L
[37m[36mINFO[0m[0m 02/11 04:50:06 | Train environments: [0, 2, 3], Test environments: [1]
[37m[36mINFO[0m[0m 02/11 04:50:06 | Batch sizes for each domain: [8, 0, 8, 8] (total=24)
[37m[36mINFO[0m[0m 02/11 04:50:06 | steps-per-epoch for each domain: 141.50, 328.25, 337.62 -> min = 141.50
[37m[36mINFO[0m[0m 02/11 04:50:07 | # of params = 23518277
[37m[36mINFO[0m[0m 02/11 04:52:27 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    step        epoch       loss        penalty     step_time   eval_time  
[37m[36mINFO[0m[0m 02/11 04:52:27 | 0.467765    0.463277    0.545718    0.560906    1.328675    0.616608    0.632509    0.467765    0.463277    0.526657    0.573171    0.493891    0.477037    0           0.000000    1.755714    0.224114    1.103073    139.299123 
[37m[36mINFO[0m[0m 02/11 04:55:29 | 0.618824    0.593220    0.880662    0.873098    0.365134    0.994700    0.996466    0.618824    0.593220    0.804265    0.810976    0.843021    0.811852    200         1.413428    0.580089    0.054918    0.217438    137.893723 
[37m[36mINFO[0m[0m 02/11 04:58:20 | 0.619294    0.615819    0.885889    0.872965    0.369522    0.997350    0.992933    0.619294    0.615819    0.820259    0.817073    0.840059    0.808889    400         2.826855    0.356533    0.044663    0.160009    138.872074 
[37m[36mINFO[0m[0m 02/11 05:01:16 | 0.654588    0.647834    0.892864    0.883554    0.341433    0.997350    0.996466    0.654588    0.647834    0.813785    0.820122    0.867456    0.834074    600         4.240283    0.334486    0.036743    0.173306    142.247451 
[37m[36mINFO[0m[0m 02/11 05:04:15 | 0.575059    0.568738    0.856507    0.829363    0.442858    0.995583    0.996466    0.575059    0.568738    0.776085    0.728659    0.797853    0.762963    800         5.653710    0.303040    0.032088    0.161484    145.851906 
[37m[36mINFO[0m[0m 02/11 05:07:12 | 0.560941    0.564972    0.889588    0.856339    0.412143    0.997350    0.985866    0.560941    0.564972    0.806550    0.769817    0.864865    0.813333    1000        7.067138    0.282946    0.031117    0.194701    138.520189 
[37m[36mINFO[0m[0m 02/11 05:10:13 | 0.668706    0.659134    0.895106    0.875221    0.348024    0.995583    1.000000    0.668706    0.659134    0.813024    0.806402    0.876712    0.819259    1200        8.480565    0.276258    0.028563    0.206566    139.736510 
[37m[36mINFO[0m[0m 02/11 05:13:11 | 0.594824    0.589454    0.917539    0.881085    0.323896    0.999117    0.996466    0.594824    0.589454    0.856055    0.820122    0.897445    0.826667    1400        9.893993    0.245139    0.028280    0.174305    142.878394 
[37m[36mINFO[0m[0m 02/11 05:16:06 | 0.568471    0.551789    0.914351    0.867888    0.362679    0.999117    0.996466    0.568471    0.551789    0.845011    0.782012    0.898926    0.825185    1600        11.307420   0.241031    0.027360    0.157376    144.061867 
[37m[36mINFO[0m[0m 02/11 05:19:05 | 0.583529    0.568738    0.910894    0.867798    0.346829    1.000000    0.992933    0.583529    0.568738    0.839680    0.792683    0.893003    0.817778    1800        12.720848   0.259551    0.025040    0.182454    142.012167 
[37m[36mINFO[0m[0m 02/11 05:22:03 | 0.682824    0.661017    0.922567    0.889722    0.324208    0.999117    0.989399    0.682824    0.661017    0.852628    0.833841    0.915957    0.845926    2000        14.134276   0.239367    0.025053    0.193176    139.316564 
[37m[36mINFO[0m[0m 02/11 05:25:02 | 0.693176    0.670433    0.934441    0.899946    0.306676    0.998233    0.992933    0.693176    0.670433    0.873953    0.850610    0.931137    0.856296    2200        15.547703   0.212241    0.023804    0.168010    145.649923 
[37m[36mINFO[0m[0m 02/11 05:27:55 | 0.586353    0.574388    0.929288    0.871786    0.360652    0.999117    0.982332    0.586353    0.574388    0.872049    0.804878    0.916698    0.828148    2400        16.961131   0.191748    0.022974    0.165933    140.023679 
[37m[36mINFO[0m[0m 02/11 05:30:51 | 0.608471    0.604520    0.934882    0.875286    0.355400    0.998233    0.982332    0.608471    0.604520    0.881569    0.809451    0.924843    0.834074    2600        18.374558   0.210355    0.022296    0.168808    142.226854 
[37m[36mINFO[0m[0m 02/11 05:33:51 | 0.646118    0.636535    0.923965    0.861440    0.363478    1.000000    0.996466    0.646118    0.636535    0.865194    0.778963    0.906701    0.808889    2800        19.787986   0.191657    0.022669    0.175870    144.864303 
[37m[36mINFO[0m[0m 02/11 05:36:47 | 0.667765    0.649718    0.949416    0.888359    0.319939    1.000000    0.992933    0.667765    0.649718    0.908225    0.826220    0.940022    0.845926    3000        21.201413   0.179615    0.021865    0.201688    135.322602 
[37m[36mINFO[0m[0m 02/11 05:39:49 | 0.626353    0.612053    0.942875    0.876018    0.340301    1.000000    0.996466    0.626353    0.612053    0.894897    0.806402    0.933728    0.825185    3200        22.614841   0.179763    0.021374    0.196393    142.898090 
[37m[36mINFO[0m[0m 02/11 05:42:55 | 0.674353    0.657250    0.939303    0.879266    0.384069    0.998233    0.989399    0.674353    0.657250    0.892612    0.824695    0.927064    0.823704    3400        24.028269   0.177460    0.020938    0.186996    148.657141 
[37m[36mINFO[0m[0m 02/11 05:45:52 | 0.638588    0.629002    0.944716    0.872404    0.377041    0.998233    0.996466    0.638588    0.629002    0.890708    0.789634    0.945205    0.831111    3600        25.441696   0.147536    0.021624    0.179367    140.495312 
[37m[36mINFO[0m[0m 02/11 05:48:46 | 0.638588    0.629002    0.943489    0.885958    0.412039    0.998233    0.989399    0.638588    0.629002    0.904798    0.853659    0.927434    0.814815    3800        26.855124   0.189913    0.019245    0.196106    135.208501 
[37m[36mINFO[0m[0m 02/11 05:51:43 | 0.655529    0.655367    0.958883    0.877594    0.350217    1.000000    0.985866    0.655529    0.655367    0.931074    0.824695    0.945576    0.822222    4000        28.268551   0.140642    0.021140    0.182999    140.779427 
[37m[36mINFO[0m[0m 02/11 05:54:36 | 0.648000    0.629002    0.908027    0.842448    0.471609    0.946113    0.943463    0.648000    0.629002    0.897182    0.795732    0.880785    0.788148    4200        29.681979   0.150195    0.019013    0.185361    135.039105 
[37m[36mINFO[0m[0m 02/11 05:57:46 | 0.649412    0.649718    0.957441    0.891232    0.374208    0.998233    0.989399    0.649412    0.649718    0.918888    0.836890    0.955202    0.847407    4400        31.095406   0.144938    0.018993    0.252220    140.246686 
[37m[36mINFO[0m[0m 02/11 06:00:45 | 0.663529    0.651601    0.940341    0.874769    0.458937    0.992049    0.975265    0.663529    0.651601    0.920792    0.847561    0.908182    0.801481    4600        32.508834   0.120281    0.020161    0.183908    141.717841 
[37m[36mINFO[0m[0m 02/11 06:03:45 | 0.690353    0.681733    0.913801    0.856496    0.554040    0.995583    0.985866    0.690353    0.681733    0.846154    0.786585    0.899667    0.797037    4800        33.922261   0.134139    0.019261    0.172074    145.354998 
[37m[36mINFO[0m[0m 02/11 06:06:42 | 0.669176    0.645951    0.953887    0.876003    0.406217    0.994700    0.971731    0.669176    0.645951    0.928789    0.841463    0.938171    0.814815    5000        35.335689   0.107268    0.019221    0.180180    141.303769 
[37m[36mINFO[0m[0m 02/11 06:06:42 | Cumulative gradient change saved at train_output/VLCS/CORAL/[1]/250211_04-50-06_resnet50_GENIE/sum_cumulative_g_change.npy
[37m[36mINFO[0m[0m 02/11 06:06:43 | ---
[37m[36mINFO[0m[0m 02/11 06:06:43 | test-domain validation(oracle) = 69.035%
[37m[36mINFO[0m[0m 02/11 06:06:43 | training-domain validation(iid) = 69.318%
[37m[36mINFO[0m[0m 02/11 06:06:43 | last = 66.918%
[37m[36mINFO[0m[0m 02/11 06:06:43 | last (inD) = 87.600%
[37m[36mINFO[0m[0m 02/11 06:06:43 | training-domain validation (iid, inD) = 89.995%
[37m[36mINFO[0m[0m 02/11 06:06:43 | === Summary ===
[37m[36mINFO[0m[0m 02/11 06:06:43 | Command: /jsm0707/GENIE/train_all.py resnet50_GENIE config/resnet50_GENIE.yaml --algorithm CORAL --test_envs 1 --dataset VLCS --trial_seed 1 --hparams_seed 4
[37m[36mINFO[0m[0m 02/11 06:06:43 | Unique name: 250211_04-50-06_resnet50_GENIE
[37m[36mINFO[0m[0m 02/11 06:06:43 | Out path: train_output/VLCS/CORAL/[1]/250211_04-50-06_resnet50_GENIE
[37m[36mINFO[0m[0m 02/11 06:06:43 | Algorithm: CORAL
[37m[36mINFO[0m[0m 02/11 06:06:43 | Dataset: VLCS
