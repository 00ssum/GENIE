[37m[36mINFO[0m[0m 05/02 05:21:27 | Command :: /jsm0707/GENIE/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm GENIE --test_envs 2 --dataset DomainNet --trial_seed 2 --hparams_seed 4
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: DomainNet
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 4
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/DomainNet/GENIE/[2]/250502_05-21-27_resnet50_sgd
	out_root: train_output/DomainNet/GENIE/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 2
	unique_name: 250502_05-21-27_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 6.164076981615215e-05
	batch_size: 10
	weight_decay: 3.184710792983041e-06
	momentum: 0.9903641916058693
	convergence_rate: 0.02352745664367345
	moving_avg: 0.9093139163665226
	p: 0.17159261258202024
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[DomainNet] #envs=6, #classes=345
	env0: clip (#48129)
	env1: info (#51605)
	env2: paint (#72266)
	env3: quick (#172500)
	env4: real (#172947)
	env5: sketch (#69128)

[37m[36mINFO[0m[0m 05/02 05:21:27 | n_steps = 15001
[37m[36mINFO[0m[0m 05/02 05:21:27 | checkpoint_freq = 1000
[37m[36mINFO[0m[0m 05/02 05:21:27 | n_steps is updated to 15001 => 15001 for checkpointing
[37m[36mINFO[0m[0m 05/02 05:21:27 | 
[37m[36mINFO[0m[0m 05/02 05:21:28 | Testenv name escaping te_paint -> te_paint
[37m[36mINFO[0m[0m 05/02 05:21:28 | Test envs = [2], name = te_paint
[37m[36mINFO[0m[0m 05/02 05:21:28 | Train environments: [0, 1, 3, 4, 5], Test environments: [2]
[37m[36mINFO[0m[0m 05/02 05:21:28 | Batch sizes for each domain: [10, 10, 0, 10, 10, 10] (total=50)
[37m[36mINFO[0m[0m 05/02 05:21:28 | steps-per-epoch for each domain: 3850.40, 4128.40, 13800.00, 13835.80, 5530.30 -> min = 3850.40
[37m[36mINFO[0m[0m 05/02 05:21:30 | # of params = 24214937
[37m[36mINFO[0m[0m 05/02 06:03:54 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    env4_in     env4_out    env5_in     env5_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 05/02 06:03:54 | 0.003494    0.003390    0.003029    0.003105    5.901864    0.003065    0.003117    0.002519    0.003391    0.003494    0.003390    0.004826    0.004406    0.002566    0.002515    0.002170    0.002098    0           0.000000    5.860681    1.200712    2543.40314 
[37m[36mINFO[0m[0m 05/02 06:49:04 | 0.293083    0.296409    0.338355    0.336472    3.175082    0.445538    0.438857    0.187288    0.165875    0.293083    0.296409    0.215971    0.232638    0.461751    0.475729    0.381227    0.369259    1000        0.259713    4.218935    0.183994    2525.41281 
[37m[36mINFO[0m[0m 05/02 07:35:12 | 0.358881    0.364561    0.424703    0.421787    2.724047    0.532594    0.535273    0.241401    0.211511    0.358881    0.364561    0.325080    0.343913    0.548823    0.553355    0.475616    0.464882    2000        0.519427    2.899552    0.209972    2558.10060 
[37m[36mINFO[0m[0m 05/02 08:20:41 | 0.394911    0.404138    0.468336    0.458885    2.517837    0.596977    0.585247    0.265333    0.232148    0.394911    0.404138    0.377268    0.389449    0.595585    0.602012    0.506519    0.485570    3000        0.779140    2.578445    0.209902    2519.20921 
[37m[36mINFO[0m[0m 05/02 09:05:43 | 0.357861    0.357919    0.475893    0.462059    2.494702    0.613495    0.597091    0.274804    0.227304    0.357861    0.357919    0.397261    0.415014    0.579938    0.583365    0.513969    0.487523    4000        1.038853    2.416833    0.173200    2528.72157 
[37m[36mINFO[0m[0m 05/02 09:50:40 | 0.399893    0.405245    0.498098    0.492129    2.378708    0.619364    0.614753    0.295030    0.251042    0.399893    0.405245    0.439333    0.455014    0.603066    0.618896    0.533696    0.520940    5000        1.298566    2.279887    0.177289    2520.32960 
[37m[36mINFO[0m[0m 05/02 10:35:46 | 0.406881    0.406767    0.526119    0.508411    2.256159    0.666398    0.641974    0.311767    0.258018    0.406881    0.406767    0.471210    0.486551    0.627199    0.628784    0.554021    0.526727    6000        1.558280    2.197696    0.172731    2533.39476 
[37m[36mINFO[0m[0m 05/02 11:21:10 | 0.398492    0.397772    0.529632    0.516785    2.216781    0.659931    0.641558    0.322667    0.277008    0.398492    0.397772    0.473036    0.489246    0.633747    0.643384    0.558776    0.532731    7000        1.817993    2.104154    0.172125    2551.87090 
[37m[36mINFO[0m[0m 05/02 12:06:04 | 0.416706    0.420743    0.548548    0.528941    2.179350    0.681799    0.658182    0.329813    0.267416    0.416706    0.420743    0.494471    0.503072    0.647718    0.654659    0.588937    0.561374    8000        2.077706    2.060507    0.190718    2502.56199 
[37m[36mINFO[0m[0m 05/02 12:53:12 | 0.426721    0.426970    0.551363    0.527751    2.174037    0.693720    0.669403    0.329983    0.264219    0.426721    0.426970    0.503116    0.510812    0.647913    0.647634    0.582084    0.546691    9000        2.337419    1.990829    0.302078    2525.70402 
[37m[36mINFO[0m[0m 05/02 13:42:14 | 0.427551    0.431606    0.568927    0.541624    2.100712    0.715380    0.678442    0.352558    0.281465    0.427551    0.431606    0.510167    0.515072    0.665968    0.665616    0.600564    0.567523    10000       2.597133    1.937292    0.356240    2585.79035 
[37m[36mINFO[0m[0m 05/02 14:30:28 | 0.405497    0.409534    0.564517    0.531385    2.157113    0.699979    0.652468    0.353527    0.279237    0.405497    0.409534    0.516181    0.522174    0.658791    0.647894    0.594109    0.555154    11000       2.856846    1.907325    0.350616    2543.49807 
