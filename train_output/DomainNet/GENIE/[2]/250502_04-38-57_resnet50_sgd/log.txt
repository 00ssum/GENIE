[37m[36mINFO[0m[0m 05/02 04:38:57 | Command :: /jsm0707/GENIE/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm GENIE --test_envs 2 --dataset DomainNet --trial_seed 1 --hparams_seed 4
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: DomainNet
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 4
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/DomainNet/GENIE/[2]/250502_04-38-57_resnet50_sgd
	out_root: train_output/DomainNet/GENIE/[2]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [2]
	trial_seed: 1
	unique_name: 250502_04-38-57_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.5
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 7.71168362195819e-05
	batch_size: 8
	weight_decay: 0.0007466379205516084
	momentum: 0.8991753954472675
	convergence_rate: 0.004264708055982978
	moving_avg: 0.9823101033953
	p: 0.7035462942034698
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[DomainNet] #envs=6, #classes=345
	env0: clip (#48129)
	env1: info (#51605)
	env2: paint (#72266)
	env3: quick (#172500)
	env4: real (#172947)
	env5: sketch (#69128)

[37m[36mINFO[0m[0m 05/02 04:38:58 | n_steps = 15001
[37m[36mINFO[0m[0m 05/02 04:38:58 | checkpoint_freq = 1000
[37m[36mINFO[0m[0m 05/02 04:38:58 | n_steps is updated to 15001 => 15001 for checkpointing
[37m[36mINFO[0m[0m 05/02 04:38:58 | 
[37m[36mINFO[0m[0m 05/02 04:38:59 | Testenv name escaping te_paint -> te_paint
[37m[36mINFO[0m[0m 05/02 04:38:59 | Test envs = [2], name = te_paint
[37m[36mINFO[0m[0m 05/02 04:38:59 | Train environments: [0, 1, 3, 4, 5], Test environments: [2]
[37m[36mINFO[0m[0m 05/02 04:38:59 | Batch sizes for each domain: [8, 8, 0, 8, 8, 8] (total=40)
[37m[36mINFO[0m[0m 05/02 04:38:59 | steps-per-epoch for each domain: 4813.00, 5160.50, 17250.00, 17294.75, 6912.88 -> min = 4813.00
[37m[36mINFO[0m[0m 05/02 04:39:01 | # of params = 24214937
[37m[36mINFO[0m[0m 05/02 05:21:06 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    env4_in     env4_out    env5_in     env5_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 05/02 05:21:06 | 0.003235    0.002975    0.003000    0.002779    5.897850    0.003922    0.004052    0.001768    0.002228    0.003235    0.002975    0.004239    0.003710    0.001886    0.002168    0.003182    0.001736    0           0.000000    6.172245    1.347077    2523.61280 
[37m[36mINFO[0m[0m 05/02 06:06:09 | 0.023835    0.024078    0.036317    0.036829    5.629377    0.046722    0.054130    0.031610    0.026935    0.023835    0.024078    0.010514    0.014609    0.044876    0.042759    0.047864    0.045714    1000        0.207771    5.797688    0.156910    2546.48977 
[37m[36mINFO[0m[0m 05/02 06:51:31 | 0.282635    0.269979    0.279106    0.281057    3.598421    0.373182    0.382961    0.167377    0.149210    0.282635    0.269979    0.116377    0.129710    0.409264    0.420654    0.329331    0.322749    2000        0.415541    4.813205    0.165784    2556.07032 
[37m[36mINFO[0m[0m 05/02 07:36:54 | 0.359071    0.349132    0.373560    0.376772    3.016846    0.481197    0.490701    0.216500    0.192229    0.359071    0.349132    0.216130    0.231101    0.527140    0.542774    0.426830    0.427052    3000        0.623312    3.593866    0.162994    2560.25452 
[37m[36mINFO[0m[0m 05/02 08:21:27 | 0.393372    0.389815    0.420948    0.419121    2.725675    0.537658    0.541403    0.240481    0.209670    0.393372    0.389815    0.279688    0.296203    0.578478    0.585764    0.468438    0.462568    4000        0.831082    3.126634    0.156072    2516.40889 
[37m[36mINFO[0m[0m 05/02 09:06:52 | 0.412987    0.411956    0.457539    0.453754    2.570097    0.580901    0.581299    0.262789    0.231567    0.412987    0.411956    0.330710    0.342754    0.605870    0.614270    0.507423    0.498879    5000        1.038853    2.880323    0.165393    2559.84248 
[37m[36mINFO[0m[0m 05/02 09:51:30 | 0.429661    0.427524    0.472655    0.470038    2.457567    0.594691    0.603532    0.276209    0.240771    0.429661    0.427524    0.359377    0.368116    0.619357    0.632745    0.513643    0.505027    6000        1.246624    2.706086    0.167892    2509.91857 
[37m[36mINFO[0m[0m 05/02 10:36:42 | 0.442167    0.441915    0.494782    0.490982    2.364953    0.615079    0.616416    0.294739    0.262087    0.442167    0.441915    0.387659    0.404812    0.639392    0.647113    0.537041    0.524485    7000        1.454394    2.577971    0.166635    2545.97961 
[37m[36mINFO[0m[0m 05/02 11:20:58 | 0.441579    0.440462    0.511447    0.504372    2.276434    0.634298    0.633143    0.305784    0.263153    0.441579    0.440462    0.414870    0.425884    0.651824    0.659863    0.550458    0.539819    8000        1.662165    2.488679    0.165494    2490.17301 
[37m[36mINFO[0m[0m 05/02 12:06:33 | 0.449639    0.444406    0.522626    0.515732    2.240144    0.649231    0.644987    0.312227    0.267416    0.449639    0.444406    0.430000    0.445420    0.660453    0.669808    0.561217    0.551031    9000        1.869936    2.422004    0.166301    2568.49701 
[37m[36mINFO[0m[0m 05/02 12:50:47 | 0.454258    0.451948    0.534830    0.522689    2.200993    0.665541    0.652571    0.320342    0.272745    0.454258    0.451948    0.447899    0.461072    0.666691    0.673856    0.573676    0.553201    10000       2.077706    2.347747    0.168023    2486.48498 
[37m[36mINFO[0m[0m 05/02 13:35:46 | 0.463875    0.461842    0.541049    0.529614    2.155620    0.671281    0.664208    0.330201    0.280787    0.463875    0.461842    0.440920    0.452174    0.678609    0.684611    0.584236    0.566293    11000       2.285477    2.315997    0.161904    2536.78770 
[37m[36mINFO[0m[0m 05/02 14:20:55 | 0.453549    0.451048    0.545119    0.529008    2.156314    0.678527    0.663896    0.328311    0.280205    0.453549    0.451048    0.459036    0.466203    0.672719    0.676111    0.587003    0.558626    12000       2.493247    2.263486    0.163671    2544.98533 
[37m[36mINFO[0m[0m 05/02 15:05:14 | 0.458668    0.455546    0.557630    0.541786    2.077894    0.682215    0.666078    0.343935    0.294545    0.458668    0.455546    0.483196    0.489681    0.685562    0.689150    0.593241    0.569476    13000       2.701018    2.201744    0.167413    2491.56220 
