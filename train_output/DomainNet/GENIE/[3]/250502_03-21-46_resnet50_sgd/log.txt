[37m[36mINFO[0m[0m 05/02 03:21:46 | Command :: /jsm0707/GENIE/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm GENIE --test_envs 3 --dataset DomainNet --trial_seed 1 --hparams_seed 15
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: DomainNet
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 15
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/DomainNet/GENIE/[3]/250502_03-21-46_resnet50_sgd
	out_root: train_output/DomainNet/GENIE/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 1
	unique_name: 250502_03-21-46_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 1.7812453400894684e-05
	batch_size: 27
	weight_decay: 3.1651536009272826e-06
	momentum: 0.9528943649421413
	convergence_rate: 0.0025748176710103194
	moving_avg: 0.919405819570856
	p: 0.31443299474225483
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[DomainNet] #envs=6, #classes=345
	env0: clip (#48129)
	env1: info (#51605)
	env2: paint (#72266)
	env3: quick (#172500)
	env4: real (#172947)
	env5: sketch (#69128)

[37m[36mINFO[0m[0m 05/02 03:21:47 | n_steps = 15001
[37m[36mINFO[0m[0m 05/02 03:21:47 | checkpoint_freq = 1000
[37m[36mINFO[0m[0m 05/02 03:21:47 | n_steps is updated to 15001 => 15001 for checkpointing
[37m[36mINFO[0m[0m 05/02 03:21:47 | 
[37m[36mINFO[0m[0m 05/02 03:21:48 | Testenv name escaping te_quick -> te_quick
[37m[36mINFO[0m[0m 05/02 03:21:48 | Test envs = [3], name = te_quick
[37m[36mINFO[0m[0m 05/02 03:21:48 | Train environments: [0, 1, 2, 4, 5], Test environments: [3]
[37m[36mINFO[0m[0m 05/02 03:21:48 | Batch sizes for each domain: [27, 27, 27, 0, 27, 27] (total=135)
[37m[36mINFO[0m[0m 05/02 03:21:48 | steps-per-epoch for each domain: 1426.07, 1529.04, 2141.22, 5124.37, 2048.26 -> min = 1426.07
[37m[36mINFO[0m[0m 05/02 03:21:49 | # of params = 24214937
[37m[36mINFO[0m[0m 05/02 04:03:44 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    env4_in     env4_out    env5_in     env5_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 05/02 04:03:44 | 0.003710    0.003797    0.002897    0.002347    5.913201    0.003636    0.004052    0.002083    0.001744    0.004048    0.002352    0.003710    0.003797    0.001843    0.002139    0.002875    0.001447    0           0.000000    5.884352    1.519492    2512.72982 
[37m[36mINFO[0m[0m 05/02 04:52:05 | 0.049000    0.046812    0.370425    0.369503    3.076878    0.399439    0.405818    0.184163    0.166457    0.398526    0.403515    0.049000    0.046812    0.498497    0.509700    0.371499    0.362025    1000        0.701226    4.805358    0.365132    2536.24329 
[37m[36mINFO[0m[0m 05/02 05:40:19 | 0.073949    0.070609    0.477167    0.472670    2.522265    0.541710    0.543481    0.246657    0.220231    0.501254    0.503287    0.073949    0.070609    0.606420    0.617075    0.489793    0.479277    2000        1.402452    2.679014    0.362985    2531.06104 
[37m[36mINFO[0m[0m 05/02 06:28:05 | 0.086884    0.085855    0.522475    0.512643    2.315348    0.603496    0.600000    0.285777    0.249201    0.539982    0.536844    0.086884    0.085855    0.653710    0.662089    0.529411    0.515081    3000        2.103678    2.318963    0.337227    2528.91562 
[37m[36mINFO[0m[0m 05/02 07:15:57 | 0.096391    0.095884    0.549646    0.537641    2.185195    0.638609    0.633558    0.309442    0.271485    0.567381    0.562721    0.096391    0.095884    0.676499    0.680332    0.556299    0.540108    4000        2.804903    2.146525    0.335853    2536.00741 
[37m[36mINFO[0m[0m 05/02 08:04:17 | 0.100536    0.099159    0.568993    0.549661    2.122610    0.662788    0.646545    0.323224    0.279333    0.585508    0.569086    0.100536    0.099159    0.690564    0.692764    0.582880    0.560579    5000        3.506129    2.044936    0.335012    2564.35123 
[37m[36mINFO[0m[0m 05/02 08:53:05 | 0.114855    0.115101    0.586064    0.564529    2.048146    0.682345    0.664727    0.341343    0.290960    0.604241    0.587906    0.114855    0.115101    0.705062    0.707046    0.597327    0.572007    6000        4.207355    1.940862    0.333737    2594.81695 
[37m[36mINFO[0m[0m 05/02 09:40:16 | 0.116623    0.115739    0.595185    0.568977    2.038432    0.696395    0.672208    0.349554    0.296095    0.611558    0.590673    0.116623    0.115739    0.712644    0.710428    0.605772    0.575479    7000        4.908581    1.877648    0.336812    2493.78220 
[37m[36mINFO[0m[0m 05/02 10:27:35 | 0.117283    0.116899    0.604451    0.577651    1.994178    0.706290    0.682909    0.361230    0.305300    0.620795    0.600083    0.117283    0.116899    0.717646    0.714938    0.616296    0.585027    8000        5.609807    1.831383    0.343189    2496.14255 
[37m[36mINFO[0m[0m 05/02 11:15:20 | 0.123239    0.121275    0.616317    0.583152    1.960781    0.727275    0.692779    0.369344    0.306947    0.630222    0.603127    0.123239    0.121275    0.724635    0.722600    0.630110    0.590307    9000        6.311033    1.773563    0.357586    2507.85534 
[37m[36mINFO[0m[0m 05/02 12:03:04 | 0.121065    0.119739    0.622231    0.585960    1.952675    0.730963    0.695584    0.379735    0.312179    0.636293    0.604373    0.121065    0.119739    0.728791    0.724103    0.635372    0.593562    10000       7.012258    1.724233    0.371264    2492.55488 
[37m[36mINFO[0m[0m 05/02 12:51:43 | 0.131138    0.133333    0.628261    0.590052    1.932561    0.742390    0.700156    0.385985    0.313729    0.639631    0.608109    0.131138    0.133333    0.731942    0.727254    0.641358    0.601013    11000       7.713484    1.696001    0.399843    2519.31358 
[37m[36mINFO[0m[0m 05/02 13:40:29 | 0.128319    0.128522    0.631648    0.590771    1.920354    0.743767    0.701091    0.387390    0.313439    0.645236    0.609424    0.128319    0.128522    0.735787    0.726792    0.646059    0.603110    12000       8.414710    1.652313    0.403632    2521.97647 
[37m[36mINFO[0m[0m 05/02 14:29:42 | 0.126819    0.127536    0.642534    0.597889    1.905108    0.754181    0.707532    0.402069    0.317411    0.657378    0.618488    0.126819    0.127536    0.739350    0.731909    0.659693    0.614105    13000       9.115936    1.614207    0.403211    2549.96284 
[37m[36mINFO[0m[0m 05/02 15:18:08 | 0.123848    0.124377    0.646956    0.599692    1.884679    0.763375    0.712727    0.403086    0.318961    0.662118    0.619179    0.123848    0.124377    0.742675    0.732979    0.663526    0.614611    14000       9.817162    1.593615    0.411268    2494.91719 
