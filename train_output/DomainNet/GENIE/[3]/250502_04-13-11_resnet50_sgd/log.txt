[37m[36mINFO[0m[0m 05/02 04:13:11 | Command :: /jsm0707/GENIE/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm GENIE --test_envs 3 --dataset DomainNet --trial_seed 2 --hparams_seed 15
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: DomainNet
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 15
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/DomainNet/GENIE/[3]/250502_04-13-11_resnet50_sgd
	out_root: train_output/DomainNet/GENIE/[3]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [3]
	trial_seed: 2
	unique_name: 250502_04-13-11_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.0
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 0.00023237243746776078
	batch_size: 11
	weight_decay: 1.8132449266758068e-05
	momentum: 0.8962492415686187
	convergence_rate: 0.017316299354338788
	moving_avg: 0.9340158556051206
	p: 0.5759346954744597
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[DomainNet] #envs=6, #classes=345
	env0: clip (#48129)
	env1: info (#51605)
	env2: paint (#72266)
	env3: quick (#172500)
	env4: real (#172947)
	env5: sketch (#69128)

[37m[36mINFO[0m[0m 05/02 04:13:12 | n_steps = 15001
[37m[36mINFO[0m[0m 05/02 04:13:12 | checkpoint_freq = 1000
[37m[36mINFO[0m[0m 05/02 04:13:12 | n_steps is updated to 15001 => 15001 for checkpointing
[37m[36mINFO[0m[0m 05/02 04:13:12 | 
[37m[36mINFO[0m[0m 05/02 04:13:13 | Testenv name escaping te_quick -> te_quick
[37m[36mINFO[0m[0m 05/02 04:13:13 | Test envs = [3], name = te_quick
[37m[36mINFO[0m[0m 05/02 04:13:13 | Train environments: [0, 1, 2, 4, 5], Test environments: [3]
[37m[36mINFO[0m[0m 05/02 04:13:13 | Batch sizes for each domain: [11, 11, 11, 0, 11, 11] (total=55)
[37m[36mINFO[0m[0m 05/02 04:13:13 | steps-per-epoch for each domain: 3500.36, 3753.09, 5255.73, 12578.00, 5027.55 -> min = 3500.36
[37m[36mINFO[0m[0m 05/02 04:13:14 | # of params = 24214937
[37m[36mINFO[0m[0m 05/02 04:55:10 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    env4_in     env4_out    env5_in     env5_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 05/02 04:55:10 | 0.003130    0.003304    0.002865    0.002411    5.907903    0.002493    0.003429    0.001962    0.001453    0.005345    0.003805    0.003130    0.003304    0.002045    0.001850    0.002477    0.001519    0           0.000000    5.881355    1.421837    2514.76147 
[37m[36mINFO[0m[0m 05/02 05:40:14 | 0.073659    0.073101    0.412438    0.414294    2.837989    0.482028    0.488416    0.212601    0.193683    0.408887    0.422196    0.073659    0.073101    0.533327    0.548874    0.425348    0.418300    1000        0.285685    3.828557    0.194903    2508.50674 
[37m[36mINFO[0m[0m 05/02 06:25:48 | 0.098268    0.098754    0.486278    0.481157    2.513376    0.570772    0.571117    0.261336    0.226335    0.493557    0.502041    0.098268    0.098754    0.599098    0.614213    0.506627    0.492080    2000        0.571369    2.604867    0.195828    2538.34456 
[37m[36mINFO[0m[0m 05/02 07:10:55 | 0.103848    0.099681    0.507349    0.496794    2.412589    0.607989    0.599377    0.282676    0.243484    0.508899    0.511174    0.103848    0.099681    0.616452    0.629825    0.520731    0.500108    3000        0.857054    2.345944    0.189142    2518.51468 
[37m[36mINFO[0m[0m 05/02 07:55:40 | 0.118812    0.116667    0.538176    0.523122    2.297544    0.641803    0.630130    0.302878    0.260731    0.545829    0.542310    0.118812    0.116667    0.647364    0.654196    0.553008    0.528246    4000        1.142738    2.198875    0.193426    2491.05130 
[37m[36mINFO[0m[0m 05/02 08:40:22 | 0.101232    0.101159    0.555139    0.535455    2.214476    0.659490    0.637299    0.317484    0.272648    0.570218    0.558708    0.101232    0.101159    0.656312    0.658967    0.572193    0.549656    5000        1.428423    2.109779    0.195725    2486.45020 
[37m[36mINFO[0m[0m 05/02 09:25:11 | 0.116957    0.115710    0.562259    0.539033    2.193673    0.677878    0.654857    0.324993    0.270323    0.564786    0.550751    0.116957    0.115710    0.666004    0.665963    0.577636    0.553273    6000        1.714108    2.029956    0.195218    2493.64057 
[37m[36mINFO[0m[0m 05/02 10:09:28 | 0.123768    0.121101    0.573656    0.544773    2.168627    0.688292    0.660468    0.343063    0.276330    0.579991    0.561337    0.123768    0.121101    0.667399    0.667177    0.589534    0.558553    7000        1.999792    1.946194    0.203386    2453.10652 
[37m[36mINFO[0m[0m 05/02 10:53:23 | 0.132630    0.131362    0.593371    0.557491    2.103972    0.704239    0.670649    0.360939    0.286697    0.603273    0.577596    0.132630    0.131362    0.686827    0.678915    0.611576    0.573599    8000        2.285477    1.879990    0.199308    2435.83317 
[37m[36mINFO[0m[0m 05/02 11:37:58 | 0.124326    0.123159    0.591982    0.561819    2.082297    0.707875    0.673143    0.363119    0.293382    0.597201    0.580156    0.124326    0.123159    0.685490    0.690046    0.606224    0.572369    9000        2.571161    1.866450    0.202421    2473.07520 
[37m[36mINFO[0m[0m 05/02 12:22:00 | 0.106819    0.105913    0.601663    0.564986    2.071110    0.716964    0.677818    0.373171    0.296386    0.602615    0.576005    0.106819    0.105913    0.697170    0.694758    0.618393    0.579964    10000       2.856846    1.816979    0.198362    2443.38306 
[37m[36mINFO[0m[0m 05/02 13:07:00 | 0.122638    0.121246    0.613399    0.571760    2.019157    0.732703    0.683532    0.388892    0.309079    0.617595    0.587560    0.122638    0.121246    0.697914    0.691145    0.629893    0.587486    11000       3.142531    1.765226    0.202119    2498.39763 
[37m[36mINFO[0m[0m 05/02 13:51:29 | 0.121848    0.119913    0.618731    0.574673    2.013218    0.739482    0.689558    0.390030    0.302490    0.627783    0.592334    0.121848    0.119913    0.700805    0.696724    0.635553    0.592260    12000       3.428215    1.739884    0.203175    2465.43701 
[37m[36mINFO[0m[0m 05/02 14:37:13 | 0.121696    0.119333    0.625002    0.582611    2.007726    0.747896    0.696104    0.403304    0.315376    0.627990    0.602366    0.121696    0.119333    0.704101    0.699644    0.641719    0.599566    13000       3.713900    1.700573    0.208000    2536.02841 
