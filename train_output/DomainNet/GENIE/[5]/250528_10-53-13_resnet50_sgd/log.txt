[37m[36mINFO[0m[0m 05/28 10:53:13 | Command :: /jsm0707/GENIE/train_all.py resnet50_sgd config/resnet50_sgd.yaml --algorithm GENIE --test_envs 5 --dataset DomainNet --trial_seed 0 --hparams_seed 18
Environment:
	Python: 3.8.10
	PyTorch: 1.13.1+cu117
	Torchvision: 0.14.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.4
	PIL: 10.4.0
Args:
	algorithm: GENIE
	attn_tune: False
	auto_lr: False
	checkpoint_freq: None
	configs: ['config/resnet50_sgd.yaml']
	data_dir: data
	dataset: DomainNet
	debug: False
	deterministic: True
	dump_scores: False
	dump_similarities: False
	evalmode: all
	evaluate: False
	full_data: False
	holdout_fraction: 0.2
	hparams_seed: 18
	in_domain: False
	model_save: None
	mpa: False
	name: resnet50_sgd
	out_dir: train_output/DomainNet/GENIE/[5]/250528_10-53-13_resnet50_sgd
	out_root: train_output/DomainNet/GENIE/[5]
	prebuild_loader: False
	resume_path: checkpoints/
	seed: 0
	show: False
	small_bs: False
	steps: None
	tb_freq: 10
	test_envs: [5]
	trial_seed: 0
	unique_name: 250528_10-53-13_resnet50_sgd
	warmup: False
	work_dir: .
HParams:
	data_augmentation: True
	val_augment: False
	resnet18: False
	linear_steps: -1
	resnet_dropout: 0.1
	class_balanced: False
	optimizer: sgd
	freeze_bn: False
	pretrained: True
	lr: 0.00021894640154701087
	batch_size: 12
	weight_decay: 7.1565884139944e-05
	momentum: 0.8683802947171396
	convergence_rate: 0.012803366168469622
	moving_avg: 0.9263892108829195
	p: 0.24651630496904078
	swad: False
	test_batchsize: 128
	model: resnet50
	feat_layers: stem_block
	attn_tune: False
	auto_lr: False
Dataset:
	[DomainNet] #envs=6, #classes=345
	env0: clip (#48129)
	env1: info (#51605)
	env2: paint (#72266)
	env3: quick (#172500)
	env4: real (#172947)
	env5: sketch (#69128)

[37m[36mINFO[0m[0m 05/28 10:53:13 | n_steps = 15001
[37m[36mINFO[0m[0m 05/28 10:53:13 | checkpoint_freq = 1000
[37m[36mINFO[0m[0m 05/28 10:53:13 | n_steps is updated to 15001 => 15001 for checkpointing
[37m[36mINFO[0m[0m 05/28 10:53:13 | 
[37m[36mINFO[0m[0m 05/28 10:53:14 | Testenv name escaping te_sketch -> te_sketch
[37m[36mINFO[0m[0m 05/28 10:53:14 | Test envs = [5], name = te_sketch
[37m[36mINFO[0m[0m 05/28 10:53:14 | Train environments: [0, 1, 2, 3, 4], Test environments: [5]
[37m[36mINFO[0m[0m 05/28 10:53:14 | Batch sizes for each domain: [12, 12, 12, 12, 12, 0] (total=60)
[37m[36mINFO[0m[0m 05/28 10:53:14 | steps-per-epoch for each domain: 3208.67, 3440.33, 4817.75, 11500.00, 11529.83 -> min = 3208.67
[37m[36mINFO[0m[0m 05/28 10:53:15 | # of params = 24214937
[37m[36mINFO[0m[0m 05/28 11:30:45 | test_in     test_out    train_in    train_out   tr_outloss  env0_in     env0_out    env1_in     env1_out    env2_in     env2_out    env3_in     env3_out    env4_in     env4_out    env5_in     env5_out    step        epoch       loss        step_time   eval_time  
[37m[36mINFO[0m[0m 05/28 11:30:45 | 0.002785    0.001881    0.003594    0.003304    5.897316    0.003714    0.004156    0.002253    0.001453    0.005414    0.004774    0.003826    0.003594    0.002761    0.002544    0.002785    0.001881    0           0.000000    5.896840    1.039434    2249.28489 
[37m[36mINFO[0m[0m 05/28 12:12:17 | 0.353290    0.349584    0.365918    0.372024    2.994726    0.463458    0.483013    0.206739    0.181184    0.422102    0.424064    0.214928    0.233855    0.522362    0.538003    0.353290    0.349584    1000        0.311656    4.347362    0.219368    2272.53139 
[37m[36mINFO[0m[0m 05/28 12:53:43 | 0.418549    0.417939    0.452754    0.452509    2.533984    0.560513    0.571948    0.268191    0.229532    0.491896    0.488618    0.343406    0.359130    0.599763    0.613316    0.418549    0.417939    2000        0.623312    2.793030    0.216156    2269.64134 
[37m[36mINFO[0m[0m 05/28 13:35:17 | 0.438078    0.430814    0.483469    0.480607    2.362576    0.595704    0.594909    0.286237    0.241643    0.520713    0.522106    0.385174    0.403072    0.629519    0.641302    0.438078    0.430814    3000        0.934968    2.481646    0.210270    2283.58004 
[37m[36mINFO[0m[0m 05/28 14:15:44 | 0.454044    0.449186    0.510868    0.503876    2.261431    0.630272    0.624312    0.312373    0.262571    0.537993    0.532208    0.433594    0.448957    0.640108    0.651334    0.454044    0.449186    4000        1.246624    2.286247    0.196330    2231.03136 
[37m[36mINFO[0m[0m 05/28 14:57:39 | 0.457534    0.451356    0.529464    0.519886    2.177512    0.652140    0.644052    0.325211    0.270710    0.561241    0.550128    0.450254    0.467391    0.658473    0.667149    0.457534    0.451356    5000        1.558280    2.192104    0.225905    2288.68118 
[37m[36mINFO[0m[0m 05/28 15:39:14 | 0.459885    0.456998    0.539860    0.529638    2.136874    0.663853    0.657351    0.337152    0.280012    0.562815    0.553103    0.470420    0.489739    0.665057    0.667987    0.459885    0.456998    6000        1.869936    2.077874    0.217687    2277.22617 
